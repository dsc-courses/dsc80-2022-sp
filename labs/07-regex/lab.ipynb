{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-recommendation",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-portsmouth",
   "metadata": {},
   "source": [
    "# Lab 7 ‚Äì Regular Expressions\n",
    "\n",
    "## DSC 80, Spring 2022\n",
    "\n",
    "### Due Date: Monday, May 16th at 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-resource",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and Markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding will be done in an accompanying `lab.py` file that is imported into the current notebook.\n",
    "\n",
    "Labs and programming assignments will be graded in (at most) two ways:\n",
    "1. The functions and classes in the accompanying `lab.py` file will be tested (a la DSC 20),\n",
    "2. The notebook may be graded (if it contains free response questions or asks you to draw plots).\n",
    "\n",
    "**Do not change the function names in the `lab.py` file!**\n",
    "- The functions in the `lab.py` file are how your assignment is graded, and they are graded by their name.\n",
    "- If you changed something you weren't supposed to, just use git to revert! Ask us if you need help with this, or google around for `git revert`.\n",
    "\n",
    "**Tips for working in the notebook**:\n",
    "- The notebooks serve to present the questions and give you a place to present your results for later review.\n",
    "- The notebooks in *lab assignments* are not graded (only the `lab.py` file is submitted and graded).\n",
    "- The notebook serves as a nice environment for 'pre-development' and experimentation before designing your function in your `lab.py` file. You can write code here, but make sure that all of your real work is in the `lab.py` file.\n",
    "\n",
    "**Tips for developing in the `lab.py` file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional helper functions to solve the lab! \n",
    "- Always document your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-examination",
   "metadata": {},
   "source": [
    "### Importing code from `lab.py`\n",
    "\n",
    "* We import our `lab.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-kidney",
   "metadata": {},
   "source": [
    "***Note:*** While working on the lab, check the Campuswire post titled \"Lab 7 Released!\" for any clarifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-cooking",
   "metadata": {},
   "source": [
    "## Question 1 ‚Äì Practice with Regular Expressions üõ†\n",
    "\n",
    "Regular expressions can be tricky, and the best way to gain familiarity with them is through lots of practice. In this question, you will work through ten exercises, each of which requires you to write a regular expression that matches strings that satisfy certain criteria. Make sure to take a close look at the doctests for each function in `lab.py`, as they provide useful guidance for the types of strings you should and shouldn't match.\n",
    "\n",
    "***Notes:*** \n",
    "- Make sure to refer to the [Regular Expression Resources](https://dsc80.com/resources/) posted on the course website. In particular, we recommend having [regex101.com](https://regex101.com/) open while working, along with the [cheat sheet](https://dsc80.com/resources/other/berkeley-regex-reference.pdf).\n",
    "\n",
    "- Each exercise has a star rating, between 1 (‚≠êÔ∏è) and 3 (‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è) stars, indicating its difficulty level (1 being the easiest, 3 being the hardest). If you are spending lots of time on 1-star exercises, take a close look at the syntax from lecture, as there is probably an easier way of writing the necessary pattern!\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 1 (‚≠êÔ∏è)\n",
    "\n",
    "Write a regular expression that matches strings that have `'['` as the third character and `']'` as the sixth character.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 2 (‚≠êÔ∏è)\n",
    "\n",
    "Write a regular expression that matches strings that are phone numbers that start with `'(858)'` and follow the format `'(xxx) xxx-xxxx'` (`'x'` represents a digit).\n",
    "\n",
    "***Note:*** There is a space between `'(xxx)'` and `'xxx-xxxx'`.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 3 (‚≠êÔ∏è)\n",
    "\n",
    "Write a regular expression that matches strings that:\n",
    "- are between 6 and 10 characters long (inclusive),\n",
    "- contain only alphanumeric characters, whitespace and `'?'`, and\n",
    "- end with `'?'`.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 4 (‚≠êÔ∏è‚≠êÔ∏è)\n",
    "\n",
    "Write a regular expression that matches strings with exactly two `'$'`, one of which is at the start of the string, such that:\n",
    "- the characters between the two `'$'` can be anything (including nothing) except the lowercase letters `'a'`, `'b'`, and `'c'`, (and `'$'`), and\n",
    "- the characters after the second `'$'` can only be the **lowercase or uppercase** letters `'a'`/`'A'`, `'b'`/`'B'`, and `'c'`/`'C'`, with every `'a'`/`'A'` before every `'b'`/`'B'`, and every `'b'`/`'B'` before every `'c'`/`'C'`. There must be at least one `'a'` or `'A'`, at least one `'b'` or `'B'`, and at least one `'c'` or `'C'`.\n",
    "    \n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 5 (‚≠êÔ∏è)\n",
    "Write a regular expression that matches strings that represent valid Python file names, including the extension. \n",
    "\n",
    "***Note:*** For simplicity, assume that file names contains only letters, numbers, and underscores (`'_'`).\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 6 (‚≠êÔ∏è)\n",
    "Write a regular expression that matches strings that:\n",
    "- are made up of only lowercase letters and exactly one underscore (`'_'`), and\n",
    "- have at least one lowercase letter on both sides of the underscore.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 7 (‚≠êÔ∏è)\n",
    "Write a regular expression that matches strings that start with and end with an underscore (`'_'`).\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 8 (‚≠êÔ∏è)\n",
    "\n",
    "Apple serial numbers are strings of length 1 or more that are made up of any characters, other than\n",
    "- the uppercase letter `'O'`, \n",
    "- the lowercase letter `'i`', and \n",
    "- the number `'1'`.\n",
    "\n",
    "Write a regular expression that matches strings that are valid Apple serial numbers.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 9 (‚≠êÔ∏è‚≠êÔ∏è)\n",
    "\n",
    "ID numbers are formatted as `'SC-NN-CCC-NNNN'`, where \n",
    "- SC represents state code in uppercase (e.g. `'CA'`),\n",
    "- NN represents a number with 2 digits (e.g. `'98'`),\n",
    "- CCC represents a three letter city code in uppercase (e.g. `'SAN'`), and\n",
    "- NNNN represents a number with 4 digits (e.g. `'1024'`).\n",
    "\n",
    "Write a regular expression that matches strings that are ID numbers corresponding to the cities of `'SAN'` or `'LAX'`, or the state of `'NY'`. Assume that there is only one city named `'SAN'` and only one city named `'LAX'`.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 10 (‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è)\n",
    "\n",
    "Write a function named `match_10` that takes in a string and:\n",
    "- converts the string to lowercase,\n",
    "- removes all non-alphanumeric characters (i.e. removes everything that is not in the `\\w` character class), and the letter `'a'`, and\n",
    "- returns a list of every **non-overlapping** three-character substring in the remaining string, starting from the beginning of the string.\n",
    "   \n",
    "For instance, consider the following doctest:\n",
    "\n",
    "```py\n",
    ">>> match_10('Ab..DEF')\n",
    "['bde']\n",
    "```\n",
    "\n",
    "Here's how `match_10` should process `'Ab..DEF'`:\n",
    "\n",
    "1. Convert to lowercase: `'ab..def'`.\n",
    "2. Remove non-alphanumeric characters and the letter `'a'`: `'bdef'`.\n",
    "3. Starting from the beginning of the string, there is only a single non-overlapping three character substring: `'bde'`. Hence, we return `['bde']`.\n",
    "\n",
    "***Note:*** Perform your operations in the exact order described above, otherwise your code may not pass all the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-harmony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-combine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-sessions",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-organizer",
   "metadata": {},
   "source": [
    "## Question 2 ‚Äì Capturing Groups in Regular Expressions üì°\n",
    "\n",
    "The dataset stored in `data/messy.txt` contains personal information from a fictional website that a user scraped from web server logs. Within this dataset, there are four fields that are of interest to you:\n",
    "1. Email Addresses (assume they are alphanumeric usernames and domain names)\n",
    "2. [Social Security Numbers](https://en.wikipedia.org/wiki/Social_Security_number#Structure)\n",
    "3. Bitcoin Addresses (alphanumeric strings of long length)\n",
    "4. Street Addresses\n",
    "\n",
    "Create a function `extract_personal` that takes in a string containing the contents of a server log file (like `open('data/messy.txt').read()`) and returns a **tuple of four separate lists** containing values of the 4 pieces of information listed above (in the order listed above). Do **not** keep empty values.\n",
    "\n",
    "***Note:*** Since this data is messy, your function will be allowed to miss ~5% of the records in each list. Good spot checking using certain useful substrings (e.g. `'@'` for emails) should help assure correctness! Your function will be tested on a sample of the file `messy.txt`.\n",
    "\n",
    "***Hint:*** There are multiple \"delimiters\" in use in the file; there are few enough of them that you can safely determine what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment with extract_personal using the file s below\n",
    "fp = os.path.join('data', 'messy.txt')\n",
    "s = open(fp, encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-boutique",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-praise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "test_fp = os.path.join('data', 'messy.test.txt')\n",
    "test_s = open(test_fp, encoding='utf8').read()\n",
    "emails, ssn, bitcoin, addresses = extract_personal(test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-enterprise",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-mistake",
   "metadata": {},
   "source": [
    "## Question 3 ‚Äì TF-IDF üìä\n",
    "\n",
    "The dataset `data/reviews.txt` contains [Amazon reviews](http://jmcauley.ucsd.edu/data/amazon/) for ~200k phones and phone accessories. The dataset has already been \"cleaned\" for you. In this question, you will create a function that takes in the reviews dataset as a Series (with one entry per review) as well as a single review, and returns the word that \"best summarizes the single review\" using TF-IDF.\n",
    "\n",
    "To do so, implement the two functions below.\n",
    "\n",
    "#### `tfidf_data`\n",
    "\n",
    "Create a function `tfidf_data` that takes in the reviews data as a Series (`reviews_ser`) and a single review (`review`) and returns a DataFrame indexed by the words in `review` with four columns:\n",
    "- `'cnt'`: the number of times each word is found in the review \n",
    "- `'tf'`: the term frequency for each word\n",
    "- `'idf'`: the inverse document frequency for each word\n",
    "- `'tfidf'` the TF-IDF for each word\n",
    "\n",
    "You may use a `for`-loop. The words in the outputted DataFrame may appear in any order.\n",
    "\n",
    "***Hint:*** You may need to use the [`'\\b'` character](https://www.regular-expressions.info/wordboundaries.html) somewhere.\n",
    "    \n",
    "<br>\n",
    "\n",
    "#### `relevant_word`\n",
    "\n",
    "Create a function `tfidf_data` that takes in the DataFrame that `tfidf_data` returns and returns the word that \"best summarizes\" the review. If there are multiple \"best\" summary words, return any one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment with tfidf_data using reviews_ser and review below \n",
    "fp = os.path.join('data', 'reviews.txt')\n",
    "reviews_ser = pd.read_csv(fp, header=None, squeeze=True)\n",
    "review = open(os.path.join('data', 'review.txt'), encoding='utf8').read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-celtic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-kidney",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "fp = os.path.join('data', 'reviews.txt')\n",
    "reviews_ser = pd.read_csv(fp, header=None, squeeze=True)\n",
    "review = open(os.path.join('data', 'review.txt'), encoding='utf8').read().strip()\n",
    "q3_tfidf = tfidf_data(reviews_ser, review)\n",
    "\n",
    "try:\n",
    "    q3_rel = relevant_word(q3_tfidf)\n",
    "except:\n",
    "    q3_rel = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-sight",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-keyboard",
   "metadata": {},
   "source": [
    "## Questions 4 and 5 ‚Äì Tweet Analysis üê•\n",
    "\n",
    "The dataset `data/ira.csv` contains tweets tagged by Twitter as likely being posted by the [Internet Research Agency](https://en.wikipedia.org/wiki/Internet_Research_Agency), the tweet factory facing allegations for attempting to influence US political elections.\n",
    "\n",
    "Questions 4 and 5 will focus on the following:\n",
    "- Question 4: Look at the hashtags present in the text and trends in their makeup.\n",
    "- Question 5: Prepare the dataset for modeling by creating features out of the text fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-evanescence",
   "metadata": {},
   "source": [
    "### Question 4 ‚Äì Hashtags #Ô∏è‚É£\n",
    "\n",
    "You may assume that a hashtag is any string without whitespace that immediately follows a `'#'`.\n",
    "\n",
    "#### `hashtag_list`\n",
    "\n",
    "Create a function `hashtag_list` that takes in a Series of tweet texts and returns a Series containing a list of hashtags present in each tweet's text. If a tweet's text doesn't contain a hashtag, the Series should contain an empty list for that tweet. Don't include the `'#'` symbol in the lists that are returned (see the doctest for an example).\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `most_common_hashtag`\n",
    "\n",
    "Create a function `most_common_hashtag` that takes in a Series of hashtag lists (as is outputted by `hashtag_list`) and returns a Series consisting of a single hashtag per tweet: \n",
    "- If the tweet's text has no hashtags, the entry should in the output Series should be `NaN`.\n",
    "- If the tweet's text has one distinct hashtag, the entry in the output Series should be that hashtag.\n",
    "- If the tweet's text has more than one hashtag, the entry in the output Series should be be the most common hashtag **in the whole input Series**. If there is a tie for most common, any of the most common can be returned.\n",
    "    - For example, if the input Series was `pd.Series([[2], [2], [3, 2, 3]])`, the output would be `pd.Series([2, 2, 2])`. Even though `3` was more common in the third list than `2`, `2` is the most common among all hashtags in the Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The doctests/public tests don't test your work on the `ira` data,\n",
    "# but the hidden tests do.\n",
    "# So, make sure to thoroughly test your work yourself!\n",
    "fp = os.path.join('data', 'ira.csv')\n",
    "ira = pd.read_csv(fp, names=['id', 'name', 'date', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-marketplace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-packet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-female",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-heavy",
   "metadata": {},
   "source": [
    "### Question 5 ‚Äì Features üìã\n",
    "\n",
    "Now, create a DataFrame of features from the `ira` data.  That is, create a function `create_features` that takes in a DataFrame `ira` that has just a single column, `'text'`, and returns a DataFrame with the same index as `ira` (i.e. the rows correspond to the same tweets) and the following columns:\n",
    "* `'num_hashtags'`, the number of hashtags present in the tweet.\n",
    "* `'mc_hashtags'`, the most common hashtag associated to the tweet (using the result of `most_common_hashtag` from Question 4).\n",
    "* `'num_tags'`, the number of tags the tweet has (look for the presence of `'@'`).\n",
    "* `'num_links'`, the number of hyperlinks present in the tweet.\n",
    "    - A hyperlink is a string starting with `'http://'` or `'https://'`, not followed by whitespaces.\n",
    "* `'is_retweet'`, a Boolean describing whether the tweet is a retweet. A retweet is a tweet that **begins** with `'RT'`.\n",
    "* `'text'`, a version of the tweet's text that is cleaned according to the following steps, **in this exact order**:\n",
    "    1. All meta-information above (retweet info, tags, hyperlinks, and hashtags) should be replaced with a single space.\n",
    "    2. Everything other than letters, numbers, and spaces should be replaced with a single space.\n",
    "    3. All letters should be lowercase.\n",
    "    4. All words should be separated by exactly one space, and leading/trailing whitespace should be removed (stripped).\n",
    "    \n",
    "The columns in the outputted DataFrame must be in the order `['text', 'num_hashtags', 'mc_hashtags', 'num_tags', 'num_links', 'is_retweet']`. (Remember, the DataFrame that `create_features` is called on only has a single column, `'text'`.)\n",
    "\n",
    "***Notes:***\n",
    "- It's a good idea to make helper function for each column.\n",
    "- The `\\w` character class in regex **does not** refer to letters, numbers, and spaces (or even just letters and numbers). As such, you can't use it here!\n",
    "- `create_features` will take a while to run on the entire dataset ‚Äì test it on a small sample first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The doctests/public tests don't test your work on the `ira` data,\n",
    "# but the hidden tests do.\n",
    "# So, make sure to thoroughly test your work yourself!\n",
    "fp = os.path.join('data', 'ira.csv')\n",
    "ira = pd.read_csv(fp, names=['id', 'name', 'date', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-platform",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-pacific",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "# (yes, we know it says \"hidden\" ‚Äì there are still truly hidden tests in this question)\n",
    "fp_hidden = 'data/ira_test.csv'\n",
    "ira_hidden = pd.read_csv(fp_hidden, header=None)\n",
    "text_hidden = ira_hidden.iloc[:, -1:]\n",
    "text_hidden.columns = ['text']\n",
    "\n",
    "test_hidden = create_features(text_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-suggestion",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-allocation",
   "metadata": {},
   "source": [
    "## Congratulations! You're done! üèÅ\n",
    "\n",
    "Submit your `lab.py` file to Gradescope. Note that you only need to submit the `lab.py` file; this notebook should not be uploaded.\n",
    "\n",
    "Before submitting, you should ensure that all of your work is in the `lab.py` file. You can do this by running the doctests below, which will verify that your work passes the public tests **and** that your work is in the `lab.py` file. Run the cell below; you should see no output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m doctest lab.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-shopping",
   "metadata": {},
   "source": [
    "In addition, `grader.check_all()` will verify that your work passes the public tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-water",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-advocacy",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> import doctest\n>>> doctest.run_docstring_examples(match_1, {'match_1': match_1})\n>>> doctest.run_docstring_examples(match_2, {'match_2': match_2})\n>>> doctest.run_docstring_examples(match_3, {'match_3': match_3})\n>>> doctest.run_docstring_examples(match_4, {'match_4': match_4})\n>>> doctest.run_docstring_examples(match_5, {'match_5': match_5})\n>>> doctest.run_docstring_examples(match_6, {'match_6': match_6})\n>>> doctest.run_docstring_examples(match_7, {'match_7': match_7})\n>>> doctest.run_docstring_examples(match_8, {'match_8': match_8})\n>>> doctest.run_docstring_examples(match_9, {'match_9': match_9})\n>>> doctest.run_docstring_examples(match_10, {'match_10': match_10})\n",
         "failure_message": "doctests",
         "hidden": false,
         "locked": false,
         "points": 10
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> emails[0] == 'test@test.com'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> ssn[0] == '423-00-9575'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> bitcoin[0] == '1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> addresses[0] == '530 High Street'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> q3_tfidf['cnt'].sum() == 85\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> 'before' in q3_tfidf.index\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> q3_tfidf.loc['a', 'cnt'] == 4\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(q3_tfidf.loc['phone', 'idf'], 0.820703, atol=0.5)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> testdata = [['RT @DSC80: Text-cleaning is cool! #NLP https://t.co/xsfdw88d #NLP1 #NLP1']]\n>>> test = pd.DataFrame(testdata, columns=['text'])['text']\n>>> out = hashtag_list(test)\n>>> out.iloc[0] == ['NLP', 'NLP1', 'NLP1']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = pd.Series([[2], [2], [3, 2, 3]])\n>>> (most_common_hashtag(out) == 2).all()\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> testdata = [['RT @DSC80: Text-cleaning is cool! #NLP https://t.co/xsfdw88d #NLP1 #NLP1']]\n>>> test = pd.DataFrame(testdata, columns=['text'])\n>>> out = create_features(test)\n>>> anscols = ['text', 'num_hashtags', 'mc_hashtags', 'num_tags', 'num_links', 'is_retweet']\n>>> ansdata = [['text cleaning is cool', 3, 'NLP1', 1, 1, True]]\n>>> ans = pd.DataFrame(ansdata, columns=anscols)\n>>> (out == ans).all().all()\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> test_hidden.columns.tolist() == ['text', 'num_hashtags', 'mc_hashtags', 'num_tags', 'num_links', 'is_retweet']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> 400 <= (test_hidden['num_tags'] == 0).sum() <= 450\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> 450 <= (test_hidden['num_links'] == 0).sum() <= 550\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
