{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-arkansas",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-covering",
   "metadata": {},
   "source": [
    "# Lab 5 ‚Äì Imputation\n",
    "\n",
    "## DSC 80, Spring 2022\n",
    "\n",
    "### Due Date: Monday, May 2nd at 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-microwave",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and Markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding will be done in an accompanying `lab.py` file that is imported into the current notebook.\n",
    "\n",
    "Labs and programming assignments will be graded in (at most) two ways:\n",
    "1. The functions and classes in the accompanying `lab.py` file will be tested (a la DSC 20),\n",
    "2. The notebook may be graded (if it contains free response questions or asks you to draw plots).\n",
    "\n",
    "**Do not change the function names in the `lab.py` file!**\n",
    "- The functions in the `lab.py` file are how your assignment is graded, and they are graded by their name.\n",
    "- If you changed something you weren't supposed to, just use git to revert! Ask us if you need help with this, or google around for `git revert`.\n",
    "\n",
    "**Tips for working in the notebook**:\n",
    "- The notebooks serve to present the questions and give you a place to present your results for later review.\n",
    "- The notebooks in *lab assignments* are not graded (only the `lab.py` file is submitted and graded).\n",
    "- The notebook serves as a nice environment for 'pre-development' and experimentation before designing your function in your `lab.py` file. You can write code here, but make sure that all of your real work is in the `lab.py` file.\n",
    "\n",
    "**Tips for developing in the `lab.py` file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional helper functions to solve the lab! \n",
    "- Always document your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-whale",
   "metadata": {},
   "source": [
    "### Importing code from `lab.py`\n",
    "\n",
    "* We import our `lab.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-decade",
   "metadata": {},
   "source": [
    "***Note:*** While working on the lab, check the Campuswire post titled \"Lab 5 Released!\" for any clarifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-slope",
   "metadata": {},
   "source": [
    "## Question 1 ‚Äì Payment Data üí∞\n",
    "\n",
    "In `data/payment.csv`, you are given a dataset of payment information for purchases made on January 1st, 2019. The dataset contains the purchaser's `'Id'`, `'credit_card_type'`, `'credit_card_number'`, and `'date_of_birth'`.\n",
    "\n",
    "Your job is to assess the missingness in the payments data. The question you need to answer is, **is `'credit_card_number'` missing at random dependent on the age of the shopper?**. Let's approach this question in two ways.\n",
    "\n",
    "#### `first_round`\n",
    "\n",
    "Look at distribution of ages by missingness of `'credit_card_number'` and determine if the missingness is dependent on age or not.\n",
    "\n",
    "Use the following steps to approach this problem:\n",
    "\n",
    "- Compute the ages of the purchasers.\n",
    "- Draw the distribution of ages by missingness of `'credit_card_number'`. Specifically, you will draw two histograms or density curves:\n",
    "    - One of ages where `'credit_card_number'` is missing.\n",
    "    - One of ages where `'credit_card_number'` is not missing.\n",
    "- Perform a permutation test for whether or not the two distributions mentioned above are drawn from the same population distribution. Use a 5% significance level. Use the **absolute difference of means** as your test statistic.\n",
    "\n",
    "Write a function `first_round` with no arguments that returns a **list** with two values:\n",
    "* The first value is the p-value from your permutation test. \n",
    "* The second value is either `'R'` if you reject the null hypothesis, or `'NR'` if you fail to reject the null.\n",
    "\n",
    "**Does the result match your guess? If not, what might be a problem?**\n",
    "\n",
    "***Hint:*** \n",
    "- Look at [Lecture 12](https://dsc80.com/resources/lectures/lec12/lec12.html#Assessing-missingness-through-data).\n",
    "- To find a customer's age, compute the number of years between their birth year and 2022.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `second_round`\n",
    "\n",
    "Run another permutation test for the empirical distribution of age conditional on `'credit_card_number'` with a 5% significance level. This time, use the **Kolmogorov-Smirnov statistic** as your test statistic.\n",
    "\n",
    "Write a function `second_round` with no arguments that returns a __list__ with three values: \n",
    "* The first value is the p-value from your new permutation test.\n",
    "* The second value is either `'R'` if you reject the null hypothesis, or `'NR'` if you fail to reject the null. \n",
    "* The third value is your final conclusion: `'D'` (the missingness of `'credit_card_number'` is dependent on age) or `'ND'` (the missingness of `'credit_card_number'` is not dependent on age).\n",
    "\n",
    "***Hint:*** \n",
    "- In both [Lecture 12](https://dsc80.com/resources/lectures/lec12/lec12.html#Assessing-missingness-through-data) and [Lecture 13](https://dsc80.com/resources/lectures/lec13/lec13.html), we ran permutation tests using the Kolmogorov-Smirnov test statistic **without `for`-loops**. You can use this same procedure; we have already imported `stats` from `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your permutation tests in the Jupyter Notebook;\n",
    "# put your final results in lab.py\n",
    "payments_fp = os.path.join('data', 'payment.csv')\n",
    "payments = pd.read_csv(payments_fp)\n",
    "payments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-consideration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-avatar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests to work\n",
    "first_pval, first_result = first_round()\n",
    "second_pval, second_result, second_result1 = second_round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-receipt",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-aside",
   "metadata": {},
   "source": [
    "## Question 2 ‚Äì Missing Heights üïµÔ∏è\n",
    "\n",
    "In the file `data/missing_heights.csv` are the heights of children and their fathers (`'child'` and `'father'`). The `'child_X'` columns are missing values in varying proportions; for each X, `'child_X'` is X\\% not missing (and hence (100-X)\\% missing). The missingness of these `'child_X'` columns were created as MAR dependent on `'father'` height (similar to what was done in Lecture 12). The missingness of these `'child_X'` columns are all equally dependent on father height.\n",
    "\n",
    "You will attempt to **verify** the missingness of the `'child_X'` columns as being dependent on the `'father'` height column by using permutation tests. Your permutation tests should use the Kolmogorov-Smirnov test statistic. You can use `scipy.stats`' built-in K-S function to run your permutation tests and compute your p-values; you don't need to simulate manually using a `for`-loop.\n",
    "\n",
    "#### `verify_child`\n",
    "\n",
    "Write a function `verify_child` that takes in the `heights` DataFrame and returns a Series of p-values from your permutation tests, indexed by the columns `'child_X'`.\n",
    "\n",
    "***Notes:*** \n",
    "\n",
    "- Unlike in Question 1, your permutation tests should run within your `verify_child` function.\n",
    "- You may loop over the **columns** of `heights`.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `missing_data_amounts`\n",
    "\n",
    "Now, interpret your results. In the function `missing_data_amounts`, return a __list__ of correct statements from the options below:\n",
    "1. The p-value for `'child_50'` is small because the *sampling distribution* of test statistics has low variance.\n",
    "2. MAR is hardest to determine when there are very different proportions of missing and not-missing values.\n",
    "3. The difference between p-values for `'child_5'` and `'child_95'` is due to randomness.\n",
    "4. We should expect the p-value of `'child_X'` and `'child_(100-X)'` to be similar.\n",
    "5. We should only expect the p-value of `'child_X'` and `'child_(100-X)'` to be similar if the columns are MCAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-wrist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-accused",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests to work\n",
    "heights_fp = os.path.join('data', 'missing_heights.csv')\n",
    "heights = pd.read_csv(heights_fp)\n",
    "out1_q2 = verify_child(heights.copy())\n",
    "out2_q2 = missing_data_amounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-prompt",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-mexico",
   "metadata": {},
   "source": [
    "## Question 3 ‚Äì Imputing Heights üßçüìè\n",
    "\n",
    "In [Lecture 13](https://dsc80.com/resources/lectures/lec13/lec13.html), you learned how to perform single-valued imputation conditionally on a **categorical** column: impute with the mean for each group. That is, for each distinct value of the **categorical** column, there is a single imputed value.\n",
    "\n",
    "Here, you will perform single-valued imputation conditionally on a **quantitative** column. \n",
    "\n",
    "You will work with a version of the `heights` DataFrame, `new_heights`, that has a `'father'` column and a single `'child'` column. The `'child'` column has missing values. To impute the `'child'` column, transform the `'father'` column into a categorical column by binning the values of `'father'` into [quartiles](https://en.wikipedia.org/wiki/Quartile). Once this is done, you can impute `'child'` as in lecture (and described above).\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `cond_single_imputation`\n",
    "\n",
    "Write a function `cond_single_imputation` that takes in a DataFrame with columns `'father'` and `'child'` (where `'child'` has missing values) and performs a single-valued mean imputation of the `'child'` column, conditional on `'father'`. Your function should return a **Series**.\n",
    "\n",
    "***Hints:*** \n",
    "- `pd.qcut` may be helpful!\n",
    "- The `transform` is useful for this question (see [Lecture 7](https://dsc80.com/resources/lectures/lec07/lec07.html#The-transform-method)), though it's also possible using `aggregate`. \n",
    "- As a reminder, *loops are not allowed*, and functions mentioned in \"Hints\" are not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-sherman",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-mother",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests to work\n",
    "heights_fp = os.path.join('data', 'missing_heights.csv')\n",
    "new_heights = pd.read_csv(heights_fp)[['father', 'child_50']]\n",
    "new_heights = new_heights.rename(columns={'child_50': 'child'})\n",
    "out_q3 = cond_single_imputation(new_heights.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-perfume",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-strand",
   "metadata": {},
   "source": [
    "## Question 4 ‚Äì Probabilistic Imputation üé≤\n",
    "\n",
    "In [Lecture 13](https://dsc80.com/resources/lectures/lec13/lec13.html#Probabilistic-imputation), you learned how to impute a quantitative column by sampling from the observed values. **One problem with this technique is that the imputation will never generate imputed values that weren't already in the dataset.** For example, 57, 57.5, and 59 are values in the `'child'` column of `new_heights` while 58 is not. Thus, any imputation done by sampling from the observed values in the `'child'` column will not be able to generate a height of 58, even though it's clearly a reasonable value to occur in the dataset.\n",
    "\n",
    "To keep things simple, you will impute the `'child'` column **unconditionally** from the distribution of `'child'` heights present in the dataset. This means that you will use the values present in `'child'` to impute missing values, without looking at other columns.\n",
    "\n",
    "An approach to quantitative imputation that overcomes the limitation mentioned above is as follows:\n",
    "- Create a histogram of observed `'child'` heights, using 10 bins.\n",
    "- Use the histogram to generate a number within the observed range of `'child'` heights:\n",
    "    - The likelihood a generated number belongs to a given bin is equal to the area of that bin. (Remember, in histograms, areas are proportions.)\n",
    "    - Any number within a fixed bin is equally likely to occur.\n",
    "    \n",
    "Let's illustrate this approach with an example. Let `demo` be the array of 10 numbers defined below.\n",
    "\n",
    "```py\n",
    "demo = np.array([10, 11, 11, 13, 14, 14, 13.5, 14, 15, 16])\n",
    "```\n",
    "\n",
    "- The first step is creating a histogram of `demo`. Note that with this small dataset, we will use 3 bins, but you will be using 10 bins in your imputation process.\n",
    "\n",
    "<img src='imgs/demo_histogram.png' width=300>\n",
    "\n",
    "- Note that in your process, you don't actually need to draw a histogram ‚Äì instead, use `np.histogram`.\n",
    "- In the histogram above, we see that $2 \\cdot 0.15 = 0.3 = 30\\%$ of values lie in the [10, 12) bin, $2 \\cdot 0.1 = 0.2 = 20\\%$ of values lie in the [12, 14) bin, and $2 \\cdot 0.25 = 0.5 = 50\\%$ of values lie in the [14, 16] bin.\n",
    "- Next, we need to pick a bin at random. There's a 30\\% chance we pick the [10, 12) bin, a 20\\% chance we pick the [12, 14) bin, and a 50\\% chance we pick the [14, 16] bin. `np.random.choice` will be helpful in picking a bin at random.\n",
    "- Once we pick a bin, we pick a number **uniformly at random** from within the bin. For instance, suppose we randomly chose the [14, 16] bin in the previous step. We then must select a (real) number between 14 and 16 uniformly at random. `np.random.uniform` can help you here.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `quantitative_distribution`\n",
    "    \n",
    "Create a function `quantitative_distribution` that takes in a Series, `child`, in which some values are missing, and an integer `N > 0`, and returns an **array** of `N` imputed values using the method described above. \n",
    "\n",
    "***Note:*** You may use a `for`-loop.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `impute_height_quant`\n",
    "\n",
    "Create a function `impute_height_quant` that takes in a Series, `child`, in which some values are missing and imputes them using the scheme above. `impute_height_quant` should return a Series that is the same length of `child` but with no missing values. **You should use `quantitative_distribution` to help you do this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-daughter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-width",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests to work\n",
    "heights_fp = os.path.join('data', 'missing_heights.csv')\n",
    "heights = pd.read_csv(heights_fp)\n",
    "child = heights['child_50']\n",
    "quantitative_distribution_out_q4 = quantitative_distribution(child.copy(), 100)\n",
    "impute_height_quant_out_q4 = impute_height_quant(child.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-south",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-bottom",
   "metadata": {},
   "source": [
    "## Question 5 ‚Äì The Rules of Web Scraping üöî\n",
    "\n",
    "In Lecture 14, we will start studying web scraping. This question will give you an introduction to the rules behind it.\n",
    "\n",
    "Many sites have a published policy allowing or disallowing automatic access to their site. Often, this policy is in a text file named `robots.txt`. [Here is a good article](https://moz.com/learn/seo/robotstxt) that explains what these files are, where to find them, and how to use them. **After reading the article**, please answer the following questions.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Multiple-Choice Questions\n",
    "\n",
    "**1. What is the purpose of `robots.txt`?**\n",
    "\n",
    "1. To informs agents which pages to crawl.\n",
    "\n",
    "2. To informs agents that the site is automated.\n",
    "\n",
    "3. To inform agents that robots will chase them down if their info is stolen.\n",
    "\n",
    "\n",
    "**2. Where do you put your `robots.txt` file?**\n",
    "\n",
    "1. In the folder you want to disallow.\n",
    "\n",
    "2. In the root directory of your website.\n",
    "\n",
    "3. In a Google search.\n",
    "\n",
    "**3. Is it illegal to scrape a site if there is no `robots.txt` present?** (***Hint:*** [Read this](https://medium.com/@tjwaterman99/web-scraping-is-now-legal-6bf0e5730a78) if you're not sure. If you aren't able to access the article because Medium says \"You've read all your free member-only stories...\", try opening the article in a new incognito or private browsing window üòÖ.)\n",
    "\n",
    "1. Yes.\n",
    "\n",
    "2. No.\n",
    "\n",
    "**4. Can each subdomain on a root domain use separate `robots.txt` files?**\n",
    "\n",
    "1. Yes.\n",
    "\n",
    "2. No.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "### Website Hunt\n",
    "\n",
    "Next, **find three websites that explicitly use a `robots.txt` file and allow scraping by everyone, and three that do not allow scraping by generic user-agents**.\n",
    "\n",
    "When browsing through `robots.txt` files, you may notice that some have entries for several different user-agents. The user-agent you are interested in for this question is the generic user-agent, which is denoted by `*`. A `/` after a `Disallow:` indicates that no scraping is allowed, whereas nothing after the `Disallow:` indicates that all scraping is allowed.\n",
    "\n",
    "***Notes:***\n",
    "- During your search, you may notice that very few websites allow scraping by everyone. When trying to find websites that satisfy this criteria, you are encouraged to think contextually about what kinds of websites would and wouldn't mind you scraping their data. Would a government website likely mind scraping by everyone? How about a website someone created just for fun?\n",
    "\n",
    "- Some websites may cause Gradescope to time out. Please change a website if you encounter this issue. \n",
    "\n",
    "- Below, you are asked to store the URLs of the websites you find in a list. When storing URLs, you can add `'/robots.txt'` to the end, but you don't have to ‚Äì either format will be accepted.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "#### `answers`\n",
    "\n",
    "Create an argument-free function `answers` that returns **two lists**:\n",
    "\n",
    "* one containing your answers to the multiple-choice questions, and\n",
    "* one containing the URLs of the sites you found. The first 3 URLs in the list should be of websites that allow scraping by everyone, and the last 3 URLs in the list should be of websits that disallow scraping by generic user-agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-mortality",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-howard",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-specification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests to work\n",
    "mc_answers, websites = answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-michael",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-technique",
   "metadata": {},
   "source": [
    "## Congratulations! You're done! üèÅ\n",
    "\n",
    "Submit your `lab.py` file to Gradescope. Note that you only need to submit the `lab.py` file; this notebook should not be uploaded.\n",
    "\n",
    "Before submitting, you should ensure that all of your work is in the `lab.py` file. You can do this by running the doctests below, which will verify that your work passes the public tests **and** that your work is in the `lab.py` file. Run the cell below; you should see no output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m doctest lab.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-roman",
   "metadata": {},
   "source": [
    "In addition, `grader.check_all()` will verify that your work passes the public tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-reservation",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-mountain",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(first_round(), list) and isinstance(second_round(), list)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> first_pval < 1\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> first_result in ['NR', 'R']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> second_pval < 1\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> second_result in ['NR', 'R']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> second_result1 in ['ND', 'D']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out1_q2['child_50'] < out1_q2['child_95']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> out1_q2['child_5'] > out1_q2['child_50']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> set(out2_q2) <= set(range(1, 6))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out_q3.isna().sum() == 0\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> (new_heights['child'].std() - out_q3.std()) > 0.5\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> quantitative_distribution_out_q4.min() >= 56\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> quantitative_distribution_out_q4.max() <= 79\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> np.isclose(quantitative_distribution_out_q4.mean(), child.mean(), atol=1)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> np.isclose(quantitative_distribution_out_q4.std(), 3.5, atol=0.65)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> impute_height_quant_out_q4.isna().sum() == 0\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> np.isclose(impute_height_quant_out_q4.mean(), child.mean(), atol=0.5)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> np.isclose(impute_height_quant_out_q4.mean(), child.mean(), atol=0.2)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> np.isclose(impute_height_quant_out_q4.std(), child.std(), atol=0.15)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(mc_answers) == 4\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> len(websites) == 6\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
