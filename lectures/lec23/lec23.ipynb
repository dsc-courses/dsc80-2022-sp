{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rc('figure', dpi=100, figsize=(7, 5))\n",
    "plt.rc('font', size=12)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 23 – Pipelines and Evaluation\n",
    "\n",
    "## DSC 80, Spring 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For fun: read [this article from CBS 8 San Diego](https://www.cbs8.com/article/news/investigations/san-diego-pays-113-million-in-overtime/509-cccb0373-b602-448d-90d7-a40b0f30a2a6) 💰🚒🚔.\n",
    "\n",
    "> Last year, the city paid \\\\$52.93 million in overtime to the fire department. San Diego Police Department received the second highest out of any other department with \\\\$36.6 million."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements\n",
    "\n",
    "- Discussion 7 is due for extra credit **tomorrow at 11:59PM**.\n",
    "    - See [this post](https://campuswire.com/c/G325FA25B/feed/1473) for a clarification.\n",
    "- Lab 8 is due on **Monday, May 23rd at 11:59PM**.\n",
    "- Project 4 is due **Thursday, May 26th at 11:59PM**.\n",
    "    - See [this post](https://campuswire.com/c/G325FA25B/feed/1511) for more public tests for Question 5.\n",
    "- Lab 7 grades are released.\n",
    "    - See [this post](https://campuswire.com/c/G325FA25B/feed/1515) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "- Models in `sklearn`.\n",
    "- Pipelines.\n",
    "- Model evaluation 🧪."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Models in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Predicting `'tip'` from `'total_bill'` and `'size'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = sns.load_dataset('tips')\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we instantiate and fit. By calling `fit`, we are saying \"minimize mean squared error and find $w^*$\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "# Note that there are two arguments to fit – X and y!\n",
    "# (It is not necessary to write X= and y=)\n",
    "lr.fit(X=tips[['total_bill', 'size']], y=tips['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, the `predict` method is available. Note that the argument to `predict` can be any 2D array with two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted tip from a table of 3 that spends $25 \n",
    "lr.predict([[25, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted tip from a table of 14 that spends $1000 – probably not accurate!\n",
    "lr.predict([[1000, 14]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the intercepts and slopes individually. This model is of the form\n",
    "\n",
    "$$\\text{predicted tip} = w_0^* + w_1^* \\cdot \\text{total bill} + w_2^* \\cdot \\text{table size}$$\n",
    "\n",
    "so we should expect three parameters total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to compute the RMSE of our model, we need to find its predictions on every row in the training data (`tips`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = lr.predict(tips[['total_bill', 'size']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.mean((all_preds - tips['tip']) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It turns out that fit `LinearRegression` objects also have a `score` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr.score(tips[['total_bill', 'size']], tips['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look like the RMSE... what is it? 🤔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: $R^2$\n",
    "\n",
    "- $R^2$, or the **coefficient of determination**, is a measure of the **quality of a linear fit**.\n",
    "- There are a few equivalent ways of computing it, assuming your model **is linear and has an intercept term**:\n",
    "\n",
    "$$R^2 = \\frac{\\text{var}(\\text{predicted $y$ values})}{\\text{var}(\\text{actual $y$ values})}$$\n",
    "\n",
    "$$R^2 = \\left[ \\text{correlation}(\\text{predicted $y$ values}, \\text{actual $y$ values}) \\right]^2$$\n",
    "\n",
    "- In the simple linear regression case, it is the square of the correlation coefficient, $r$.\n",
    "- **Key idea:** $R^2$ ranges from 0 to 1. **The closer it is to 1, the better the linear fit is.**\n",
    "    - $R^2$ has no units of measurement, unlike RMSE.\n",
    "- Interpretation: $R^2$ is the **proportion of variance in $y$ that the linear model explains**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Calculating $R^2$\n",
    "\n",
    "Recall, `all_preds` contains the predicted `'tip'` for every data point in `tips`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Method 1: $R^2 = \\frac{\\text{var}(\\text{predicted $y$ values})}{\\text{var}(\\text{actual $y$ values})}$**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(all_preds) / np.var(tips['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Method 2:** $R^2 = \\left[ \\text{correlation}(\\text{predicted $y$ values}, \\text{actual $y$ values}) \\right]^2$\n",
    "\n",
    "Note: By correlation here, we are referring to $r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.corrcoef(all_preds, tips['tip'])) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Method 3:** `lr.score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr.score(tips[['total_bill', 'size']], tips['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three methods provide the same result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `LinearRegression` summary\n",
    "\n",
    "|Property|Example|Description|\n",
    "|---|---|---|\n",
    "|Initialize model parameters| `lr = LinearRegression()` | Create (empty) linear regression model|\n",
    "|Fit the model to the data | `lr.fit(data, responses)` | Determines regression coefficients|\n",
    "|Use model for prediction |`lr.predict(newdata)`| Use regression line make predictions|\n",
    "|Evaluate the model| `lr.score(data, responses)` | Calculate the $R^2$ of the LR model|\n",
    "|Access model attributes| `lr.coef_` | Access the regression coefficients|\n",
    "\n",
    "***Note:*** Once `fit`, estimators like `LinearRegression` are just transformers (`predict` <-> `transform`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgs/image_0.png\" width=\"50%\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "So far, we've used transformers for feature engineering and models for prediction. We can combine these steps into a single `Pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `Pipeline`s in `sklearn`\n",
    "\n",
    "- A `Pipeline` object is instantiated using a **list** containing transformer(s) and a model (estimator).\n",
    "```py\n",
    "pl = Pipeline([feat_trans1, feat_trans2, ..., mdl])\n",
    "```\n",
    "- Once a `Pipeline` is instantiated, you can fit **all** steps (transformers and model) using `fit`.\n",
    "```py\n",
    "pl.fit(data, responses)\n",
    "```\n",
    "- To make predictions using **raw (untransformed) data**, use `pl.predict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating a `Pipeline`\n",
    "\n",
    "- To instantiate a `Pipeline`, we must provide a list with zero or more transformers followed by a single model.\n",
    "    - All \"steps\" must have `fit` methods, and all but the last must have `transform` methods.\n",
    "- The list we provide `Pipeline` with must be a list of **tuples**, where\n",
    "    - The first element is a \"name\" (that we choose) for the step.\n",
    "    - The second element is a transformer or estimator instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's build a `Pipeline` that:\n",
    "- One-hot-encodes the categorical features in `tips`.\n",
    "- Fits a regression model on the one-hot-encoded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_cat = tips[['sex', 'smoker', 'day', 'time']]\n",
    "tips_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline([\n",
    "    ('one-hot', OneHotEncoder()),\n",
    "    ('lin-reg', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that `pl` is instantiated, we `fit` it the same way we would fit the individual steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.fit(tips_cat, tips['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to make predictions using **raw data**, all we need to do is use `pl.predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.predict([['Male', 'Yes', 'Sat', 'Lunch']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.predict(tips_cat.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pl` performs **both** feature transformation and prediction with just a single call to `predict`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can access individual \"steps\" of a `Pipeline` through the `named_steps` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.named_steps['one-hot'].transform(tips_cat).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.named_steps['lin-reg'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More sophisticated `Pipeline`s\n",
    "\n",
    "- In the previous example, we one-hot-encoded every input column. **What if we want to perform different transformations on different columns?**\n",
    "- **Solution:** Use a `ColumnTransformer`.\n",
    "    - Instantiate a `ColumnTransformer` using a list of tuples, where:\n",
    "        - The first element is a \"name\" we choose for the transformer.\n",
    "        - The second element is a transformer instance (e.g. `OneHotEncoder()`).\n",
    "        - The third element is a **list of relevant column names**.\n",
    "    - `ColumnTransformer` is extremely useful, but it was only added to `sklearn` in 2018!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='imgs/image_3.png' width=50%></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform different transformations on the quantitative and categorical features of `tips` (so, we will not transform `'tip'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_features = tips.drop('tip', axis=1)\n",
    "tips_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To the **quantitative features (`'total_bill'` and `'size'`)**, we will apply the `StandardScaler` transformer.\n",
    "- To the **categorical features**, we will apply the `OneHotEncoder` transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('quant', StandardScaler(), ['total_bill', 'size']),\n",
    "        ('cat', OneHotEncoder(), ['sex', 'smoker', 'day', 'time'])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a `Pipeline` using `preproc` as a transformer, and `fit` it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline([\n",
    "    ('preprocessor', preproc), \n",
    "    ('lin-reg', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.fit(tips_features, tips['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction is as easy as calling `predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.predict(tips_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pl` also has a `score` method, the same way a fit `LinearRegression` instance does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl.score(tips_features, tips['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, we can access the individual \"steps\" in `pl` using the `named_steps` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.named_steps['preprocessor'].transform(tips_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** `ColumnTransformer` has a `remainder` argument that you can use to specify what to do with columns that aren't being transfromed (`'drop'` or `'passthrough'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model evaluation 🧪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation\n",
    "\n",
    "- You and Billy are studying for an upcoming exam. You both decide to test your understanding by taking a **practice exam**.\n",
    "    - Your logic: If you do well on the practice exam, you should do well on the real exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- You each take the practice exam once and look at the solutions afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Your strategy:** Memorize the answers to all practice exam questions, e.g. \"Question 1: A; Question 2: C; Question 3: A.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Billy's strategy:** Learn high-level concepts from the solutions, e.g. \"data are NMAR if the likelihood of missingness depends on the missing values themselves.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Who will do better on the **practice exam**? Who will probably do better on the **real exam**? 🧐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating the quality of a model\n",
    "\n",
    "- So far, we've computed the RMSE (and $R^2$) of our fit regression models on the **data that we used to fit them**, i.e. the **training data**.\n",
    "- We've said that Model A is **better** than Model B if Model A's RMSE is **lower** than Model B's RMSE.\n",
    "    - Remember, our **training data** is a sample from the data generating process.\n",
    "    - Just because a model fits the training data well doesn't mean it will **generalize** and work well on **similar, unseen samples**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Overfitting and underfitting\n",
    "\n",
    "Let's collect two samples $\\{(x_i, y_i)\\}$ from the same **data generating process**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23) # For reproducibility\n",
    "\n",
    "def sample_dgp(n=100):\n",
    "    x = np.linspace(-2, 3, n)\n",
    "    y = x ** 3 + (np.random.normal(0, 3, size=n))\n",
    "    return x.reshape(-1, 1), y\n",
    "\n",
    "x1, y1 = sample_dgp()\n",
    "x2, y2 = sample_dgp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's just look at Sample 1. The relationship between $x$ and $y$ is roughly **cubic**; that is, $y \\approx x^3$ (remember, in reality, you won't get to see the DGP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x1, y1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's fit three **polynomial** models on Sample 1:\n",
    "- Degree 1.\n",
    "- Degree 3.\n",
    "- Degree 15.\n",
    "\n",
    "The `PolynomialFeatures` transformer will be helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = PolynomialFeatures(3)\n",
    "d2.fit_transform([[1], [2], [3], [4], [5]]) # fit_transform fits and transforms the same input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degs = [1, 3, 15]\n",
    "\n",
    "def fit_polys(x, y):\n",
    "    # Create all three Pipelines\n",
    "    pls = [\n",
    "        Pipeline([('poly', PolynomialFeatures(d)), ('lin-reg', LinearRegression())])\n",
    "        for d in degs\n",
    "    ]\n",
    "\n",
    "    # Fit all three Pipelines\n",
    "    [pl.fit(x, y) for pl in pls];\n",
    "\n",
    "    # Make all three sets of predictions\n",
    "    preds = [pl.predict(x) for pl in pls]\n",
    "    return preds\n",
    "    \n",
    "preds_1 = fit_polys(x1, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we look at our three models' predictions on Sample 1 (which they were trained on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1, 3, figsize=(15, 5), dpi=100)\n",
    "plt.suptitle('Performance on Sample 1\\n')\n",
    "\n",
    "for i in range(1, 4):\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.scatter(x1, y1, label='actual')\n",
    "    plt.plot(x1, preds_1[i-1], color='orange', label='predictions', linewidth=5)\n",
    "    rmse_d = np.sqrt(np.mean((preds_1[i-1] - y1) ** 2))\n",
    "    plt.title(f'Degree {degs[i-1]}, RMSE: {np.round(rmse_d, 2)}')\n",
    "plt.legend(loc=(1.04, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The degree 15 polynomial has the lowest RMSE on Sample 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do things look in Sample 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1, 3, figsize=(15, 5), dpi=100)\n",
    "plt.suptitle('Performance on Sample 2\\n')\n",
    "\n",
    "for i in range(1, 4):\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.scatter(x2, y2, label='actual')\n",
    "    plt.plot(x1, preds_1[i-1], color='orange', label='predictions', linewidth=5)\n",
    "    rmse_d = np.sqrt(np.mean((preds_1[i-1] - y2) ** 2))\n",
    "    plt.title(f'Degree {degs[i-1]}, RMSE: {np.round(rmse_d, 2)}')\n",
    "plt.legend(loc=(1.04, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The degree 3 polynomial has the lowest RMSE on Sample 2. \n",
    "- Note that **we didn't get to see Sample 2 when fitting our models**! \n",
    "- As such, it seems that the degree 3 polynomial **generalizes better** to unseen data than the degree 15 polynomial does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What if we fit a degree 1, degree 3, and degree 15 polynomial **on Sample 2** as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_2 = fit_polys(x2, y2)\n",
    "\n",
    "plt.subplots(1, 3, figsize=(15, 5), dpi=100)\n",
    "plt.suptitle('Models Fit on Samples 1 and 2\\n')\n",
    "\n",
    "for i in range(1, 4):\n",
    "    plt.subplot(1, 3, i)\n",
    "#     plt.scatter(x2, y2, label='actual')\n",
    "    plt.plot(x1, preds_1[i-1], color='orange', label='Fit on Sample 1', linewidth=5)\n",
    "    plt.plot(x2, preds_2[i-1], color='purple', label='Fit on Sample 2', linewidth=5)\n",
    "    plt.title(f'Degree {degs[i-1]}')\n",
    "plt.legend(loc=(1.04, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Key idea:** The degree 15 polynomial seems to **vary more** than the degree 3 and 1 polynomials do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias and variance\n",
    "\n",
    "The training data we have access to is a sample from the DGP. We are concerned with our model's performance **across different datasets** from the same DGP.\n",
    "\n",
    "Suppose we **fit** a model $H$ (e.g. a degree 3 polynomial) on **several different datasets** from a DGP. There are three sources of error that arise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* ⭐️ **Bias**: **The expected deviation between a predicted value and an actual value**.\n",
    "    - In other words, **for a given $x_i$, how far is $H(x_i)$ from the true $y_i$, on average?**\n",
    "    - Low bias is good! ✅\n",
    "    - High bias is a sign of **underfitting**, i.e. that our model is too **basic** to capture the relationship between our features and response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ⭐️ **Model variance (\"variance\")**: **The variance of a model's predictions**.\n",
    "    - In other words, **for a given $x_i$, what is the variance of $H(x_i)$ across all datasets**?\n",
    "    - Low model variance is good! ✅\n",
    "    - High model variance is a sign of **overfitting**, i.e. that our model is too **complicated** and is prone to fitting to the noise in our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Observation variance**: The variance due to the random noise in the process we are trying to model (e.g. measurement error). _We can't control this, without collecting more data!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here, suppose \n",
    "- the <span style='color:red'>red bulls-eye</span> represents your **true weight and height** 🧍, and \n",
    "- the <span style='color:blue'>blue darts</span> represent **predictions of your weight and height** using different models that were fit on the same DGP. \n",
    "<br>\n",
    "\n",
    "<center><img src=\"imgs/image_5.png\" width=\"40%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Avoiding overfitting\n",
    "\n",
    "- We won't know whether our model has **overfit** to our sample (training data) unless we get to see how well it performs on a new sample from the same DGP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Idea 💡:** **Split** our sample into a **training set** and **test set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Use **only** the training set to fit the model (i.e. find $w^*$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Use the test set to evaluate the model's error (RMSE, $R^2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is like \"generating\" new data from the same DGP!\n",
    "    - _Similar_ to bootstrapping (but not quite the same, because there is no resampling involved).\n",
    "    - **If our sample is not representative of the DGP, this method has limited effectiveness!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgs/train-test.png\" width='50%'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train-test split 🚆\n",
    "\n",
    "`sklearn.model_selection.train_test_split` implements a train-test split for us! 🙏🏼 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If `X` is an array/DataFrame of features and `y` is an array/Series of responses,\n",
    "\n",
    "```py\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "```\n",
    "\n",
    "randomly splits the features and responses into training and test sets, such that the test set contains 0.25 of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the documentation!\n",
    "train_test_split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform a train/test split on our `tips` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips.drop('tip', axis=1)\n",
    "y = tips['tip']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # Don't have to choose 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, let's check the sizes of `X_train` and `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rows in X_train:', X_train.shape[0])\n",
    "display(X_train.head())\n",
    "print('Rows in X_test:', X_test.shape[0])\n",
    "display(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[0] / tips.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example prediction pipeline\n",
    "\n",
    "Steps:\n",
    "- Fit a model on the training set.\n",
    "- Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips[['total_bill', 'size']] # For this example, we'll use just the already-quantitative columns in tips\n",
    "y = tips['tip']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll use a stand-alone `LinearRegression` model without a `Pipeline`, but this process would work the same if we were using a `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our model's performance on the **training** set first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = lr.predict(X_train)\n",
    "rmse_train = np.sqrt(np.mean((pred_train - y_train) ** 2))\n",
    "rmse_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the **test** set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = lr.predict(X_test)\n",
    "rmse_test = np.sqrt(np.mean((pred_test - y_test) ** 2))\n",
    "rmse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since `rmse_train` and `rmse_test` are similar, it **doesn't seem like our model is overfitting** to the training data. If `rmse_test` was much larger than `rmse_train`, it would be evidence that our model is unable to **generalize well**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary, next time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- $R^2$ is a measure of a model's \"goodness-of-fit\". For linear models, it ranges between 0 (poor) and 1 (great).\n",
    "- `Pipeline`s in `sklearn` combine one or more transformers with a single model (estimator), allowing us to perform feature engineering and prediction through a single object.\n",
    "- We want to build models that **generalize well** to unseen data.\n",
    "    - Models that have high **bias** are too simple to represent complex relationships in data, and **underfit**.\n",
    "    - Models that have high **variance** are overly complex for the relationships in the data, and vary a lot when fit on different datasets. Such models **overfit** to the training data.\n",
    "- In order to prevent overfitting, we should perform a **train-test split** in which we only train our model on a subset of the data available to us, and use the remainder for evaluation."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "livereveal": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
