{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rc('figure', dpi=100, figsize=(7, 5))\n",
    "plt.rc('font', size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 21 ‚Äì Feature Engineering and Modeling\n",
    "\n",
    "## DSC 80, Spring 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements\n",
    "\n",
    "- Lab 7 is due **today at 11:59PM**.\n",
    "- Project 4 has been released!\n",
    "    - The checkpoint is due **this Thursday at 11:59PM**.\n",
    "    - The full project is due **Thursday, May 26th at 11:59PM**.\n",
    "    - Start early!\n",
    "- üì£ Come to the DSC **Town Hall**, tomorrow from 3-5PM in the SDSC Auditorium. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "- Feature engineering ‚öôÔ∏è.\n",
    "- Modeling.\n",
    "- Example: Restaurant tips üßë‚Äçüç≥.\n",
    "\n",
    "We won't finish the `galton` example from the last lecture, but you should read through the [end of it](https://dsc80.com/resources/lectures/lec20/lec20.html#Attempt-%233:-Adding-gender-as-a-feature), as it is a nice complement to today's lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature engineering ‚öôÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The goal of feature engineering\n",
    "\n",
    "- **Feature engineering** is the act of finding **transformations** that transform data into effective **quantitative variables**.\n",
    "\n",
    "- A feature function $\\phi$ (phi, pronounced \"fea\") is a mapping from raw data to $d$-dimensional space, i.e. $\\phi: \\text{raw data} \\rightarrow \\mathbb{R}^d$.\n",
    "\n",
    "    - If two observations $x_i$ and $x_j$ are \"similar\" in the raw data space, then $\\phi(x_i)$ and $\\phi(x_j)$ should also be \"similar.\"\n",
    "\n",
    "- A \"good\" choice of features depends on many factors:\n",
    "    - The kind of data (quantitative, ordinal, nominal),\n",
    "    - The relationship(s) and association(s) being modeled,\n",
    "    - The model type (e.g. linear models, decision tree models, neural networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Predicting ratings ‚≠êÔ∏è\n",
    "\n",
    "- We want to build a multiple regression model that uses the features (`'UID'`, `'AGE'`, `'STATE'`, `'HAS_BOUGHT'`, and `'REVIEW'`) below to predict `'RATING'`.\n",
    "\n",
    "- Why can't we build a model right away?\n",
    "- What must we do so that we can build a model?\n",
    "\n",
    "|UID|AGE|STATE|HAS_BOUGHT|REVIEW|\\||RATING|\n",
    "|---|---|---|---|---|---|---|\n",
    "|74|32|NY|True|\"Meh.\"|\\||&#10025;&#10025;|\n",
    "|42|50|WA|True|\"Worked out of the box...\"|\\||&#10025;&#10025;&#10025;&#10025;|\n",
    "|57|16|CA|NULL|\"Hella tots lit yo...\"|\\||&#10025;|\n",
    "|...|...|...|...|...|\\||...|\n",
    "|(int)|(int)|(str)|(bool)|(str)|\\||(str)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Issues: Missing values, emojis and strings instead of numbers, unrelated columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Uninformative features\n",
    "\n",
    "- `'UID'` was likely used to join the user information (e.g., `'AGE'` and `'STATE'`) with some `reviews` dataset.\n",
    "- Even though `'UID'`s are stored as **numbers**, the numerical value of a user's `'UID'` won't help us predict their `'RATING'`.\n",
    "- If we include the `'UID'` feature, our model will find whatever patterns it can between `'UID'`s and `'RATING'`s in the training (observed data).\n",
    "    - This will lead to a lower training RMSE.\n",
    "- However, since there is truly no relationship between `'UID'` and `'RATING'`, this will lead to **worse** model performance on unseen data (bad).\n",
    "- **Transformation:** drop `'UID'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dropping features\n",
    "\n",
    "There are certain scenarios where manually dropping features might be helpful:\n",
    "\n",
    "1. When the features **do not contain information** associated with the prediction task. \n",
    "2. When the feature is **not available at prediction time.**  \n",
    "- The goal of building a model to predict `'RATING'`s is so that we can **predict `'RATING'`s for users who haven't actually made a `'RATING'`s yet**.\n",
    "- As such, our model should only depend on features that we would know before the user makes their `'RATING'`.\n",
    "- For instance, if users only enter `'REVIEW'`s after entering `'RATING'`s, we shouldn't use `'REVIEW'`s as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Encoding ordinal features\n",
    "\n",
    "|UID|AGE|STATE|HAS_BOUGHT|REVIEW|\\||RATING|\n",
    "|---|---|---|---|---|---|---|\n",
    "|74|32|NY|True|\"Meh.\"|\\||&#10025;&#10025;|\n",
    "|42|50|WA|True|\"Worked out of the box...\"|\\||&#10025;&#10025;&#10025;&#10025;|\n",
    "|57|16|CA|NULL|\"Hella tots lit yo...\"|\\||&#10025;|\n",
    "|...|...|...|...|...|\\||...|\n",
    "|(int)|(int)|(str)|(bool)|(str)|\\||(str)|\n",
    "\n",
    "How do we encode the `'RATING'` column, an ordinal variable, as a quantitative variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Transformation:** Replace \"number of &#10025;\" with \"number\".\n",
    "    - This is an **ordinal encoding**, a transformation that maps ordinal values to the positive integers in a way that preserves order.\n",
    "    - Example: (freshman, sophomore, junior, senior) -> (0, 1, 2, 3).\n",
    "    - **Important:** This transformation preserves \"distances\" between ratings.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "order_values = ['‚ú©', '‚ú©‚ú©', '‚ú©‚ú©‚ú©', '‚ú©‚ú©‚ú©‚ú©', '‚ú©‚ú©‚ú©‚ú©‚ú©']\n",
    "ordinal_enc = {y:x + 1 for (x, y) in enumerate(order_values)}\n",
    "ordinal_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.DataFrame().assign(RATING=['‚ú©', '‚ú©‚ú©', '‚ú©‚ú©‚ú©', '‚ú©‚ú©', '‚ú©‚ú©‚ú©', '‚ú©', '‚ú©‚ú©‚ú©', '‚ú©‚ú©‚ú©‚ú©', '‚ú©‚ú©‚ú©‚ú©‚ú©'])\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.replace(ordinal_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Encoding nominal features\n",
    "\n",
    "|UID|AGE|STATE|HAS_BOUGHT|REVIEW|\\||RATING|\n",
    "|---|---|---|---|---|---|---|\n",
    "|74|32|NY|True|\"Meh.\"|\\||&#10025;&#10025;|\n",
    "|42|50|WA|True|\"Worked out of the box...\"|\\||&#10025;&#10025;&#10025;&#10025;|\n",
    "|57|16|CA|NULL|\"Hella tots lit yo...\"|\\||&#10025;|\n",
    "|...|...|...|...|...|\\||...|\n",
    "|(int)|(int)|(str)|(bool)|(str)|\\||(str)|\n",
    "\n",
    "How do we encode the `'STATE'` column, a nominal variable, as a quantitative variable?\n",
    "- In other words, how do we turn `'STATE'`s into meaningful numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Idea:** Ordinal encoding. AL -> 1, AK -> 2, ..., WY -> 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ‚ùå An ordinal encoding is **not** appropriate, because `'STATE'` is not an ordinal variable - Wyoming is not inherently \"more\" of anything than Alabama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Another idea:** Use one binary variable per state, i.e. `'is_AL'`, `'is_AK'`, ..., `'is_WY'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One-hot encoding\n",
    "\n",
    "- One-hot encoding is a transformation that turns a categorical feature into several binary features.\n",
    "- Suppose column `'col'` has $N$ unique values, $A_1$, $A_2$, ..., $A_N$. For each unique value $A_i$, we define the following **feature function**:\n",
    "\n",
    "$$\\phi_i(x) = \\left\\{\\begin{array}{ll}1 & {\\rm if\\ } x = A_i \\\\ 0 &  {\\rm if\\ } x\\neq A_i \\\\ \\end{array}\\right. $$\n",
    "\n",
    "- Note that 1 means \"yes\" and 0 means \"no\".\n",
    "- One-hot encoding is also called \"dummy encoding\", and $\\phi(x)$ may also be referred to as an \"indicator variable\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: One-hot encoding `'STATE'`\n",
    "\n",
    "- For each unique value of `'STATE'` in our dataset, we must create a column for just that `'STATE'`.\n",
    "\n",
    "<center><img src=\"imgs/image_4.png\"></center>\n",
    "\n",
    "- Observations:\n",
    "    - In any given row, only one of the one-hot-encoded columns will contain a 1; the rest will contain a 0.\n",
    "    - Most of the values in the one-hot-encoded columns are 0, i.e. these columns are **sparse**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform the one-hot encoding ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pd.DataFrame().assign(STATE=['NY', 'WA', 'CA', 'NY', 'OR'])\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to access all **unique** values of `'STATE'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_states = states['STATE'].unique()\n",
    "unique_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How might we create one-hot-encoded columns manually?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states['STATE'] == unique_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(states['STATE'] == unique_states[1], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_states(states_ser):\n",
    "    return pd.Series(states_ser == unique_states, index=unique_states, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states['STATE'].apply(ohe_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soon, we will learn how to \"automatically\" perform one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quantitative scaling\n",
    "\n",
    "The feature transformations we've discussed so far have involved converting **categorical** variables into **quantitative** variables. However, at times we'll need to transform **quantitative** variables into new **quantitative** variables.\n",
    "- **Standardization**: $x_i \\rightarrow \\frac{x_i - \\bar{x}}{\\sigma_x}$.\n",
    "- **Linearization via a non-linear transformation**: e.g. $\\text{log}$ and $\\text{sqrt}$. See Lab 8 for more.\n",
    "\n",
    "<center><img src='imgs/transform.png' width=50%></center>\n",
    "\n",
    "- **Discretization:** Convert data into percentiles (or more generally, quantiles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modeling\n",
    "\n",
    "* **Data Generating Process**: The real-world phenomena that we are interested in studying.\n",
    "    - *Example:* Every year, city employees are hired and fired, earn salaries and benefits, etc.\n",
    "    - Unless we work for the city, we can't observe this process directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Model:** A theory about the data generating process.\n",
    "    - *Example:* If an employee is $X$ years older than average, then they will make \\$100,000 in salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Fit Model**: A model that is learned from a particular set of observations, i.e. training data.\n",
    "    - *Example:* If an employee is 5 years older than average, they will make \\$100,000 in salary.\n",
    "    - How is this estimate determined? What makes it \"good\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Goals of modeling\n",
    "\n",
    "- To make accurate **predictions** regarding unseen data drawn from the data generating process.\n",
    "    - Given this dataset of past UCSD data science students' salaries, can we predict your future salary? (regression)\n",
    "    - Given this dataset of emails, can we predict if this new email is spam or not? (binary classification)\n",
    "    - Given this dataset of images, can we predict if this new image is of a dog, cat, or zebra? (multiclass classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    \n",
    "- To make **inferences** about the structure of the data generating process (i.e. to understand complex phenomena).\n",
    "    - Is there a linear relationship between the heights of children and the heights of their biological fathers?\n",
    "    - The weights of smoking and non-smoking mothers' babies babies in my _sample_ are different ‚Äì how _confident_ am I that this difference exists in the _population_?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='imgs/taxonomy.png' width=60%></center>\n",
    "\n",
    "- Of the two focuses of models, we will focus on **prediction**.\n",
    "- In the above taxonomy, we will focus on **supervised learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data to models\n",
    "\n",
    "- The modeling techniques we are most familiar with (e.g. linear regression) require:\n",
    "    - Quantitative inputs.\n",
    "    - Strong relationships between inputs ($X$) and outputs ($Y$).\n",
    "- Often, these properties don't exist in the raw data.\n",
    "\n",
    "<center><img src=\"imgs/image_0.png\" width=30%></center>\n",
    "\n",
    "- That's where feature engineering comes into play.\n",
    "\n",
    "<center><img src=\"imgs/image_1.png\" width=50%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Restaurant tips üßë‚Äçüç≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tips = sns.load_dataset('tips')\n",
    "tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predicting tips\n",
    "\n",
    "- **Goal:** Given various information about a table, we want to predict the **tip** that a server will earn.\n",
    "- Why might a server be interested in doing this?\n",
    "    - To determine which tables are likely to tip the most (inference).\n",
    "    - To understand the relationship between diners and tips (inference).\n",
    "    - To predict earnings over the next month (prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exploratory data analysis (EDA)\n",
    "\n",
    "- The most natural feature to look at first is `'total_bill'`.\n",
    "- As such, we should explore the relationship between `'total_bill'` and `'tip'`, as well as the distributions of both columns individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=tips, x='total_bill', y='tip');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "sns.histplot(tips['total_bill'], kde=True, ax=ax1)\n",
    "sns.histplot(tips['tip'], kde=True, ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Observations\n",
    "|`'total_bill'`|`'tip'`|\n",
    "|---|---|\n",
    "|Right skewed|Right skewed|\n",
    "|Mean around \\$20|Mean around \\$3|\n",
    "|Mode around \\$15|Possibly bimodal?|\n",
    "|No large bills|Large outliers?|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='imgs/convo.png' width=50%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model #1: Constant\n",
    "\n",
    "- Let's start simple. Suppose our model assumes every tip is given by a constant dollar amount:\n",
    "\n",
    "$$\\texttt{tip} = h^{\\text{true}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Model:** There is a single tip amount $h^{\\text{true}}$ that all customers pay.\n",
    "    - Correct? No!\n",
    "    - Useful? Perhaps. An estimate of $h^{\\text{true}}$, denoted by $h^*$, can allow us to predict future tips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The true parameter $h^{\\text{true}}$ is determined by the universe (i.e. the data generating process).\n",
    "    - We can't observe the parameter; we need to **estimate it from the data**.\n",
    "    - Hence, our estimate depends on our dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### All models are wrong...\n",
    "\n",
    "\"...but some are useful.\"\n",
    "\n",
    "> \"Since all models are wrong the scientist cannot obtain a \"correct\" one by excessive elaboration. On the contrary following William of Occam he should **seek an economical description of natural phenomena**. Just as the ability to devise simple but evocative models is the signature of the great scientist so overelaboration and overparameterization is often the mark of mediocrity.\"\n",
    "\n",
    "> \"Since all models are wrong the scientist must be alert to what is importantly wrong. It is inappropriate to be concerned about mice when there are tigers abroad.\"\n",
    "\n",
    "-- George Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimating $h^{\\text{true}}$\n",
    "\n",
    "- There are several ways we _could_ estimate $h^{\\text{true}}$.\n",
    "    - We could use domain knowledge (e.g. everyone clicks the \\$1 tip option when buying coffee).\n",
    "\n",
    "- From DSC 40A, we already know one way:\n",
    "    - **Choose a loss function**, which measures how \"good\" a single prediction is.\n",
    "    - **Minimize empirical risk**, to find the best estimate for the dataset that we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Empirical risk minimization\n",
    "\n",
    "- Depending on which loss function we choose, we will end up with different $h^*$ (which are estimates of $h^{\\text{true}})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we choose **squared loss**, then our empirical risk is **mean squared error**:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i = 1}^n ( y_i - h )^2 \\overset{\\text{calculus}}\\implies h^* = \\text{mean}(y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we choose **absolute loss**, then our empirical risk is **mean absolute error**:\n",
    "\n",
    "$$\\text{MAE} = \\frac{1}{n} \\sum_{i = 1}^n | y_i - h | \\overset{\\text{algebra}}\\implies h^* = \\text{median}(y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The mean tip\n",
    "\n",
    "Let's suppose we choose squared loss, meaning that $h^* = \\text{mean}(y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tip = tips['tip'].mean()\n",
    "mean_tip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, **minimizing MSE is the same as minimizing RMSE**, however RMSE has the added benefit that it is in the same units as our data. We will compute and keep track of the RMSEs of the different models we build (as we did last lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(actual, pred):\n",
    "    return np.sqrt(np.mean((actual - pred) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(tips['tip'], mean_tip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_dict = {}\n",
    "rmse_dict['constant, tip'] = rmse(tips['tip'], mean_tip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the mean minimizes RMSE for the constant model, it is **impossible** to change the `mean_tip` argument above to another number and yield a **lower** RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model #2: Tip percentages instead of tips\n",
    "\n",
    "- If we are going to make a constant prediction, a more natural constant to predict might be the tip **percentage**.\n",
    "    - We know this from domain knowledge: in the US (where this dataset was collected), it is customary to tip a percentage.\n",
    "\n",
    "* We can **derive** the `'pct_tip'` feature ourselves using existing information: $$\\texttt{pct_tip} = \\frac{\\texttt{tip}}{\\texttt{total_bill}}$$\n",
    "\n",
    "    - This is an example of quantitative scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = tips.assign(pct_tip=(tips['tip'] / tips['total_bill']))\n",
    "sns.histplot(tips['pct_tip'], kde=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The mean tip percentage\n",
    "\n",
    "- Our model is now\n",
    "\n",
    "$$\\texttt{pct_tip} = h^{\\text{true}}$$\n",
    "\n",
    "- $h^{\\text{true}}$ is the \"true fixed tip percentage\" that exists in the universe, that we can't observe.\n",
    "- To come up with an estimate of $h^{\\text{true}}$, we choose a loss function and minimize empirical risk on our observed dataset.\n",
    "- Again, we'll choose squared loss, so our estimate $h^*$ will be the **mean tip percentage** in `tips`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pct_tip = tips['pct_tip'].mean()\n",
    "mean_pct_tip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Computing the RMSE of this model is a bit more nuanced.\n",
    "- To fairly compare this model to the previous model, we must still be predicting `'tip'`, but above we have predicted `'pct_tip'`.\n",
    "- **Key idea:**, `'pct_tip'` is a **multiplier** that we apply to `'total_bill'` to get `'tip'`. That is:\n",
    "\n",
    "$$\\text{predicted tip} = \\text{total bill} \\cdot \\text{mean pct-tip}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips['total_bill'] * mean_pct_tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_dict['constant, pct_tip'] = rmse(tips['tip'], tips['total_bill'] * mean_pct_tip)\n",
    "rmse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Constant tip vs. constant tip percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pct_tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A constant prediction of 16.08\\% yields a lower RMSE than a constant prediction of \\\\$3.\n",
    "- However, both RMSEs are over \\\\$1, which is relatively high compared to the mean tip amount of \\\\$3.\n",
    "- How can we bring this RMSE down?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model #3: Linear model\n",
    "\n",
    "* **Model:** Tips are made according to a linear function:\n",
    "\n",
    "$$\\text{predicted tip} = w_0 + w_1 \\cdot \\text{tip}$$\n",
    "\n",
    "- By choosing a loss function and minimizing empirical risk, we can find $w_0^*$ and $w_1^*$.\n",
    "    - This process is **fitting** our model to the data.\n",
    "    - $w_0^*$ and $w_1^*$ can be thought of as estimates of the true intercept and slope that exist in nature.\n",
    "    \n",
    "- In order to use a linear model, the data should have a linear association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=tips, x='total_bill', y='tip');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fitting a linear model\n",
    "\n",
    "Again, we will learn more about `sklearn` in the coming lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X=tips[['total_bill']], y=tips['tip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_, lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above coefficients state that the \"best way\" (according to squared loss) to make tip predictions using a linear model is to assume people\n",
    "- Tip ~\\\\$0.92 up front, and\n",
    "- ~10.5\\% of every dollar thereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr.predict(X=tips[['total_bill']])\n",
    "rmse_dict['linear model'] = rmse(tips['tip'], preds)\n",
    "rmse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "- We built three models:\n",
    "    - A constant model: $\\text{predicted tip} = h^*$.\n",
    "    - A linear model with no intercept: $\\text{predicted tip} = w^* \\cdot \\text{total bill}$.\n",
    "        - This was the model that involved tip percentage.\n",
    "    - A linear model with an intercept: $\\text{predicted tip} = w_0^* + w_1^* \\cdot \\text{total bill}$.\n",
    "- As we added more features, our RMSEs decreased.\n",
    "    - This was guaranteed to happen, since we were only looking at our training data.\n",
    "- It is not clear that the final linear model is actually \"better\"; it doesn't seem to **reflect reality** better than the previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's next?\n",
    "\n",
    "There's a lot of information in `tips` that we didn't use ‚Äì `'sex'`, `'day'`, and `'time'`, for example. How might we **encode** this information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary, next time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- To transform a categorical ordinal variable into a quantitative variable, use an **ordinal** encoding.\n",
    "- To transform a categorical nominal variable into a quantitative variable, use **one-hot** encoding.\n",
    "- A model is an assumption about a data generating process.\n",
    "    - Models can be used for both inference and prediction.\n",
    "    - All models are wrong (because they are oversimplifications of reality), but even simple models can be useful in practice.\n",
    "- **Next time:** Finish the `tips` example. Start formally learning `sklearn`."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "livereveal": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
