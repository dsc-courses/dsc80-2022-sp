{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rc('figure', dpi=100, figsize=(7, 5))\n",
    "plt.rc('font', size=12)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 27 ‚Äì Classifier Evaluation and Fairness\n",
    "\n",
    "## DSC 80, Spring 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements\n",
    "- The Final Exam is on **Saturday, June 4th from 11:30AM-2:30PM in-person**!\n",
    "    - See [this Campuswire post](https://campuswire.com/c/G325FA25B/feed/1754) for all the details, **including seating assignments and charts**.\n",
    "    - Lectures 1-26 (including some of today's lecture), Projects 1-5, Labs 1-9, and Discussions 1-8 are all in scope.\n",
    "    - **Discussion today is office hours; a [Fall 2021 Final walkthrough video](https://www.youtube.com/watch?v=8JZ71x-gr8E) was posted.**\n",
    "- Project 5 is due on **Thursday, June 9th at 11:59PM**!\n",
    "- If at least 80% of the class fills out both [CAPEs](https://cape.ucsd.edu/) and the [End-of-Quarter Survey](https://docs.google.com/forms/d/e/1FAIpQLSepSEBy0KC1-RHGF6dixYKZ-2p3SVdiPHB9spXPlA6PZNUy4A/viewform), then everyone will receive an extra 0.5% added to their overall course grade. \n",
    "    - Deadline: **Saturday at 8AM**.\n",
    "    - Currently at ~40%.\n",
    "- The Grade Report will be updated before Friday's lecture, with Lab 8-9 and Project 4 grades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "- Classifier evaluation.\n",
    "    - Last in-scope topic for the final.\n",
    "- Example: Tumor malignancy prediction (via logistic regression).\n",
    "- Fairness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classifier evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall\n",
    "\n",
    "| | Predicted Negative | Predicted Positive |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN = 90 ‚úÖ | FP = 1 ‚ùå |\n",
    "| <span style='color:orange'><b>Actually Positive</b></span> | <span style='color:orange'>FN = 8</span> ‚ùå | <span style='color:orange'>TP = 1</span> ‚úÖ |\n",
    "\n",
    "<center><i><small>UCSD Health test results</small></i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "ü§î **Question:** What proportion of individuals who actually have COVID did the test **identify**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**üôã Answer:** $\\frac{1}{1 + 8} = \\frac{1}{9} \\approx 0.11$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "More generally, the **recall** of a binary classifier is the proportion of <span style='color:orange'><b>actually positive instances</b></span> that are correctly classified. We'd like this number to be as close to 1 (100%) as possible.\n",
    "\n",
    "$$\\text{recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "To compute recall, look at the <span style='color:orange'><b>bottom (positive) row</b></span> of the above confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall isn't everything, either!\n",
    "\n",
    "$$\\text{recall} = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "ü§î **Question:** Can you design a \"COVID test\" with perfect recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**üôã Answer:** Yes ‚Äì **just predict that everyone has COVID!**\n",
    "\n",
    "| | Predicted Negative | Predicted Positive |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN = 0 ‚úÖ | FP = 91 ‚ùå |\n",
    "| <span style='color:orange'><b>Actually Positive</b></span> | <span style='color:orange'>FN = 0</span> ‚ùå | <span style='color:orange'>TP = 9</span> ‚úÖ |\n",
    "\n",
    "<center><i><small>everyone-has-COVID classifier</small></i></center>\n",
    "\n",
    "\n",
    "$$\\text{recall} = \\frac{TP}{TP + FN} = \\frac{9}{9 + 0} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Like accuracy, recall on its own is not a perfect metric. Even though the classifier we just created has perfect recall, it has 91 false positives!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision\n",
    "\n",
    "| | Predicted Negative | <span style='color:orange'>Predicted Positive</span> |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN = 0 ‚úÖ | <span style='color:orange'>FP = 91</span> ‚ùå |\n",
    "| **Actually Positive** | FN = 0 ‚ùå | <span style='color:orange'>TP = 9</span> ‚úÖ |\n",
    "\n",
    "<center><i><small>everyone-has-COVID classifier</small></i></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The **precision** of a binary classifier is the proportion of <span style='color:orange'><b>predicted positive instances</b></span> that are correctly classified. We'd like this number to be as close to 1 (100%) as possible.\n",
    "\n",
    "$$\\text{precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "To compute precision, look at the <span style='color:orange'><b>right (positive) column</b></span> of the above confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Tip:** A good way to remember the difference between precision and recall is that in the denominator for üÖøÔ∏èrecision, both terms have üÖøÔ∏è in them (TP and FP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that the \"everyone-has-COVID\" classifier has perfect recall, but a precision of $\\frac{9}{9 + 91} = 0.09$, which is quite low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- üö® **Key idea:** There is a \"tradeoff\" between precision and recall. For a particular prediction task, one may be important than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision and recall\n",
    "\n",
    "<center><img src=\"imgs/Precisionrecall.svg.png\" width=30%></center>\n",
    "\n",
    "<center>(<a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\">source</a>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision and recall\n",
    "\n",
    "$$\\text{precision} = \\frac{TP}{TP + FP} \\: \\: \\: \\:  \\: \\: \\: \\: \\text{recall} = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "ü§î **Question:** When might high **precision** be more important than high recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**üôã Answer:** For instance, in deciding whether or not someone committed a crime. Here, **false positives are really bad** ‚Äì they mean that an innocent person is charged!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "ü§î **Question:** When might high **recall** be more important than high precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**üôã Answer:** For instance, in medical tests. Here, **false negatives are really bad** ‚Äì they mean that someone's disease goes undetected!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discussion Question\n",
    "\n",
    "Consider the confusion matrix shown below.\n",
    "\n",
    "| | Predicted Negative | Predicted Positive |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN = 22 ‚úÖ | FP = 2 ‚ùå |\n",
    "| **Actually Positive** | FN = 23 ‚ùå | TP = 18 ‚úÖ |\n",
    "\n",
    "What is the accuracy of the above classifier? The precision? The recall?\n",
    "\n",
    "<br>\n",
    "\n",
    "After calculating all three on your own, click below to see the answers.\n",
    "\n",
    "<details>\n",
    "    <summary>Accuracy</summary>\n",
    "    (22 + 18) / (22 + 2 + 23 + 18) = 40 / 65\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary>Precision</summary>\n",
    "    18 / (18 + 2) = 9 / 10\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary>Recall</summary>\n",
    "    18 / (18 + 23) = 18 / 41\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "End of Final Exam content! üéâ\n",
    "\n",
    "(Note that the remaining content is still relevant for Project 5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Tumor malignancy prediction (via logistic regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wisconsin breast cancer dataset\n",
    "\n",
    "The Wisconsin breast cancer dataset (WBCD) is a commonly-used dataset for demonstrating binary classification. It is built into `sklearn.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "loaded = load_breast_cancer() # explore the value of `loaded`!\n",
    "data = loaded['data']\n",
    "labels = 1 - loaded['target']\n",
    "cols = loaded['feature_names']\n",
    "bc = pd.DataFrame(data, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 stands for \"malignant\", i.e. cancerous, and 0 stands for \"benign\", i.e. safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(labels).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to use the features in `bc` to predict `labels`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: Logistic regression\n",
    "\n",
    "Logistic _regression_ is a linear _classification?_ technique that builds upon linear regression. It models **the probability of belonging to class 1, given a feature vector**:\n",
    "\n",
    "$$P(y = 1 | \\vec{x}) = \\sigma (\\underbrace{w_0 + w_1 x^{(1)} + w_2 x^{(2)} + ... + w_d x^{(d)}}_{\\text{linear regression model}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here, $\\sigma(t) = \\frac{1}{1 + e^{-t}}$ is the **sigmoid** function; its outputs are between 0 and 1 (which means they can be interpreted as probabilities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "ü§î **Question:** Suppose our logistic regression model predicts the probability that a tumor is malignant is 0.75. What class do we predict ‚Äì malignant or benign? What if the predicted probability is 0.3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "üôã **Answer:** We have to pick a threshold (e.g. 0.5)!\n",
    "- If the predicted probability is above the threshold, we predict malignant (1).\n",
    "- Otherwise, we predict benign (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fitting a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bc, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did `clf` come up with 1s and 0s?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the predicted labels come from applying a **threshold** of 0.5 to the predicted probabilities. We can access the predicted probabilities via the `predict_proba` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [:, 1] refers to the predicted probabilities for class 1\n",
    "clf.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our model still has $w^*$s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating our model\n",
    "\n",
    "Let's see how well our model does on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which metric is more important for this task ‚Äì precision or recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_confusion_matrix(clf, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What if we choose a different threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "ü§î **Question:** Suppose we choose a threshold **higher** than 0.5. What will happen to our model's precision and recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "üôã **Answer:** Precision will increase, while recall will decrease*.\n",
    "- If the \"bar\" is higher to predict 1, then we will have fewer false positives. \n",
    "- The denominator in $\\text{precision} = \\frac{TP}{TP + FP}$ will get smaller, and so precision will increase.\n",
    "- However, the number of false negatives will increase, as we are being more \"strict\" about what we classify as positive, and so $\\text{recall} = \\frac{TP}{TP + FN}$ will decrease.\n",
    "- *It is possible for either or both to stay the same, if changing the threshold slightly (e.g. from 0.5 to 0.500001) doesn't change any predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Similarly, if we decrease our threshold, our model's precision will decrease, while its recall will increase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Trying several thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The classification threshold is not actually a hyperparameter of `LogisticRegression`, because the threshold doesn't change the coefficients ($w^*$s) of the logistic regression model itself (see [this article](https://stats.stackexchange.com/questions/390186/is-decision-threshold-a-hyperparameter-in-logistic-regression#:~:text=The%20decision%20threshold%20is%20not,how%20hyper%2Dparameters%20are%20tuned.) for more details).\n",
    "\n",
    "As such, if we want to imagine how our predicted classes would change with thresholds other than 0.5, we need to manually threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "precisions = np.array([])\n",
    "recalls = np.array([])\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = clf.predict_proba(X_test)[:, 1] >= t\n",
    "    precisions = np.append(precisions, metrics.precision_score(y_test, y_pred))\n",
    "    recalls = np.append(recalls, metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results in `plotly`, which is interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=thresholds, y=precisions,\n",
    "        labels={'x': 'Threshold', 'y': 'Precision'}, title='Precision vs. Threshold', width=1000, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=thresholds, y=recalls, \n",
    "        labels={'x': 'Threshold', 'y': 'Recall'}, title='Recall vs. Threshold', width=1000, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=recalls, y=precisions, hover_name=thresholds, \n",
    "        labels={'x': 'Recall', 'y': 'Precision'}, title='Precision vs. Recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above curve is called a precision-recall (or PR) curve.\n",
    "\n",
    "ü§î **Question:** Based on the PR curve above, what threshold would you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Combining precision and recall\n",
    "\n",
    "If we care equally about a model's precision $PR$ and recall $RE$, we can combine the two using a single metric called the **F1-score**:\n",
    "\n",
    "$$\\text{F1-score} = \\text{harmonic mean}(PR, RE) = 2\\frac{PR \\cdot RE}{PR + RE}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = metrics.precision_score(y_test, clf.predict(X_test))\n",
    "re = metrics.recall_score(y_test, clf.predict(X_test))\n",
    "\n",
    "2 * pr * re / (pr + re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both F1-score and accuracy are overall measures of a binary classifier's performance. But remember, accuracy is misleading in the presence of class imbalance, and doesn't take into account the kinds of errors the classifier makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other evaluation metrics for binary classifiers\n",
    "\n",
    "We just scratched the surface! This [excellent table from Wikipedia](https://en.wikipedia.org/wiki/Template:Diagnostic_testing_diagram) summarizes the many other metrics that exist.\n",
    "\n",
    "<center><img src='imgs/wiki-table.png' width=75%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested in exploring further, a good next metric to look at is **true negative rate (i.e. specificity)**, which is the analogue of recall for true negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall, from Lecture 1\n",
    "\n",
    "<center><img src='imgs/depixel2.png' width=60%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fairness: why do we care?\n",
    "\n",
    "- Sometimes, a model performs better for certain groups than others; in such cases we say the model is **unfair**.\n",
    "- Since ML models are now used in processes that significantly affect human lives, it is important that they are fair!\n",
    "    * Job applications and college admissions.\n",
    "    * Criminal sentencing and parole grants.\n",
    "    * Predictive policing.\n",
    "    * Credit and loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: COMPAS and recidivism prediction\n",
    "\n",
    "COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) is a \"black-box\" model that estimates the likelihood that someone who has commited a crime will recidivate (commit another crime).\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"imgs/compas.jpeg\"></center>\n",
    "\n",
    "[Propublica found](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) that the model's false positive rate is higher for African-Americans than it is for White Americans, and that its false negative rate is lower for African-Americans than it is for White Americans.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Facial recognition\n",
    "\n",
    "* The table below comes from [a paper](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf) that analyzes several \"gender classifiers\", and shows that popular classifiers perform much worse for women and those with darker skin colors.\n",
    "* Police departments are beginning to [use these models](https://www.usatoday.com/story/tech/2018/07/09/orlando-police-decide-keep-testing-amazon-facial-recognition-program/768507002/) for surveillance.\n",
    "* Self-driving cars use similar models to recognize pedestrians!\n",
    "\n",
    "<center><img src=\"imgs/imgnet.jpeg\" width=60%></center>\n",
    "\n",
    "Note:\n",
    "\n",
    "$$PPV = \\text{precision} = \\frac{TP}{TP+FP},\\:\\:\\:\\:\\:\\: TPR = \\text{recall} = \\frac{TP}{TP + FN}, \\:\\:\\:\\:\\:\\: FPR = \\frac{FP}{FP+TN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does bias occur?\n",
    "\n",
    "Remember, our models learn patterns from the training data. Various sources of bias may be present within training data:\n",
    "* Training data may not be representative of the population.\n",
    "    * There may be fewer data points for minority groups, leading to poorer model performance.\n",
    "* The features chosen may be more useful in making predictions for certain groups than others.\n",
    "* Training data may encode existing human biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Gender associations\n",
    "\n",
    "- English is not a gendered language ‚Äì words like \"teacher\" and \"scientist\" are not inherently gendered (unlike in, say, French). \n",
    "- However, English does have gendered pronouns (e.g. \"he\", \"she\").\n",
    "- Humans subconsciously associate certain words with certain genders.\n",
    "- What gender does English associate the following words with?\n",
    "\n",
    "<center>soldier, teacher, nurse, doctor, dog, cat, president, nanny</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Gender associations\n",
    "\n",
    "* Unlike English, Turkish üáπüá∑ **does not** have gendered pronouns ‚Äì there is only a single, gender-neutral pronoun (\"o\").\n",
    "* Let's see what happens when we use Google Translate to translate Turkish sentences that **should be** gender-neutral back to English.\n",
    "* Click [this link](https://translate.google.com/?sl=tr&tl=en&text=o%20bir%20asker%20%0Ao%20bir%20√∂ƒüretmen%0Ao%20bir%20m√ºhendis%0Ao%20bir%20hem≈üire&op=translate) to follow along.\n",
    "* Why is this happening?\n",
    "    * Answer: Google Translate is \"trained\" on a large corpus of English text, and these associations are present in those English texts.\n",
    "    * Ideally, the results should contain a gender-neutral singular \"they\", rather than \"he\" or \"she\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Image searches\n",
    "\n",
    "A 2015 study examined the image queries of vocations and the gender makeup in the search results. Since 2015, the behavior of Google Images has been improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In 2015, a Google Images search for \"**nurse**\" returned...\n",
    "\n",
    "<center><img src='imgs/nurses2015.jpg'></center>\n",
    "\n",
    "Search for \"nurse\" now, what do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In 2015, a Google Images search for \"**doctor**\" returned...\n",
    "\n",
    "<center><img src='imgs/doctors2015.jpg'></center>\n",
    "\n",
    "Search for \"doctor\" now, what do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ethics: What gender ratio _should_ we expect in the results?\n",
    "\n",
    "- Should it be 50/50?\n",
    "- Should it reflect the true gender distribution of those jobs?\n",
    "- More generally, what do you expect from your search results?\n",
    "    - This is a philosophical and ethical question, but one that **we need to think about as data scientists**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='imgs/google-photos-paper.png' width=70%></center>\n",
    "\n",
    "Excerpts:\n",
    "\n",
    "> \"male-dominated professions tend to have even more men\n",
    "in their results than would be expected if the proportions\n",
    "reflected real-world distributions.\n",
    "\n",
    "> \"People‚Äôs existing perceptions of gender ratios in occupations\n",
    "are quite accurate, but that manipulated search results have an effect on perceptions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How did this unequal representation occur?\n",
    "\n",
    "* The training data that Google Images searches from encoded existing biases.\n",
    "    - While 60% of doctors may be male, 80% of photos (including stock photos) of doctors on the internet may be of male doctors.\n",
    "* Models (like PageRank) that \"rank\" images find the, say, 5 \"most relevant\" image, not the 5 \"most typical\" images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary, next time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- Accuracy alone is not always a meaningful representation of a classifier's quality, particularly when the **classes are imbalanced**.\n",
    "    - Precision and recall are classifier evaluation metrics that consider the **types of errors** being made.\n",
    "    - There is a \"tradeoff\" between precision and recall. One may be more important than the other, depending on the task.\n",
    "- A logistic regression model makes classifications by first predicting a probability and then thresholding that probability.\n",
    "    - The default threshold is 0.5; by moving the threshold, we change the balance between precision and recall.\n",
    "- A model is **unfair** if it performs better for some groups than others.\n",
    "    - Unfairness often arises through biased training data, which encodes existing human biases.\n",
    "- **Next time:** A mathematical framework for assessing the unfairness of a model, and a practical example. High-level overview of the quarter."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "livereveal": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
