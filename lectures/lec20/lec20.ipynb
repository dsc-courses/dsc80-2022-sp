{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import re\n",
    "import util\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rc('figure', dpi=100, figsize=(7, 5))\n",
    "plt.rc('font', size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 20 ‚Äì Features\n",
    "\n",
    "## DSC 80, Spring 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements\n",
    "\n",
    "- Lab 7 is due on **Monday, May 16th at 11:59PM**.\n",
    "- üì£ Come to the DSC **Town Hall**, where you can voice your feedback about the DSC program to faculty. \n",
    "    - Tuesday, May 16th from 3-5PM in the SDSC Auditorium.\n",
    "    - [RSVP by **noon on Friday** to secure **free pizza üçï**!](https://docs.google.com/forms/d/e/1FAIpQLScfP_EFEYt1d5N7dWXGQqQaOik3nY_KTIMYuB1uuEgjH83vRw/viewform)\n",
    "- Project 4 will be released over the weekend üëÄ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "- Recap: TF-IDF.\n",
    "- Features.\n",
    "- Example: Predicting child heights üìè."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term frequency-inverse document frequency\n",
    "\n",
    "The **term frequency-inverse document frequency (TF-IDF)** of word $t$ in document $d$ is the product:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\\text{tfidf}(t, d) &= \\text{tf}(t, d) \\cdot \\text{idf}(t) \\\\\\ &= \\frac{\\text{number of occurrences of $t$ in $d$}}{\\text{total number of words in $d$}} \\cdot \\log \\left(\\frac{\\text{total number of documents}}{\\text{number of documents in which $t$ appears}} \\right) \\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $\\text{tfidf}(t, d)$ is large, then $t$ is a good summary of $d$.\n",
    "    - But to know if $\\text{tfidf}(t, d)$ is large, we need to compare it to $\\text{tfidf}(t_i, d)$, for several different words $t_i$.\n",
    "\n",
    "- TF-IDF is a **heuristic** ‚Äì it has no probabilistic justification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: State of the Union addresses üé§\n",
    "\n",
    "Recall, last class, we computed the TF-IDF for every word and every SOTU speech. We used TF-IDFs to **summarize** speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_struct(speech):\n",
    "    L = speech.strip().split('\\n', maxsplit=3)\n",
    "    L[3] = re.sub(r\"[^A-Za-z' ]\", ' ', L[3]).lower()\n",
    "    return dict(zip(['speech', 'president', 'date', 'contents'], L))\n",
    "\n",
    "def five_largest(row):\n",
    "    return list(row.index[row.argsort()][-5:])\n",
    "\n",
    "sotu = open('data/stateoftheunion1790-2022.txt').read()\n",
    "speeches = sotu.split('\\n***\\n')[1:]\n",
    "speeches_df = pd.DataFrame(list(map(extract_struct, speeches)))\n",
    "unique_words = pd.Series(speeches_df['contents'].str.split().sum()).value_counts()\n",
    "unique_words = unique_words.iloc[:500].index\n",
    "\n",
    "tfidf_dict = {}\n",
    "tf_denom = speeches_df['contents'].str.split().str.len()\n",
    "for word in unique_words:\n",
    "    re_pat = fr' {word} ' # Imperfect pattern for speed\n",
    "    tf = speeches_df['contents'].str.count(re_pat) / tf_denom\n",
    "    idf = np.log(len(speeches_df) / speeches_df['contents'].str.contains(re_pat).sum())\n",
    "    tfidf_dict[word] =  tf * idf\n",
    "    \n",
    "tfidf = pd.DataFrame(tfidf_dict)\n",
    "\n",
    "keywords = tfidf.apply(five_largest, axis=1)\n",
    "keywords_df = pd.concat([\n",
    "    speeches_df['president'],\n",
    "    speeches_df['date'],\n",
    "    keywords\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keywords_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: What if we remove the $\\log$ from $\\text{idf}(t)$?\n",
    "\n",
    "Let's try it and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_nl_dict = {}\n",
    "tf_denom = speeches_df['contents'].str.split().str.len()\n",
    "for word in unique_words:\n",
    "    re_pat = fr' {word} ' # Imperfect pattern for speed\n",
    "    tf = speeches_df['contents'].str.count(re_pat) / tf_denom\n",
    "    idf_nl = len(speeches_df) / speeches_df['contents'].str.contains(re_pat).sum()\n",
    "    tfidf_nl_dict[word] =  tf * idf_nl\n",
    "    \n",
    "tfidf_nl = pd.DataFrame(tfidf_nl_dict)\n",
    "\n",
    "keywords_nl = tfidf_nl.apply(five_largest, axis=1)\n",
    "keywords_nl_df = pd.concat([\n",
    "    speeches_df['president'],\n",
    "    speeches_df['date'],\n",
    "    keywords_nl\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_nl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The role of $\\log$ in $\\text{idf}(t)$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\\text{tfidf}(t, d) &= \\text{tf}(t, d) \\cdot \\text{idf}(t) \\\\\\ &= \\frac{\\text{number of occurrences of $t$ in $d$}}{\\text{total number of words in $d$}} \\cdot \\log \\left(\\frac{\\text{total number of documents}}{\\text{number of documents in which $t$ appears}} \\right) \\end{align*} $$\n",
    "\n",
    "- Remember, for any positive input $x$, $\\log(x)$ is (much) smaller than $x$.\n",
    "- In $\\text{idf}(t)$, the $\\log$ \"dampens\" the impact of the ratio $\\frac{\\text{# documents}}{\\text{# documents with $t$}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If a word is very common, the ratio will be close to 1. The log of the ratio will be close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1000 / 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(1000 / 999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If a word is very rare, the ratio will be very large. However, for instance, a word being seen in **2 out of 50** documents is not very different than being seen in **2 out of 500** documents (it is very rare in both cases), and so $\\text{idf}(t)$ should be similar in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(50 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(500 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(50 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(500 / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='imgs/DSLC.png' width=50%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reflection\n",
    "\n",
    "So far this quarter, we've learned how to:\n",
    "\n",
    "- Extract information from tabular data using `pandas` and regular expressions.\n",
    "- Clean data so that it best represents a data generating process.\n",
    "    - Missingness analyses and imputation.\n",
    "- Collect data from the internet through scraping and APIs, and parse it using BeautifulSoup.\n",
    "- Perform exploratory data analysis through aggregation, visualization, and the computation of summary statistics like TF-IDF.\n",
    "- Infer about the relationships between samples and populations through hypothesis and permutation testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **We haven't** learned how to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Features\n",
    "\n",
    "* A **feature** is a measurable property or characteristic of a phenomenon being observed.\n",
    "    * Other words for \"feature\" include \"(explanatory) variable\" and \"attribute\".\n",
    "* In DataFrames, features typically correspond to **columns**, while rows typically correspond to different individuals.\n",
    "* There are two types of features:\n",
    "    * Features that come as part of a dataset, e.g. weight and height.\n",
    "    * Features that we **create**, e.g. $\\text{BMI} = \\frac{\\text{weight (kg)}}{\\text{[height (m)]}^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Note:** TF-IDF is a **feature** we've created that summarizes documents!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: San Diego employee salaries\n",
    "\n",
    "What features are present in `salaries`? What features can we create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries = pd.read_csv('https://transcal.s3.amazonaws.com/public/export/san-diego-2020.csv')\n",
    "util.anonymize_names(salaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Employee salaries.\n",
    "    - This feature came with the dataset.\n",
    "- Employee salaries, standardized by job status.\n",
    "    - We'd need to compute this feature, using information that is already in `salaries`.\n",
    "- Employee genders.\n",
    "    - We'd need to merge `salaries` with another data source, like the SSA baby names dataset, to create this feature.\n",
    "    - How accurate would the resulting feature be?\n",
    "- Job \"category\".\n",
    "    - We could compute this using TF-IDF (which would allow us to find the most important word in each job title)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What makes a good feature?\n",
    "\n",
    "- A good feature should be...\n",
    "\n",
    "    - Faithful to the data generating process.\n",
    "    - Strongly associated to the phenomenon of interest.\n",
    "    - Easily used in standard modeling techniques (e.g. quantitative and scaled).\n",
    "    \n",
    "- Often times, the columns in a dataset aren't good features on their own. In such cases, we may need to \"engineer\" features that are useful.\n",
    "    - Useful for what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Predicting child heights üìè"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Galton's heights dataset\n",
    "\n",
    "- When studying missingness, we worked with a dataset containing the heights of children and their parents.\n",
    "- The dataset was collected by Francis Galton, the founder of eugenics. \n",
    "- He was interested in **predicting a child's height**, given various attributes (father's height, mother's height, child gender, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galton = pd.read_csv('data/galton.csv')\n",
    "galton.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exploratory data analysis\n",
    "\n",
    "The following **scatter matrix** contains a scatter plot of all pairs of quantitative attributes, and a histogram for each quantitative attribute on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(galton, figsize=(12, 8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is a linear model suitable for prediction? If so, on which attributes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Attempt #1: Predict child's height using father's height\n",
    "\n",
    "We will assume that the relationship between father's heights and child's heights is linear. That is,\n",
    "\n",
    "$$\\text{predicted child's height} = w_0^* + w_1^* \\cdot \\text{father's height}$$\n",
    "\n",
    "where $w_0^*$ and $w_1^*$ are carefully chosen **parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`seaborn`'s `lmplot` function can automatically plot the \"line of best fit\" on a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=galton, x='father', y='childHeight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: Simple linear regression\n",
    "\n",
    "For any father's height $x_i$, their predicted child's height is given by\n",
    "\n",
    "$$H(x_i) = w_0 + w_1x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question:** How do we determine which intercept, $w_0$, and slope, $w_1$, to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **One answer:** Pick the $w_0$ and $w_1$ that minimize **mean squared error**. If $x_i$ and $y_i$ correspond to the $i$th father's height and child's height, respectively, then:\n",
    "\n",
    "$$\\begin{align*}\\text{MSE} &= \\frac{1}{n} \\sum_{i = 1}^n \\big( y_i - H(x_i) \\big)^2\n",
    "\\\\ &= \\frac{1}{n} \\sum_{i = 1}^n \\big( y_i - w_0 - w_1x_i \\big)^2\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In DSC 40A, you found the formulas for the best intercept, $w_0^*$, and the best slope, $w_1^*$, through calculus. \n",
    "    - The resulting line, $H(x_i) = w_0^* + w_1^* x_i$, is called the **line of best fit**, or the **regression line**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Specifically, if $r$ is the correlation coefficient, $\\sigma_x$ and $\\sigma_y$ are the standard deviations of $x$ and $y$, and $\\bar{x}$ and $\\bar{y}$ are the means of $x$ and $y$, then:\n",
    "\n",
    "$$w_1^* = r \\cdot \\frac{\\sigma_y}{\\sigma_x}$$\n",
    "\n",
    "$$w_0^* = \\bar{y} - w_1^* \\bar{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Key idea: The lower the MSE is, the \"better\" the model fits the _training_ data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding the regression line programatically\n",
    "\n",
    "There are several packages that can perform linear regression; `scipy.stats` is one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linregress(x=galton['father'], y=galton['childHeight'])\n",
    "lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lm` object has several attributes, most notably, `slope` and `intercept.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_child(father):\n",
    "    return lm.intercept + lm.slope * father"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pred_child` words on scalar values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_child(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it also works on arrays/Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_child(galton['father'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, a lower MSE means a better fit on the training data. Let's compute the MSE of this simple linear model; it will be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(actual, pred):\n",
    "    return np.mean((actual - pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(galton['childHeight'], pred_child(galton['father']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: MSE vs. RMSE\n",
    "\n",
    "An issue with mean squared error is that its units are the **square** of the units of the $y$-values.\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i = 1}^n \\big( y_i - H(x_i) \\big)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, the number below is 11.892 \"inches squared\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(galton['childHeight'], pred_child(galton['father']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To correct the units of mean squared error, we can take the square root. The result, **root mean squared error (RMSE)** is also a measure of how well a model fits training data.\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i = 1}^n \\big( y_i - H(x_i) \\big)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Important:** The line that minimizes MSE is the same line that minimizes RMSE and SSE (sum of squared errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def rmse(actual, pred):\n",
    "    return np.sqrt(np.mean((actual - pred) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dictionary to keep track of the RMSEs of the various models we create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_dict = {}\n",
    "rmse_dict['father only'] = rmse(galton['childHeight'], pred_child(galton['father']))\n",
    "rmse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing our single-feature predictions\n",
    "\n",
    "- How well does our linear model capture the underlying relationship between the heights of fathers and their children?\n",
    "- What improvements can we make to our linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=galton, x='father', y='childHeight', label='actual child heights')\n",
    "sns.scatterplot(x=galton['father'], \n",
    "                y=pred_child(galton['father']), \n",
    "                label='predicted child heights'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Attempt #2: Predict child's height using father's and mother's heights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What if the father is very tall and the mother is very short?\n",
    "* Adding mother's height as a **feature** should help our predictions.\n",
    "* When performing linear regression with two features, the result is a **plane of best fit**.\n",
    "\n",
    "$$\\text{predicted child's height} = w_0^* + w_1^* \\cdot \\text{father's height} + w_2^* \\cdot \\text{mother's height}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiple regression in `sklearn`\n",
    "\n",
    "We'll cover `sklearn` in more detail in the coming lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical pattern in `sklearn` is instantiate, fit, and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X=galton[['father', 'mother']], y=galton['childHeight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "After calling `fit` on `lr`, we can access the intercept and coefficients of the plane of best fit (i.e. these are $w_0^*$, $w_1^*$, and $w_2^*$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_, lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, we don't actually need to access these directly. Fit `LinearRegression` objects have the `predict` method, which we can use directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr.predict(galton[['father', 'mother']])\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How well does this model perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_dict['father and mother'] = rmse(galton['childHeight'], predictions)\n",
    "rmse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like this two-feature model has a lower RMSE than the original single-feature model (which we'd expect), but it's only slightly lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing our two-feature predictions\n",
    "\n",
    "Here, we must draw a 3D scatter plot and plane, with one axis for father's height, one axis for mother's height, and one axis for child's height. The code below does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX, YY = np.mgrid[60:80:2, 55:75:2]\n",
    "Z = lr.intercept_ + lr.coef_[0] * XX + lr.coef_[1] * YY\n",
    "plane = go.Surface(x=XX, y=YY, z=Z, colorscale='Oranges')\n",
    "\n",
    "fig = go.Figure(data=[plane])\n",
    "fig.add_trace(go.Scatter3d(x=galton['father'], \n",
    "                           y=galton['mother'], \n",
    "                           z=galton['childHeight'], mode='markers', marker = {'color': '#656DF1'}))\n",
    "\n",
    "fig.update_layout(scene = dict(\n",
    "    xaxis_title = 'father',\n",
    "    yaxis_title = 'mother',\n",
    "    zaxis_title = 'child'),\n",
    "    width=1000, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to visualize in 2D, we must pick a single feature to display on the $x$-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=galton, x='father', y='childHeight', label='actual child heights')\n",
    "sns.scatterplot(x=galton['father'], \n",
    "                y=predictions, \n",
    "                label='predicted child heights using father and mother'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=galton, x='mother', y='childHeight', label='actual child heights')\n",
    "sns.scatterplot(x=galton['mother'], \n",
    "                y=predictions, \n",
    "                label='predicted child heights using father and mother'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Attempt #3: Adding gender as a feature\n",
    "\n",
    "- In Attempt #2, the predicted height of a child depended only on their father's height and mother's height.\n",
    "- However, we'd expect children of different genders to be of different heights, even for a fixed set of parent's heights.\n",
    "    - For instance, sisters are usually shorter than brothers.\n",
    "- Is this theory substantiated by the data?\n",
    "    - To check, we can start by plotting separate regression lines for each gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(data=galton, x='father', y='childHeight', hue='gender', \n",
    "           palette={'male': 'purple', 'female': 'green'});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: It appears that the two lines have similar slopes, but different intercepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Attempt #3: Adding gender as a feature\n",
    "\n",
    "There's an issue: gender is a categorical feature, but in order to use it as a feature in a regression model, it must be quantitative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galton.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Solution:** Create a column named `'gender=female'`, that is\n",
    "- 1 when `'gender'` is `'female'`, and\n",
    "- 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galton['gender=female'] = (galton['gender'] == 'female').astype(int)\n",
    "galton.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, we can use `'gender=female'` as a feature, just as we used `'father'` and `'mother'` as features.\n",
    "\n",
    "$$\\text{predicted child's height} \\\\ = w_0^* + w_1^* \\cdot \\text{father's height} + w_2^* \\cdot \\text{mother's height} + w_3^* \\cdot \\text{gender=female}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_three_features = LinearRegression()\n",
    "lr_three_features.fit(galton[['father', 'mother', 'gender=female']], galton['childHeight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_three_features = lr_three_features.predict(galton[['father', 'mother', 'gender=female']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_dict['father, mother, and gender'] = rmse(galton['childHeight'], predictions_three_features)\n",
    "rmse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE of our new three feature model is significantly lower than the RMSEs of the earlier models. This indicates that `'gender=female'` is very useful in predicting child's heights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing our three-feature predictions\n",
    "\n",
    "To visualize our data and linear model, we'd need 4 dimensions:\n",
    "- One for father's height.\n",
    "- One for mother's height.\n",
    "- One for `'gender=female'`.\n",
    "- One for child's height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Humans can't visualize in 4D, but there may be a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_three_features.intercept_, lr_three_features.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we are given the values of $w_0^*$, $w_1^*$, $w_2^*$, and $w_3^*$. This means our linear model is of the form:\n",
    "\n",
    "$$\\text{predicted child's height} \\\\ = 21.736 + 0.393 \\cdot \\text{father's height} + 0.318 \\cdot \\text{mother's height} - 5.215 \\cdot \\text{gender=female}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But remember, `'gender=female'` is either 1 or 0. Let's look at those two cases separately.\n",
    "\n",
    "- **For female children:**\n",
    "\n",
    "$$\\text{predicted child's height} = 16.521 + 0.393 \\cdot \\text{father's height} + 0.318 \\cdot \\text{mother's height}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **For male children:**\n",
    "\n",
    "$$\\text{predicted child's height} = 21.736 + 0.393 \\cdot \\text{father's height} + 0.318 \\cdot \\text{mother's height}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- These are really two **parallel planes** in 3D, with different $z$-intercepts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "XX, YY = np.mgrid[60:80:2, 55:75:2]\n",
    "Z_female = (lr_three_features.intercept_ + lr_three_features.coef_[2]) + lr_three_features.coef_[0] * XX + lr_three_features.coef_[1] * YY\n",
    "Z_male = lr_three_features.intercept_ + lr_three_features.coef_[0] * XX + lr_three_features.coef_[1] * YY\n",
    "\n",
    "plane_female = go.Surface(x=XX, y=YY, z=Z_female, colorscale ='Greens')\n",
    "plane_male = go.Surface(x=XX, y=YY, z=Z_male, colorscale='Purples')\n",
    "\n",
    "fig = go.Figure(data=[plane_female, plane_male])\n",
    "\n",
    "galton_female = galton[galton['gender'] == 'female']\n",
    "galton_male = galton[galton['gender'] == 'male']\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=galton_female['father'], \n",
    "                           y=galton_female['mother'], \n",
    "                           z=galton_female['childHeight'], mode='markers', marker = {'color': 'green'}))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=galton_male['father'], \n",
    "                           y=galton_male['mother'], \n",
    "                           z=galton_male['childHeight'], mode='markers', marker = {'color': 'purple'}))\n",
    "\n",
    "fig.update_layout(scene = dict(\n",
    "    xaxis_title = 'father',\n",
    "    yaxis_title = 'mother',\n",
    "    zaxis_title = 'child'),\n",
    "    width=1000, height=800,\n",
    "    showlegend=False,\n",
    "    title=\"Predicted child's heights given parents' heights and gender (purple=male, green=female)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to visualize in 2D, we must pick a single feature to display on the $x$-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=galton, x='father', y='childHeight', label='actual child heights')\n",
    "sns.scatterplot(x=galton['father'], \n",
    "                y=predictions_three_features, \n",
    "                label='predicted child heights using father, mother, and gender'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary, next time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- The $\\log$ is necessary in computing $\\text{idf}(t)$; without it, the inverse document frequency is overemphasized in TF-IDF and the resulting scores are not as meaningful.\n",
    "- A feature is a measurable property or characteristic of a phenomenon being observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Next time: feature engineering\n",
    "\n",
    "- Next class, we will learn more about **feature engineering**.\n",
    "- When we created the `'gender=female'` column in `galton`, we **engineered** a feature that we thought would be useful for our model.\n",
    "- More generally, **feature engineering** is the act of finding transformations that transform data into effective **quantitative variables**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question:** How do we decide what features to create?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "livereveal": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
