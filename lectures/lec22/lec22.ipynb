{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rc('figure', dpi=100, figsize=(7, 5))\n",
    "plt.rc('font', size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 22 ‚Äì Modeling and `sklearn`\n",
    "\n",
    "## DSC 80, Spring 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements\n",
    "\n",
    "- Discussion 8 (regression) is today from 7-8:30PM.\n",
    "- Project 4 has been released!\n",
    "    - The checkpoint is due **tomorrow at 11:59PM**.\n",
    "    - The full project is due **Thursday, May 26th at 11:59PM**.\n",
    "    - Start early!\n",
    "- Lab 8 is due on **Monday, May 23rd at 11:59PM**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "- Example: Restaurant tips üßë‚Äçüç≥.\n",
    "- `sklearn` overview.\n",
    "- Transformers in `sklearn`.\n",
    "- Models in `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Restaurant tips üßë‚Äçüç≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tips = sns.load_dataset('tips')\n",
    "tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model #1: Constant\n",
    "\n",
    "The first model we looked at last class used a constant tip **amount** prediction for every table.\n",
    "\n",
    "$$\\text{predicted tip} = h^*$$\n",
    "\n",
    "If we use squared loss, the \"best prediction is the mean of the observed tips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tip = tips['tip'].mean()\n",
    "mean_tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(actual, pred):\n",
    "    return np.sqrt(np.mean((actual - pred) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(tips['tip'], mean_tip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_dict = {}\n",
    "rmse_dict['constant, tip'] = rmse(tips['tip'], mean_tip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model #2: Tip percentages instead of tips\n",
    "\n",
    "The next model we created used a constant tip **percentage** prediction for every table.\n",
    "\n",
    "$$\\text{predicted tip percentage} = h^*$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = tips.assign(pct_tip=(tips['tip'] / tips['total_bill']))\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pct_tip = tips['pct_tip'].mean()\n",
    "mean_pct_tip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Computing the RMSE of this model is a bit more nuanced.\n",
    "- To fairly compare this model to the previous model, we must still be predicting `'tip'`, but above we have predicted `'pct_tip'`.\n",
    "- **Key idea:** `'pct_tip'` is a **multiplier** that we apply to `'total_bill'` to get `'tip'`. That is:\n",
    "\n",
    "$$\\text{predicted tip} = \\text{total bill} \\cdot \\text{mean pct-tip}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips['total_bill'] * mean_pct_tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_dict['constant, pct_tip'] = rmse(tips['tip'], tips['total_bill'] * mean_pct_tip)\n",
    "rmse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Constant tip vs. constant tip percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pct_tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A constant prediction of 16.08\\% yields a lower RMSE than a constant prediction of \\\\$3.\n",
    "- However, both RMSEs are over \\\\$1, which is relatively high compared to the mean tip amount of \\\\$3.\n",
    "- How can we bring this RMSE down?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model #3: Linear model\n",
    "\n",
    "* **Model:** Tips are made according to a linear function:\n",
    "\n",
    "$$\\text{predicted tip} = w_0^* + w_1^* \\cdot \\text{tip}$$\n",
    "\n",
    "- By choosing a loss function and minimizing empirical risk, we can find $w_0^*$ and $w_1^*$.\n",
    "    - This process is **fitting** our model to the data.\n",
    "    - $w_0^*$ and $w_1^*$ can be thought of as estimates of the true intercept and slope that exist in nature.\n",
    "    \n",
    "- In order to use a linear model, the data should have a linear association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=tips, x='total_bill', y='tip', line_kws={'color': 'red'});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fitting a linear model\n",
    "\n",
    "We'll learn more about `sklearn` today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X=tips[['total_bill']], y=tips['tip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_, lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above coefficients state that the \"best way\" (according to squared loss) to make tip predictions using a linear model is to assume people\n",
    "- Tip ~\\\\$0.92 up front, and\n",
    "- ~10.5\\% of every dollar thereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr.predict(X=tips[['total_bill']])\n",
    "rmse_dict['simple linear model'] = rmse(tips['tip'], preds)\n",
    "rmse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap\n",
    "\n",
    "- We built three models:\n",
    "    - A constant model: $\\text{predicted tip} = h^*$.\n",
    "    - A linear model with no intercept: $\\text{predicted tip} = w^* \\cdot \\text{total bill}$.\n",
    "        - This was the model that involved tip percentage.\n",
    "    - A linear model with an intercept: $\\text{predicted tip} = w_0^* + w_1^* \\cdot \\text{total bill}$.\n",
    "- As we added more features, our RMSEs decreased.\n",
    "    - This was guaranteed to happen, since we were only looking at our training data.\n",
    "- It is not clear that the final linear model is actually \"better\"; it doesn't seem to **reflect reality** better than the previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's next?\n",
    "\n",
    "There's a lot of information in `tips` that we didn't use ‚Äì `'sex'`, `'smoker'`, `'day'`, and `'time'`, for example. How might we **encode** this information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One-hot encoding categorical variables\n",
    "\n",
    "- Recall, we use **one-hot encoding** to transform **nominal categorical columns** into **binary columns**.\n",
    "- Are all of the categorical columns in `tips` nominal?\n",
    "    - Answer: `'day'` is ordinal, but we _can_ still use one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['sex', 'smoker', 'day', 'time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's one-hot encode the categorical columns in `tips`. Here's another way to do so manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tips.copy().loc[:, ['total_bill', 'size']]\n",
    "for c in categorical_cols:\n",
    "    for val in tips[c].unique():\n",
    "        features[f'{c}={val}'] = (tips[c] == val).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `features` has the same number of rows as `tips`; it just has more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's fit a linear model using all features in `features`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X=features, y=tips['tip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_dict['all features'] = rmse(tips['tip'], lr.predict(X=features))\n",
    "rmse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE of our latest model is the lowest of all linear models we've built so far (which is to be expected), **but not by much**. Perhaps these latest features weren't that useful.\n",
    "\n",
    "We can visualize our latest model's predictions, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr.predict(X=features)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(tips['total_bill'], tips['tip'], label='actual tips')\n",
    "plt.scatter(tips['total_bill'], preds, label='predicted tips')\n",
    "plt.xlabel('total_bill')\n",
    "plt.ylabel('tip')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why don't our model's predictions lie on a straight line? ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: Periodic sales\n",
    "\n",
    "- Recall, we can transform existing quantitative features into new quantitative features.\n",
    "    - Often, we do this to make **linear relationships** more evident.\n",
    "- Here, we have a dataset consisting of daily sales volumes from a (fictional) store.\n",
    "- We'd like to **predict future sales**, given current trends.\n",
    "    - What is the \"shape\" of the relationship between `'day'` and `'units sold'`?\n",
    "        - Why is there periodicity (\"peaks\" and \"valleys\")?\n",
    "    - How can we **transform** this relationship so that a linear model is suitable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('data/sinusoidal.csv').sort_values(by='day').reset_index(drop=True)\n",
    "sales.plot(kind='scatter', x='day', y='units sold', title='Daily Sales Volume');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=sales, x='day', y='units sold', line_kws={'color': 'red'});\n",
    "sns.residplot(data=sales, x='day', y='units sold', color='orange', label='residuals');\n",
    "plt.title('Units Sold vs. Day')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a **pattern** in the residual plot here, which is indicative that a linear model is not the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Periodic sales\n",
    "\n",
    "- **Solution:** Transform either `'day'` or `'units_sold'` so that the resulting relationship between the two variables is roughly linear.\n",
    "- Observation: It appears that sales are **sinusoidal**.\n",
    "    - The \"peaks\" are spaced out by 7 days (1 week), so the period of the sinusoid is 7.\n",
    "    - The height difference between a peak and a valley is approximately 10, so the amplitude of the sinusoid is $\\frac{10}{2} = 5$.\n",
    "    - Sales seem to be made up of a linear growth term and a sinusoidal term.\n",
    "- As such, we can **transform `'day'`** using the feature function:\n",
    "\n",
    "$$ \\phi(x) = x + 5\\sin\\left(\\frac{2\\pi}{7} \\cdot x\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_day(day):\n",
    "    return day +  5 * np.sin(2 * np.pi * day / 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['day_transformed'] = transform_day(sales['day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw two scatter plots:\n",
    "- `'day_transformed'` vs. `'day'`.\n",
    "- `'units sold'` vs. `'day'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sales['day'], sales['day_transformed'], label='day_transformed')\n",
    "plt.scatter(sales['day'], sales['units sold'], label='units sold')\n",
    "plt.xlabel('day')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While neither the orange scatter plot nor the blue scatter plot look linear, the relationship between the $y$-values in the two scatter plots _is_ roughly linear!\n",
    "\n",
    "Our new linear model will use `'day_transformed'` as the $x$ and `'units sold'` as the $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.plot(kind='scatter', x='day_transformed', y='units sold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=sales, x='day_transformed', y='units sold', line_kws={'color': 'red'})\n",
    "sns.residplot(data=sales, x='day_transformed', y='units sold', color='orange', label='residuals')\n",
    "plt.title('Units Sold vs. Transformed Day')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the residual plot seems random, which is ideal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `sklearn` overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The steps of the modeling pipeline\n",
    "\n",
    "<center><img src=\"imgs/image_0.png\" width=\"60%\"></center>\n",
    "\n",
    "1. Create features to best reflect the \"meaning\" behind data.\n",
    "2. Choose a model that is appropriate to capture the relationships between features and the response.\n",
    "3. Select a loss function and fit the model (i.e., determine $w^*$).\n",
    "4. Evaluate the model (e.g. using RMSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Features and models using `sklearn`\n",
    "\n",
    "<center><img src=\"imgs/sklearn.png\" width=\"20%\"></center>\n",
    "    \n",
    "* Scikit-learn (`sklearn`) implements many common steps in the feature and model creation pipeline.\n",
    "    - It is **widely** used throughout [industry](https://scikit-learn.org/stable/testimonials/testimonials.html#:~:text=It%20is%20very%20widely%20used,very%20approachable%20and%20very%20powerful.) and academia.\n",
    "* It interfaces with `numpy` arrays, and to an extent, `pandas` DataFrames.\n",
    "* Huge benefit: the [documentation online](https://scikit-learn.org/stable/modules/classes.html) is **excellent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `preprocessing` and `linear_models`\n",
    "\n",
    "For the **feature creation** step of the modeling pipeline, we will use `sklearn`'s [`preprocessing`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) module.\n",
    "\n",
    "<center><img src=\"imgs/feature_part.png\" width=\"30%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For the **model creation** step of the modeling pipeline, we will use `sklearn`'s [`linear_model`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) module.\n",
    "\n",
    "<center><img src=\"imgs/model_part.png\" width=\"36%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformers in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transformer classes\n",
    "\n",
    "- **Transformers** take in \"raw\" data and output \"processed\" data. They are used for **creating features**.\n",
    "    - The input should be a multi-dimensional `numpy` array.\n",
    "        - Inputs can be DataFrames, but `sklearn` only looks at the values (i.e. it calls `to_numpy()` on input DataFrames).\n",
    "    - The output is a `numpy` array (never a DataFrame or Series)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Transformers, like most relevant features of `sklearn`, are **classes**, not functions, meaning you need to instantiate them and call their methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example transformer: `Binarizer`\n",
    "\n",
    "The `Binarizer` transformer allows us to map a quantitative sequence to a sequence of 1s and 0s, depending on whether values are above or below a threshold.\n",
    "\n",
    "|Property|Example|Description|\n",
    "|---|---|---|\n",
    "|Initialize with parameters| `binar = Binarizer(thresh)` | set x=1 if x > thresh, else 0|\n",
    "|Transform data in a dataset | `feat = binar.transform(data)` | Binarize all columns in `data`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import the relevant class from `sklearn.preprocessing`. (Tip: import just the relevant classes you need from `sklearn`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try binarizing `'total_bill'`. We'll say a \"large\" bill is one that is over \\$20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tips = sns.load_dataset('tips') # To remove the columns we \"engineered\" before\n",
    "tips['total_bill'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize a `Binarizer` object with the threshold we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi = Binarizer(threshold=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we call `bi`'s `transform` method and pass it the data we'd like to transform. Note that its input and output are both 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_bills = bi.transform(tips[['total_bill']]) # Must pass transform a 2D array/DataFrame\n",
    "transformed_bills[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We can verify that it worked correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((tips['total_bill'] > 20).astype(int) == transformed_bills.flatten()).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example transformer: `StdScaler`\n",
    "\n",
    "- `StdScaler` **standardizes** data using the mean and standard deviation of the data.\n",
    "\n",
    "$$z_i = \\frac{x_i - \\bar{x}}{\\sigma_x}$$\n",
    "\n",
    "- Unlike `Binarizer`, `StdScaler` **requires some knowledge (mean and SD) of the dataset before transforming**.\n",
    "- As such, we need to **`fit`** an `StdScaler` transformer before we can use the `transform` method.\n",
    "* Typical usage: fit transformer on a sample; use that fit transformer to transform future data.\n",
    "\n",
    "\n",
    "|Property|Example|Description|\n",
    "|---|---|---|\n",
    "|Initialize with parameters| `stdscaler = StandardScaler()` | z-scale the data (no parameters) |\n",
    "|Fit the transformer| `stdscaler.fit(data)` | compute the mean and SD of `data`|\n",
    "|Transform data in a dataset | `feat = stdscaler.transform(newdata)` | z-scale `newdata` with mean and SD of `data`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It only makes sense to standardize the already-quantitative columns of `tips`, so let's select just those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_quant = tips[['total_bill', 'tip', 'size']]\n",
    "tips_quant.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize a `StandardScaler` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following **does not work!** The error message is very helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler.transform(tips_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we need to first call the `fit` method on `stdscaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler.fit(tips_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `transform` will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First column is 'total_bill', second column is 'tip', third column is 'size'\n",
    "tips_quant_z = stdscaler.transform(tips_quant)\n",
    "tips_quant_z[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the mean and variance `stdscaler` computed for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler.var_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can call `transform` on DataFrames other than `tips_quant`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler.transform(tips_quant.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example transformer: `OneHotEncoder`\n",
    "\n",
    "Let's keep just the categorical columns in `tips`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_cat = tips[['sex', 'smoker', 'day', 'time']]\n",
    "tips_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like `StdScaler`, we will need to `fit` our `OneHotEncoder` transformer before it can transform anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "ohe.fit(tips_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the unique values (i.e. categories) in each column by using the `categories_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_features = ohe.transform(tips_cat)\n",
    "ohe_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the resulting matrix is **sparse** ‚Äì most of its elements are 0 ‚Äì `sklearn` uses a more efficient representation than a regular `numpy` array. That's no issue, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the column names from `tips_cat` are no longer stored anywhere (remember, `fit` converts the input to a `numpy` array before proceeding).\n",
    "\n",
    "We can use the `get_feature_names` method on `ohe` to access the names of the one-hot-encoded columns, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.get_feature_names() # x0, x1, x2, and x3 correspond to column names in tips_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ohe` also has an `inverse_transform` method, which takes a one-hot-encoded matrix and returns a categorical matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.inverse_transform(ohe_features[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Models in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model classes\n",
    "\n",
    "- `sklearn` model classes (called \"estimators\") behave like transformers, in that we need to instantiate and `fit` them.\n",
    "- The difference is that we also need to specify what our \"response\" or \"target\" variable is, i.e. what we are trying to predict.\n",
    "    - Calling `fit` is the same as \"training our model\".\n",
    "- There are several models in the [`linear_model`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) package; we will start with `LinearRegression`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The `LinearRegression` class\n",
    "\n",
    "We've seen this a few times in lecture already, but never formally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** From [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression), we have\n",
    "\n",
    "> LinearRegression fits a linear model with coefficients w = (w1, ‚Ä¶, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
    "\n",
    "In other words, `LinearRegression` minimizes mean squared error by default.\n",
    "\n",
    "Additionally, by default the `fit_intercept` argument is set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Predicting `'tip'` from `'total_bill'` and `'size'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we instantiate and fit. By calling `fit`, we are saying \"minimize mean squared error and find $w^*$\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "# Note that there are two arguments to fit ‚Äì X and y!\n",
    "# (It is not necessary to write X= and y=)\n",
    "lr.fit(X=tips[['total_bill', 'size']], y=tips['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, the `predict` method is available. Note that the argument to `predict` can be any 2D array with two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted tip from a table of 3 that spends $25 \n",
    "lr.predict([[25, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted tip from a table of 14 that spends $1000 ‚Äì probably not accurate!\n",
    "lr.predict([[1000, 14]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the intercepts and slopes individually. This model is of the form\n",
    "\n",
    "$$\\text{predicted tip} = w_0^* + w_1^* \\cdot \\text{total bill} + w_2^* \\cdot \\text{table size}$$\n",
    "\n",
    "so we should expect three parameters total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to compute the RMSE of our model, we need to find its predictions on every row in the training data (`tips`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = lr.predict(tips[['total_bill', 'size']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.mean((all_preds - tips['tip']) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It turns out that fit `LinearRegression` objects also have a `score` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr.score(tips[['total_bill', 'size']], tips['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look like the RMSE... what is it? ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: $R^2$\n",
    "\n",
    "- $R^2$, or the **coefficient of determination**, is a measure of the **quality of a linear fit**.\n",
    "- There are a few equivalent ways of computing it, assuming your model has an intercept term:\n",
    "\n",
    "$$R^2 = \\frac{\\text{var}(\\text{predicted $y$ values})}{\\text{var}(\\text{actual $y$ values})}$$\n",
    "\n",
    "$$R^2 = \\left[ \\text{correlation}(\\text{predicted $y$ values}, \\text{actual $y$ values}) \\right]^2$$\n",
    "\n",
    "- In the simple linear regression case, it is the square of the correlation coefficient, $r$.\n",
    "- **Key idea:** $R^2$ ranges from 0 to 1. **The closer it is to 1, the better the linear fit is.**\n",
    "- Interpretation: $R^2$ is the **proportion of variance in $y$ that the linear model explains**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Calculating $R^2$\n",
    "\n",
    "Recall, `all_preds` contains the predicted `'tip'` for every data point in `tips`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 1: $R^2 = \\frac{\\text{var}(\\text{predicted $y$ values})}{\\text{var}(\\text{actual $y$ values})}$**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(all_preds) / np.var(tips['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2:** $R^2 = \\left[ \\text{correlation}(\\text{predicted $y$ values}, \\text{actual $y$ values}) \\right]^2$\n",
    "\n",
    "Note: By correlation here, we are referring to $r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.corrcoef(all_preds, tips['tip'])) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 3:** `lr.score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr.score(tips[['total_bill', 'size']], tips['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three methods provide the same result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `LinearRegression` summary\n",
    "\n",
    "|Property|Example|Description|\n",
    "|---|---|---|\n",
    "|Initialize model parameters| `lr = LinearRegression()` | Create (empty) linear regression model|\n",
    "|Fit the model to the data | `lr.fit(data, responses)` | Determines regression coefficients|\n",
    "|Use model for prediction |`lr.predict(newdata)`| Use regression line make predictions|\n",
    "|Evaluate the model| `lr.score(data, responses)` | Calculate the $R^2$ of the LR model|\n",
    "|Access model attributes| `lr.coef_` | Access the regression coefficients|\n",
    "\n",
    "***Note:*** Once `fit`, estimators like `LinearRegression` are just transformers (`predict` <-> `transform`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary, next time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- Quantitative feature transformations allow us to use linear models to model non-linear data.\n",
    "- Transformers in `sklearn` are used for **feature engineering**, while estimators in `sklearn` are used for **models**.\n",
    "- A common pattern:\n",
    "    - Instantiate.\n",
    "    - `fit`.\n",
    "    - `transform` / `predict`.\n",
    "- We like linear models with **low RMSE** and **high $R^2$**!\n",
    "- **Next time:** Combining transformers and estimators in a single **pipeline**."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "livereveal": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
