{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"discussion.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion 6\n",
    "\n",
    "### Due Saturday May 14th, 11:59:59PM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex and NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discussion import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions\n",
    "\n",
    "### Resources\n",
    "\n",
    "**Online Simulators**\n",
    "\n",
    " - https://pythex.org/\n",
    "\n",
    " - https://regex101.com/\n",
    " \n",
    "**Cheat sheets**\n",
    "\n",
    " - https://dsc80.com/resources/other/berkeley-regex-reference.pdf\n",
    "\n",
    " - https://www.debuggex.com/cheatsheet/regex/python\n",
    "\n",
    " - https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**: Identify duplicate words in a sentence\n",
    "\n",
    "Given an input sentence as a string, provide a list of words that are duplicated. If there is no duplication return an empty list\n",
    "\n",
    "**Hint 1:** Checkout capture groups and backreferencing in regex\n",
    "\n",
    "**Hint 2:** Use the online simulators with the doctests as test cases for faster experimentation and debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use capture group number to identify duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**: Extract laptop specifications\n",
    "\n",
    "Given a df with product description - Return df with added columns of `processor` (i3, i5), `generation` (9th Gen, 10th Gen), `storage` (512 GB SSD, 1 TB HDD), `display_in_inch` (15.6 inch, 14 inch). The below image provides details on column names and the exact patterns.\n",
    "\n",
    "If there is no specific information present, keep a null (`NaN`) value.\n",
    "\n",
    "**Hint:** You can write regex patterns in `.str.extract()` pandas methods. Note that this method may return multiple columns based on the number of capture groups present.\n",
    "\n",
    "<img src='imgs/laptop_specs.PNG'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'pd_column.str.extract(r'pattern')' to extract the required pattern\n",
    "df = pd.read_csv('data/laptop_details.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "out = laptop_details(pd.read_csv('data/laptop_details.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing - Dealing with Text Data\n",
    "\n",
    "- Unstructured data is everywhere - Everything you read, see and listen\n",
    "- Quantifying text data and extracting features from it is important to generate insights and build models\n",
    "\n",
    "\n",
    "- Text representation ia a huge area of study - Representing a piece of text as a vector of numbers (BoW, TF-IDF, semantic embeddings etc.)\n",
    "\n",
    "- In this section, we will focus on Bag-of-Words representations using uni-grams and bi-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the musical instuments reviews dataset which contains information on reviews and ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv('data/musical_instruments_reviews.csv')\n",
    "review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams in text\n",
    "\n",
    " - uni-gram consists of a single word from a text sequence\n",
    " - Extending this, an n-gram consists of consecutive 'n' words from a text sequence\n",
    " - Eg. For `text = 'i love data science'`, uni-grams are `['i', 'love', 'data', 'science']`, bi-grams are `['i love', 'love data', 'data science']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the uni-grams and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First normalize the reviews by converting to lower and removing all puntuations\n",
    "reviews = review_df['reviewText'].str.lower().str.replace('[^\\w\\s]','', regex=True)\n",
    "reviews = reviews.tolist()\n",
    "# reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the unigrams from all the reviews\n",
    "unigrams = []\n",
    "for review in reviews:\n",
    "    words = review.split()\n",
    "    unigrams.extend(words)\n",
    "unigrams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting unigram counts\n",
    "pd.Series(unigrams).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Does this make sense? Both the values and their counts?\n",
    "- What are the positives/drawbacks of using unigram bag-of-words for text representations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does 'reviewText' differ from 'summary'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = review_df['summary'].str.lower().str.replace('[^\\w\\s]','', regex=True)\n",
    "reviews = reviews.tolist()\n",
    "\n",
    "# Getting all the unigrams from all the reviews\n",
    "unigrams = []\n",
    "for review in reviews:\n",
    "    words = review.split()\n",
    "    unigrams.extend(words)\n",
    "\n",
    "pd.Series(unigrams).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: Create bi-gram counts of the whole reviews text corpus.\n",
    "\n",
    "Given a DataFrame like `review_df` and a column string (either `reviewText` or `summary`),\n",
    "return a Series with bi-gram counts of that column sorted in descending order. The index of the series should be a tuple of bi-grams and the value should indicate the count of times that bi-gram appears in the whole corpus.\n",
    "\n",
    "Perform the text normalization (lower case conversion and removing all punctuations) like we did in the uni-gram case before creating bi-gram counts.\n",
    "\n",
    "**Hint:** Use splitting and zipping to create bi-gram combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "out_bigrams_text = bigram_counts(pd.read_csv('data/musical_instruments_reviews.csv'), 'reviewText')\n",
    "out_bigrams_summary = bigram_counts(pd.read_csv('data/musical_instruments_reviews.csv'), 'summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words\n",
    "\n",
    "- The bag of words model represents texts (e.g. review, summary) as vectors of word counts.\n",
    "- It is called 'bag of words' because it doesn't consider order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Bag-of-Words Count Matrix\n",
    "\n",
    "Let's create a BoW count matrix of 'summary' using 'bi-grams'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_bigrams_summary = bigram_counts(pd.read_csv('data/musical_instruments_reviews.csv'), 'summary')\n",
    "out_bigrams_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = review_df['summary'].str.lower().str.replace('[^\\w\\s]','', regex=True)\n",
    "# reviews = reviews.tolist()\n",
    "\n",
    "# We can reduce sparsity in representations by filtering the bigrams as well.\n",
    "k = 1000\n",
    "\n",
    "counts_dict = {}\n",
    "for bigram in out_bigrams_summary.index[:k]:\n",
    "    bigram = ' '.join(bigram)\n",
    "    regex_pattern = fr'\\b{bigram}\\b'\n",
    "    counts_dict[bigram] = reviews.str.count(regex_pattern).astype(int).tolist()\n",
    "    \n",
    "counts_df = pd.DataFrame(counts_dict)\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = pd.concat([reviews.to_frame(), counts_df], axis=1).set_index('summary')\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "- Addresses the BoW drawback of giving high weightage to common words\n",
    "- TF-IDF tries to **give high weightage to words that are unique to that particular document**\n",
    "- For comparison, BoW is simply TF\n",
    "\n",
    "\n",
    "- TF-IDF = Term Frequency * Inverse Document Frequency\n",
    "    - TF is a function of that document\n",
    "    - IDF is a function of the corpus\n",
    "    \n",
    "\n",
    "- Refer to Lecture 19 for detailed explanations and calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to Ponder\n",
    "\n",
    "- Pros and cons of Bag-of-Words? Can there be better representations than just counts?\n",
    "- Pros and cons of TF-IDF?\n",
    "- Pros and cons of uni-grams vs. bi-grams? Which are suitable in what cases?\n",
    "- Pros and cons of long text vs. short text?\n",
    "- What combinations of above might work well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You're done!\n",
    "\n",
    "* Submit your `.py` file to Gradescope. Note that you only need to submit the `.py` file; this notebook should not be uploaded. Make sure that all of your work is in the `.py` file and not here by running the doctests: `python -m doctest discussion.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> res = duplicate_words('I went to the market market with my my my family')\n>>> res == ['market', 'my']\nTrue",
         "failure_message": "multiple duplicates",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> res = duplicate_words('necessity is the mother of invention')\n>>> res == []\nTrue",
         "failure_message": "no duplicates",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> res = duplicate_words('fair fairer fairest')\n>>> res == []\nTrue",
         "failure_message": "word boundary",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out.shape == (21, 5)\nTrue",
         "failure_message": "doctest: shape",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out['generation'].nunique() == 4\nTrue",
         "failure_message": "doctest: unique generations",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> list(out.iloc[0].values)[1:] == ['i5', '9th Gen', '512 GB SSD', '15.65 inch']\nTrue",
         "failure_message": "first row",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> list(out.iloc[-1].values)[1:] == ['i7', '9th Gen', '512 GB SSD', '15.6 inch']\nTrue",
         "failure_message": "last row",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> avg_display_inch = out['display_inch'].str.split().str[0].astype(float).mean()\n>>> np.isclose(avg_display_inch, 14.795000000000002, atol=0.01)\nTrue",
         "failure_message": "doctest: average display inches",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(out_bigrams_text, pd.Series)\nTrue",
         "failure_message": "doctest: check datatype",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> out_bigrams_text.shape == (8470,)\nTrue",
         "failure_message": "doctest: shape",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> out_bigrams_text.index[0] == ('for', 'the')\nTrue",
         "failure_message": "doctest: first index",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> out_bigrams_text[0] == 36\nTrue",
         "failure_message": "doctest: first value",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> out_bigrams_text.sum() == 12116\nTrue",
         "failure_message": "sum of counts",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> isinstance(out_bigrams_summary, pd.Series)\nTrue",
         "failure_message": "doctest: check datatype",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> out_bigrams_summary.shape == (559,)\nTrue",
         "failure_message": "doctest: shape",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> out_bigrams_summary.index[1] == ('guitar', 'cable')\nTrue",
         "failure_message": "doctest: first index",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> out_bigrams_summary[1] == 7\nTrue",
         "failure_message": "doctest: first value",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> out_bigrams_summary.sum() == 680\nTrue",
         "failure_message": "sum of counts",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
