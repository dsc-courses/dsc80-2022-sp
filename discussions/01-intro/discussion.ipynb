{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"discussion.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80 - Discussion 01\n",
    "\n",
    "### Due Date: Saturday April 2, 11:59 PM\n",
    "\n",
    "**Discussions will be due by the end of the day on Saturday**\n",
    "\n",
    "* Lecture Review: models and the data science life-cycle.\n",
    "* Overview: How to work on homework.\n",
    "* Tutorial: `numpy` review and an example HW problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lecture Review\n",
    "\n",
    "### The data science lifecycle\n",
    "\n",
    "<center><img src=\"imgs/DSLC.png\" width=\"40%\"></center>\n",
    "\n",
    "The data science life-cycle:\n",
    "* Researching domain\n",
    "* Questions and hypotheses\n",
    "* Finding and cleaning data\n",
    "* Data modeling\n",
    "* Predictions and Inference\n",
    "* Decisions\n",
    "\n",
    "Some terminology of modeling:\n",
    "* A **data generating process (DGP)** is the real-world phenomenon under consideration.\n",
    "* The **true (probability) model** is a mathematical representation of the random phenomenon that generates any representative observations.\n",
    "* The **observations** are data representing the data generating process.\n",
    "* A **(fit) statistical model** of the data is the best approximation of the data generating process under the probability model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Suppose you want to predict the outcome of the next presidential election.\n",
    "\n",
    "1. What is the questions and hypothesis to answer and test?\n",
    "2. What observations you might collect?\n",
    "3. What measurements do you care about? (i.e. what do your observations look like?)\n",
    "4. What statistical model might you use?\n",
    "5. How might you assess the quality of your fit model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Suppose we want to understand the pay disparity between men and women among city of SD employees.\n",
    "\n",
    "1. Does the dataset in lecture adequate enough or will you capture other measurements/features?\n",
    "2. What is the applicability of above process to other years and cities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Overview: working on assignments\n",
    "\n",
    "The class assignments are available on the class git repository; they consist of a notebook with the problems statements, starter code in a `.py` file, and any required supplementary files (e.g. data). After pulling the HW material, you will develop your solutions using a combination of jupyter notebooks and your favorite IDE (e.g. sublime text, or the jupyterhub server). Once finished, you will submit your assignment to gradescope.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining course materials (assignments)\n",
    "\n",
    "Git is a version control system that is used to with the development of the course materials. For an introduction to using git in the course, see this [tutorial](https://drive.google.com/open?id=1m6mXfhjFInHPeJyaHdAwfiakcFYh73HC8TeAB9E9Xeo) and this hands-on [tutorial](https://docs.google.com/document/d/1E2Zg0pC8S3cyT564jug6rqAhSNraR_7Yy_4AnvHaGu4/edit?usp=sharing). To use git on a Mac, you will need to open the terminal; on Windows, you should download [git-bash](https://gitforwindows.org/).\n",
    "\n",
    "The course materials are stored in a git repository on *github* (a git server) -- you can view it in a browser [here](https://github.com/dsc-courses/dsc80-2022-sp). To obtain the course files, follow the directions in the tutorial above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The notebook / IDE balance\n",
    "\n",
    "Now that your assignment is on your computer, you are ready to work. You will be using two different tools to develop the code and create the analyses that the assignments require of you. Generally, these are:\n",
    "1. Jupyter notebooks contains the problems statements themselves; they also provide a place to test out code, understand data, and produce reports/summaries of conclusions.\n",
    "2. An IDE for developing re-usable and testable python code. Abstracting your notebook code into python library code avoids common mistakes in notoriously error prone notebook environments. Luckily, once a function is in your `.py` file, you can still import/use it in a notebook!\n",
    "\n",
    "Both of these environments are essential in the data scientist toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking your work\n",
    "\n",
    "An effective environment for testing and understanding your work is essential to success in the class. The notebook and the IDE play different roles in checking your work.\n",
    "\n",
    "* The notebook provides a place to understand the output of your function and test it against your intuition and understanding of what the correct output should be. When working with data, you should always check the correctness of your work using your understanding of that data (i.e. is my conclusion reasonable given what I know about the data?). This is typically the ultimate goal for a problem, so you should *always* interpret your answer on the data in a notebook.\n",
    "\n",
    "* Abstracting your code to library functions/classes in a `.py` file encourages using software development best-practices in your data processing and analyses. While expressive, notebooks are error-prone, manual, and hard to debug. Moving useful code to a `.py` file makes your code more clear, encourages code reuse, and makes debugging easier. Once you have moved any work from your notebook to a `.py` file, you should check the correctness of your work in two ways:\n",
    "    - Run the doctests. The doctests ensure your code *meets the contract* specified in the question (or by you, in your own projects). That is, is your code expecting the correct inputs and outputs? **Doctests do not check more than if your code is acting on the correct types**.\n",
    "    - Import your function into the notebook and test it on data as above. Use your understanding of the data to assess the correctness of your code!\n",
    "    \n",
    "### Types of Tests in a Glance\n",
    "\n",
    "<center><img src=\"imgs/testing_summary.png\" width=\"80%\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW submission\n",
    "\n",
    "Once you have finished the assignment, log into Gradescope and submit the `.py` file to the appropriate assignment. \n",
    "* Upon submission, the autograder will run the doctests and make visible if they tests passed or not. These are worth *a few* points; the purpose is to check that the autograder environment is consistent with the environment on which you developed your HW.\n",
    "* The results of the \"correctness tests\" that you will be ultimately graded on will not be visible until after the due date.\n",
    "* The autograder will tell you if your code failed to run, though generally will not tell you why. The most common reasons are listed below.\n",
    "    - **Timeout**: the autograder *will* tell you if your code failed to run after 20 minutes. If this occurs, you should try to isolate which problem is causing the timeout and either fix it or comment it out!\n",
    "    - **Syntax Errors**: Any syntax errors (e.g. bad code indentation) will cause gradescope to fail (giving a 0 on the assignment). Always double check your code passes doctests *on the commandline* (just as the autograder runs it). Further, pulling your code (from github) onto DataHub and running the tests there is a good debugging technique, as the environment is very similar to gradescope. \n",
    "    - **OOM (out of memory)**: The autograder runs a 1GB server, which is smaller than your computer. Assignments should never require more memory than this; you should think about how to simplify your code!\n",
    "    - **Change of file names**: Do not change file names while submitting to Gradescope. They should be one of `lab.py`, `discussion.py` and `project.py`\n",
    "\n",
    "If the problem persists, ask course staff why the autograder is failing.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A remark on DataHub\n",
    "\n",
    "UCSD Educational Technology Services has made servers available for use at [DataHub](datahub.ucsd.edu). Once logged in, you have not only a jupyter notebook server running, but an entire unix environment. To make best use of this environment, once logged in, replace the `/tree` in the URL with `/lab` and you can use the JupyterLab IDE/notebook environment. Here, you can use (1) jupyter notebooks, (2) terminals, and (3) a simple text editor for editing python files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: `numpy` review as a HW problem\n",
    "\n",
    "Work on this tutorial like an assignment. **Complete the questions 1 and 8, and turn them into gradescope by midnight on Saturday**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is this? \n",
    "# Autoreloads the .py files so the changes made to py file are reflected immediately.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discussion import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a review of working with Numpy arrays, see the [arrays chapter](https://www.inferentialthinking.com/chapters/05/1/Arrays.html) of Inferential Thinking (DSC10). The most relevant concepts are:\n",
    "* element-wise array operations, that avoid loops ('vectorization')\n",
    "* the functions and methods for performing array arithmetic (see the tables in the page referenced above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1** Write a function that takes in a file-path that points to a data file like `restaurants.csv` and returns an array of values of restaurant bills.\n",
    "\n",
    "*Notes*: Where is the file? What values? Look at the starter code documentation in `discussion.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data2array(os.path.join('data', 'restaurant.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** How many restaurant bills are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Suppose everyone leaves an 18% tip. Create an array of tip amounts. What is the total amount of tips in the array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** What is the average/median/min/max restaurant bills? Give answer in an array, in the order listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** How many restaurant bills are greater than $15?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** How much total money for the restaurant is there? What proportion of that comes from bills less than $5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7:** What proportion of bills have at least one other bill within $0.05 tolerance of 20 dollars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8:** What proportion of restaurant bills end in 9 in the hundredths place?\n",
    "\n",
    "Create a function `ends_in_9` that takes in an array of dollar amounts (like the output of Question 1) and returns the proportion of values that end in 9 in the hundredths place. \n",
    "\n",
    "*Hints:* Use the remainder function `%`. Be careful of floating point operations (use the rounding/integer conversion appropriately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ends_in_9(data2array('data/restaurant.csv'))\n",
    "arr = np.array([23.04, 45.00, 0.50, 0.09])\n",
    "doc_result = ends_in_9(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You're done!\n",
    "\n",
    "* Submit your `.py` file to Gradescope. Note that you only need to submit the `.py` file; this notebook should not be uploaded. Make sure that all of your work is in the `.py` file and not here by running the doctests: `python -m doctest discussion.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python -m doctest discussion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(result, np.ndarray)\nTrue",
         "failure_message": "check the returned datatype",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> result.shape[0] == 100000\nTrue",
         "failure_message": "check the rows",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> result.dtype == np.dtype('float64')\nTrue",
         "failure_message": "check the data type of values",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(result[0], 16.87)\nTrue",
         "failure_message": "is the first element 16.87?",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(result.mean(), 14.9644172)\nTrue",
         "failure_message": "test on the mean of all elements",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8": {
     "name": "q8",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 0 <= doc_result <= 1\nTrue",
         "failure_message": "check doctest",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(doc_result, 0.25)\nTrue",
         "failure_message": "correctness of answer on doctest array",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> np.isclose(result, 0.09945, atol=0.05)\nTrue",
         "failure_message": "Tested on Restaurant data; approximately correct on *all* the data",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(result, 0.09945)\nTrue",
         "failure_message": "Tested on Restaurant data; did you round to int before taking the remainder? This is likely caused by a floating point rounding error.",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
