{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"project.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Language Models üó£\n",
    "\n",
    "### Checkpoint Due Date: Thursday, May 19th at 11:59PM (Questions 1-4)\n",
    "\n",
    "### Due Date: Thursday, May 26th at 11:59PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Welcome to Project 4! Project 4 will work similar to Projects 1 and 2, in that:\n",
    "- All of your code should go in `project.py`.\n",
    "- There is a checkpoint that is due a week before the project is due. (In this case, the checkpoint consists of Questions 1-4.)\n",
    "\n",
    "As per usual, you are able to work with a partner, provided that you follow the practice of [Pair Programming](https://dsc10.com/pair-programming/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Assignment\n",
    "\n",
    "### Introduction to Language Models (LM)\n",
    "\n",
    "In this project, you will build *[statistical language models](https://en.wikipedia.org/wiki/Language_model)* using public domain books found on [Project Gutenberg](https://www.gutenberg.org/). Language models attempt to capture the likelihood that a given sequence of words occur in a given \"language\" (the precise term is \"corpus\" or \"corpora\"). Here, \"language\" is a broad term that, in addition to the normal usage, may mean the language of a particular author or style. As with all statistical models, the true data generating process is never known and thus we cannot know the true probability that a sequence of words will occur ‚Äì however, we can estimate these probabilities via various methods, some of which are more reliable than others. For example, one might guess that the probability of a sentence is simply the product of the empirical probabilities (i.e., the number of times a word is observed in a dataset divided by the number of words in that dataset). This is one of the methods of estimating the probability of a sequence of words that you will implement in this project.\n",
    "\n",
    "### Tokenizing Corpora\n",
    "\n",
    "Computing the probabilities of a language model from a book requires breaking up the text of book into sequences of words. This process is called *tokenization*. In reality though, the sequences are not made up entirely of words, but rather more general terms called *tokens*. In this project tokens will include not only whole words, but also punctuation and other terms. Below are a few examples of other types of tokens:\n",
    "\n",
    "* Punctuation. For example, the period makes sense as a token, as certain words tend to end sentences (i.e. appear right before a `'.'`), while other words tend to begin sentences (i.e. appear right after a `'.'`).\n",
    "* We have special \"START\" and \"END\" tokens that begin and end every word sequence (in our case, paragraphs of words in a given book). These make sense as tokens, as certain words may tend to begin and end paragraphs. \n",
    "\n",
    "It is useful for the tokens used to represent START and END to be single characters that can never be found in the text of the book you use to create your language model. Thus, you will use two ASCII hidden \"[control characters](https://en.wikipedia.org/wiki/C0_and_C1_control_codes#C0_(ASCII_and_derivatives))\":\n",
    "* For START, you will use the character `'\\x02'`, which refers to the \"beginning of text\".\n",
    "* For END, you will use the character `'\\x03'`, which refers to the \"end of text\".\n",
    "\n",
    "### A Note on Checking Your Work for Correctness\n",
    "\n",
    "To build a language model, you will need to perform several steps, from data cleaning to implementing mathematical formulas. **Things will get messy, and it can be difficult to assess the correctness of your work.**\n",
    "\n",
    "- The doctests will include checks on the inputs and outputs of your methods.\n",
    "- The `grade.check` public notebook tests will be **more informative** than the doctests, but they still won't guarantee that your code is correct.\n",
    "- The doctests include small-enough examples that you should be able to determine the correct outputs (e.g. probabilities) by hand.\n",
    "- Run your functions on **real books** taken from [Project Gutenberg](https://www.gutenberg.org/) and make sure that their behavior looks correct. After selecting a book, click the \"Plain Text UTF-8\" link to find the book's text ([example](https://www.gutenberg.org/cache/epub/68085/pg68085.txt))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Navigating the Project\n",
    "\n",
    "Below is an outline of the the project. If there are terms you don't understand in following descriptions, you should review the introduction for an explanation of the terms and ideas described.\n",
    "\n",
    "- [Part 1: Dissecting the Corpus ü™öü´Ä](#part1)\n",
    "    - [Question 1: Preparing the Corpus (Checkpoint Question)](#question1)\n",
    "    - [Question 2: Tokenizing the Corpus (Checkpoint Question)](#question2)\n",
    "- [Part 2: Creating Baseline Language Models üìï](#part2)\n",
    "    - [Question 3: Uniform Language Models (Checkpoint Question)](#question3)\n",
    "    - [Question 4: Unigram Language Models (Checkpoint Question)](#question4)\n",
    "- [Part 3: Creating an N-Gram Language Model üìö](#part3)\n",
    "    - [Question 5.1: Creating N-Grams](#question5a)\n",
    "    - [Question 5.2: Training the N-Gram LM](#question5b)\n",
    "    - [Question 5.3: Computing Probabilities using the N-Gram Model](#question5c)\n",
    "    - [Question 5.4: Sampling from the N-Gram Model](#question5d)\n",
    "    \n",
    "**Disclaimer: You should expect Question 5/Part 3 to take the same amount of time as Parts 1 and 2 combined.** Do not leave it to the last minute just because it looks like there is only \"one question\"! Note also that the entire project is worth **105 points**, of which **48 come from Question 5/Part 3 alone**.\n",
    "\n",
    "Good luck ‚Äì let's get started! üéâ\n",
    "    \n",
    "---\n",
    "\n",
    "While working on the project, check the Campuswire post titled \"Project 4 Released!\" for any clarifications.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dissecting the Corpus ü™öü´Ä\n",
    "\n",
    "<a name='part1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 ‚Äì Preparing the Corpus\n",
    "\n",
    "<a name='question1'></a>\n",
    "\n",
    "In this question, you will use `requests` to download a public domain book from [Project Gutenberg](https://www.gutenberg.org/), like [this one](http://www.gutenberg.org/files/57988/57988-0.txt), and prepare it for analysis in later questions. Create a function `get_book` that takes in the `url` of a \"Plain Text UTF-8\" book and **returns a string** containing the contents of the book. \n",
    "\n",
    "The returned string should satisfy the following conditions:\n",
    "* The contents of the book consist of **everything** between Project Gutenberg's START and END comments (but not including the START and END comments themselves).\n",
    "* The contents **do** include the title, author, and table of contents.\n",
    "* You should replace any Windows newline characters (`'\\r\\n'`) with standard newline characters (`'\\n'`).\n",
    "* You should check Project Gutenberg's [`robots.txt`](https://gutenberg.org/robots.txt) as well and implement a \"pause\" in your request in accordance with the website's policy. If the function is called twice in succession, it should not violate the `robots.txt` policy.\n",
    "\n",
    "***Notes:*** \n",
    "- There is no need to use BeautifulSoup here, so please don't import it.\n",
    "- You are encouraged to find whatever books on the website that interest you to test your code and the language models you develop. The text doesn't even need to be an English-language book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "beowulf = get_book('https://www.gutenberg.org/ebooks/16328.txt.utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 ‚Äì Tokenizing the Corpus\n",
    "\n",
    "<a name='question2'></a>\n",
    "\n",
    "Now, you need to **tokenize** the text of a book. To do so, create a function `tokenize` that takes in a string, `book_string`, and returns a **list of the tokens** (words, numbers, and all punctuation) satisfying the following conditions:\n",
    "* The start of every paragraph should be represented in the list with the single character `'\\x02'` (standing for START).\n",
    "* The end of every paragraph should be represented in the list with the single character `'\\x03'` (standing for STOP).\n",
    "* Tokens should include *no* whitespace.\n",
    "* Two or more newlines count as a paragraph break. Whitespace (e.g. multiple newlines) between two paragraphs of text should not appear as tokens.\n",
    "* All punctuation marks count as tokens, even if they are uncommon (e.g. `'@'`, `'+'`, and `'%'` are all valid tokens).\n",
    "\n",
    "For example, consider the following excerpt. (The first sentence is at the end of a larger paragraph, and the second sentence is at the start of a longer paragraph.)\n",
    "```\n",
    "...\n",
    "My phone's dead.\n",
    "\n",
    "I didn't get your call.\n",
    "...\n",
    "```\n",
    "should tokenize to:\n",
    "```py\n",
    "[...\n",
    "'My', 'phone', \"'\", 's', 'dead', '.', '\\x03', '\\x02', 'I', 'didn', \"'\", 't', 'get', 'your', 'call', '.'\n",
    "...]\n",
    "```\n",
    "\n",
    "***Note:*** `tokenize` should run quickly; you should avoid loops when possible (our solution only has one loop). Specifically, `tokenize` should run on the the complete works of Shakespeare (in `data/shakespeare.txt`) in **under 10 seconds** to get full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to prepare the autograder tests. It runs your `tokenize` function on the full body of Shakespeare's work. As a guide, you should expect the first three elements of `shakes` to be `'\\x02'`, `'The'`, and `'Complete'`, and the last three elements of `shakes` to be `'William'`, `'Shakespeare'`, and `'\\x03'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "import time\n",
    "start = time.time()\n",
    "shakes = tokenize(open('data/shakespeare.txt').read())\n",
    "elapsed = time.time() - start\n",
    "elapsed # Should be under 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a name='part2'></a>\n",
    "\n",
    "## Part 2: Creating Baseline Language Models üìï\n",
    "\n",
    "Now that we're able to tokenize a corpus, it is time to start building language models (LM).\n",
    "\n",
    "In this project, we will build three different language models. They all operate under the premise of assigning probabilities to sentences. Given a sentence ‚Äì that is, a sequence of tokens $w = w_1\\ldots w_n$ ‚Äì we want to be able to compute the **probability** that sentence is used: \n",
    "$$P(w) = P(w_1,\\ldots,w_n)$$\n",
    "\n",
    "However, sentences are built from tokens, and the likelihood that a token occurs where it does depends on the tokens before it. This points to using **conditional probability** to compute $P(w)$. That is, we can write:\n",
    "\n",
    "$$\n",
    "P(w) = P(w_1,\\ldots,w_n) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_1,w_2) \\cdot\\ldots\\cdot P(w_n|w_1,\\ldots,w_{n-1})\n",
    "$$  \n",
    "This is also called the **chain rule** for probabilities.\n",
    "\n",
    "**Example:** Consider the sentence \n",
    "\n",
    "<center><code>'when I drink Coke I smile'</code></center>\n",
    "    \n",
    "The probability that it occurs, according the the chain rule, is\n",
    "\n",
    "$$\n",
    "P(\\text{when}) \\cdot P(\\text{I | when}) \\cdot P(\\text{drink | when I})\\cdot P(\\text{Coke | when I drink}) \\cdot P(\\text{I | when I drink Coke}) \\cdot P(\\text{smile | when I drink Coke I})\n",
    "$$\n",
    "\n",
    "That is, the probability that the sentence occurs is the product of the probability that each subsequent token follows the tokens that came before. For example, the probability $P(\\text{Coke | when I drink})$ is likely pretty high, as Coke is something that you drink. The probability $P(\\text{pizza | when I drink})$ is likely low (if not 0), because pizza is not something that you drink.\n",
    "\n",
    "You may wonder how the language model \"knows\" that Coke is something that you drink, but pizza is not. The way that the language model \"learns\" its probabilities is by **looking at examples of existing sentences**, i.e. by being **trained on a corpus**. Throughout Parts 2 and 3, you will look at **different ways of estimating these probabilities**. In each case, you will use a corpus to assign probabilities to different tokens and combinations of tokens, and use those probabilities to generate new sentences.\n",
    "\n",
    "<br>\n",
    "\n",
    "Each language model you build will be a **class** with a few methods in common:\n",
    "\n",
    "* The `__init__` constructor: when you instantiate an LM object, you will need to pass the \"training corpus\" on which your model will be trained (i.e. a list of tokens you created in Question 2 with `tokenize`). The `train` method will then use that data to create a model which is saved in the `mdl` attribute. This code is given to you.\n",
    "* The `train` method takes in a list of tokens (e.g. the output of `tokenize`) and outputs a language model. **This language model is represented as a `Series`, whose index consists of tokens and whose values are the probabilities that the tokens occur.** (In the case of the N-Gram model in Part 3/Question 5, the model will be represented as a DataFrame instead of a Series ‚Äì more on this later.)\n",
    "* The `probability` method takes in a sequence of tokens and returns the probability that this sequence occurs under the language model.\n",
    "* The `sample` method takes in a positive integer `M` and generates a string made up of `M` tokens using the language model. **This method generates random sentences!**\n",
    "\n",
    "The description of Question 3 walks through in detail how each of these methods work. However, here's the general workflow:\n",
    "\n",
    "$$\\text{initialize LM with tokenized corpus} \\rightarrow \\text{train LM (i.e. assign probabilities to each token) using corpus} \\rightarrow \\text{used trained LM to compute probabilities of input sentences and generate random sentences}$$\n",
    "\n",
    "In Questions 3, 4, and 5, you will create classes for different language models ‚Äì uniform, unigram, and N-Gram, respectively. In each class, you will implement each of the above methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 ‚Äì Uniform Language Models\n",
    "\n",
    "<a name='question3'></a>\n",
    "\n",
    "A uniform language model is one in which any word is equally likely to appear in any position, unconditional of any other information.\n",
    "\n",
    "Let's put into context what this means by using the following example corpus:\n",
    "\n",
    "```py\n",
    ">>> corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    ">>> tokenize(corpus)\n",
    "['\\x02', 'when', 'I', 'eat', 'pizza', ',', 'I', 'smile', ',', 'but', 'when', 'I', 'drink', 'Coke', ',', 'my', 'stomach', 'hurts', '\\x03']\n",
    "```\n",
    "\n",
    "Given a tokenized corpus, you need to build the language model itself in `train`. As mentioned above, language models are stored as Series, where words are the index and probabilities are the values. **In a uniform language model**, the probability assigned to each token is **1 over the total number of unique tokens in the corpus**.\n",
    "\n",
    "The example corpus above has 14 **unique** tokens. This means that we'd have $P(\\text{\\x02}) = \\frac{1}{14}$, $P(\\text{when}) = \\frac{1}{14}$, and so on. Specifically, in this example, **the Series that `train` returns should contain the following values**:\n",
    "\n",
    "| Token | Probability |\n",
    "| --- | --- |\n",
    "| `'\\x02'` | $\\frac{1}{14}$ |\n",
    "| `'when'` | $\\frac{1}{14}$ |\n",
    "| `'I'` | $\\frac{1}{14}$ |\n",
    "| `'eat'` | $\\frac{1}{14}$ |\n",
    "| `'pizza'` | $\\frac{1}{14}$ |\n",
    "| `','` | $\\frac{1}{14}$ |\n",
    "| `'smile'` | $\\frac{1}{14}$ |\n",
    "| `'but'` | $\\frac{1}{14}$ |\n",
    "| `'drink'` | $\\frac{1}{14}$ |\n",
    "| `'Coke'` | $\\frac{1}{14}$ |\n",
    "| `'my'` | $\\frac{1}{14}$ |\n",
    "| `'stomach'` | $\\frac{1}{14}$ |\n",
    "| `'hurts'` | $\\frac{1}{14}$ |\n",
    "| `'\\x03'` | $\\frac{1}{14}$ |\n",
    "\n",
    "Note that:\n",
    "- **None of the probabilities we computed are conditional ‚Äì the uniform model does not use conditional probabilities!**\n",
    "- When looking at the Series that `train` returns (i.e. when looking at the `mdl` attribute), the `'\\x02'` and `'\\x03'` characters will show up as blank characters in the index. This is to be expected.\n",
    "- Your Series doesn't need to have the labels `'Token'` or `'Probability'` in it, like the above table does.\n",
    "\n",
    "After training the model, you need to implement two more methods, `probability` and `sample`.\n",
    "\n",
    "`probability` should take in any tuple of tokens and use the probabilities you computed in `train` (that are stored in the `mdl` attribute) to assign a probability to that sequence. For instance, suppose the input tuple is `('when', 'I', 'drink', 'Coke', 'I', 'smile')` (note that this tuple does not need to start with `'\\x02'` or end with `'\\x03'`). To compute the probability of this tuple under our language model, we would multiply the \"trained\" probabilities for each word individually. Here that would give us \n",
    "\n",
    "$$P(\\text{when I drink Coke I smile}) = P(\\text{when}) \\cdot P(\\text{I}) \\cdot P(\\text{drink}) \\cdot P(\\text{Coke}) \\cdot P(\\text{I}) \\cdot P(\\text{smile}) = \\left( \\frac{1}{14} \\right)^6$$\n",
    "\n",
    "Note that if the input tuple contains a token that was not in our corpus, `probability` should return 0.\n",
    "\n",
    "Finally, `sample` should take in a positive integer, `M`, and return a sentence made up of `M` randomly sampled tokens, in which the probabilities come from `mdl`. For instance, if `M=5`, then we'd return a sentence containing 5 randomly selected tokens from the table above, such that the probability that each token is selected is $\\frac{1}{14}$. The sampling is done with replacement, so we could end up with the same token multiple times. For instance, we might end up with `'but drink smile drink hurts'`. Note that this sentence doesn't make any grammatical sense (that's okay!) and that tokens are separated by spaces.\n",
    "\n",
    "The starter code contains a class `UniformLM` which represents a uniform language model. Complete the implementation of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip: Test out your UniformLM class on the \"shakes\" corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "tokens = tuple('one one two three one two four'.split())\n",
    "unif = UniformLM(tokens)\n",
    "unif.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 ‚Äì Unigram Language Models\n",
    "\n",
    "<a name='question4'></a>\n",
    "\n",
    "A unigram language model is one in which the **probability assigned to a token is equal to the proportion of tokens in the corpus that are equal to said token**. That is, the probability distribution associated with a unigram language model is just the empirical distribution of tokens in the corpus. \n",
    "\n",
    "Let's understand how probabilities are assigned to tokens using our example corpus from before.\n",
    "\n",
    "```py\n",
    ">>> corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    ">>> tokenize(corpus)\n",
    "['\\x02', 'when', 'I', 'eat', 'pizza', ',', 'I', 'smile', ',', 'but', 'when', 'I', 'drink', 'Coke', ',', 'my', 'stomach', 'hurts', '\\x03']\n",
    "```\n",
    "\n",
    "Here, there are 19 total tokens. 3 of them are equal to `'I'`, so $P(\\text{I}) = \\frac{3}{19}$. Here, the Series that `train` returns should contain the following values:\n",
    "\n",
    "| Token | Probability |\n",
    "| --- | --- |\n",
    "| `'\\x02'` | $\\frac{1}{19}$ |\n",
    "| `'when'` | $\\frac{2}{19}$ |\n",
    "| `'I'` | $\\frac{3}{19}$ |\n",
    "| `'eat'` | $\\frac{1}{19}$ |\n",
    "| `'pizza'` | $\\frac{1}{19}$ |\n",
    "| `','` | $\\frac{3}{19}$ |\n",
    "| `'smile'` | $\\frac{1}{19}$ |\n",
    "| `'but'` | $\\frac{1}{19}$ |\n",
    "| `'drink'` | $\\frac{1}{19}$ |\n",
    "| `'Coke'` | $\\frac{1}{19}$ |\n",
    "| `'my'` | $\\frac{1}{19}$ |\n",
    "| `'stomach'` | $\\frac{1}{19}$ |\n",
    "| `'hurts'` | $\\frac{1}{19}$ |\n",
    "| `'\\x03'` | $\\frac{1}{19}$ |\n",
    "\n",
    "As before, the `probability` method should take in a tuple and return its probability, using the probabilities stored in `mdl`. For instance, suppose the input tuple is `('when', 'I', 'drink', 'Coke', 'I', 'smile')`. Then,\n",
    "\n",
    "$$P(\\text{when I drink Coke I smile}) = P(\\text{when}) \\cdot P(\\text{I}) \\cdot P(\\text{drink}) \\cdot P(\\text{Coke}) \\cdot P(\\text{I}) \\cdot P(\\text{smile}) = \\frac{2}{19} \\cdot \\frac{3}{19} \\cdot \\frac{1}{19} \\cdot \\frac{1}{19} \\cdot \\frac{3}{19} \\cdot \\frac{1}{19}$$\n",
    "\n",
    "The `sample` method should now account for the fact that not all tokens are equally likely to be sampled. For instance, `'I'` is much more likely to appear in a randomly generated sentence created by `sample` than `'Coke'` is.\n",
    "\n",
    "The starter code contains a class `UnigramLM` which represents a uniform language model. Complete the implementation of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip: Test out your UnigramLM class on the \"shakes\" corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "tokens = tuple('one one two three one two four'.split())\n",
    "unigram = UnigramLM(tokens)\n",
    "unigram.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Baseline Language Models\n",
    "\n",
    "You've now trained two baseline language models capable of generating new text from a given training text. Attempt to answer these questions for yourself before you continue.\n",
    "\n",
    "* Which model do you think is better? Why?\n",
    "* What are the ways in which both of these models are bad?\n",
    "\n",
    "If you haven't trained your models on the `shakes` corpus, uncomment and run the cells below to do so and generate new random \"sentences\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run ‚Äì should take less than 10 seconds\n",
    "# shakes_uniform = UniformLM(shakes)\n",
    "# shakes_unigram = UnigramLM(shakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run\n",
    "# shakes_uniform.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run\n",
    "# shakes_unigram.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a name='part3'></a>\n",
    "\n",
    "## Part 3: Creating an N-Gram Language Model üìö\n",
    "\n",
    "### Recap\n",
    "\n",
    "First, let's recap what we've done so far. Recall the chain rule for probability, where $w$ is a sentence made up of tokens $w_1, w_2, ..., w_n$:\n",
    "\n",
    "$$P(w) = P(w_1,\\ldots,w_n) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_1,w_2) \\cdot\\ldots\\cdot P(w_n|w_1,\\ldots,w_{n-1})$$\n",
    "\n",
    "In Questions 3 (Uniform) and 4 (Unigram), your `train` methods computed these probabilities **unconditionally**, meaning that the probability that a token appeared in a sentence did **not depend** on the other tokens around it. That is, you said $P(w_i | w_1, w_2, ..., w_{i - 1}) = P(w_i)$. In Question 3, you let $P(w_i) = \\frac{1}{\\text{# unique tokens in corpus}}$, and in Question 4, you let $P(w_i) = \\frac{\\text{# tokens in corpus equal to $w_i$}}{\\text{# tokens in corpus}}$. Cruciually, each probability was determined by looking at the corpus that the model was trained on.\n",
    "\n",
    "<br>\n",
    "\n",
    "### N-Gram Overview\n",
    "\n",
    "Now we will build an N-Gram language model, in which the probability of a token appearing in a sentence **does depend** on the tokens that come before it. \n",
    "\n",
    "The chain rule above specifies that the probability that a token occurs at in a particular position in a sentence depends on **all** previous tokens in the sentence. However, it is often the case that the likelihood that a token appears in a sentence is influenced more by **nearby** tokens. (Remember, tokens are words, punctuation, or `'\\x02'` / `'\\x03'`).\n",
    "\n",
    "The N-Gram language model relies on the assumption that only nearby tokens matter. Specifically, it assumes that the probability that a token occurs depends only on the previous $N-1$ tokens, rather than all previous tokens. That is:\n",
    "\n",
    "$$P(w_n|w_1,\\ldots,w_{n-1}) = P(w_n|w_{n-(N-1)},\\ldots,w_{n-1})$$\n",
    "\n",
    "In an N-Gram language model, there is a hyperparameter that we get to choose when creating the model, $N$. For any $N$, the resulting N-Gram model looks at the previous $N-1$ tokens when computing probabilities. (Note that the unigram model you built in Question 4 is really an N-Gram model with $N=1$, since it looked at 0 previous tokens when computing probabilities.)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Example: Trigram Model\n",
    "\n",
    "When $N=3$, we have a \"trigram\" model. Such a model looks at the previous $N-1 = 2$ tokens when computing probabilities.\n",
    "\n",
    "Consider the tuple `('when', 'I', 'drink', 'Coke', 'I', 'smile')`, corresponding to the sentence `'when I drink Coke I smile'`. Under the trigram model, the probability of this sentence is computed as follows:\n",
    "\n",
    "$$P(\\text{when I drink Coke I smile}) = P(\\text{when}) \\cdot P(\\text{I | when}) \\cdot P(\\text{drink | when I}) \\cdot P(\\text{Coke | I drink}) \\cdot P(\\text{I | drink Coke}) \\cdot P(\\text{smile | Coke I})$$\n",
    "\n",
    "The trigram model doesn't consider the beginning of the sentence when computing the probability that the sentence ends in `'smile'`.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### N-Grams\n",
    "\n",
    "Both when working with a training corpus and when implementing the `probability` method to compute the probabilities of other sentences, you will need to work with \"chunks\" of $N$ tokens at a time.\n",
    "\n",
    "**Definition:** The **N-Grams of a text** are a list of tuples containing sliding windows of length $N$.\n",
    "\n",
    "For instance, the trigrams in the sentence `'when I drink Coke I smile'` are:\n",
    "\n",
    "```py\n",
    "[('when', 'I', 'drink'), ('I', 'drink', 'Coke'), ('drink', 'Coke', 'I'), ('Coke', 'I', 'smile')]\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Computing N-Gram Probabilities\n",
    "\n",
    "Notice in our trigram model above, we computed $P(\\text{when I drink Coke I smile})$ as being the product of several conditional probabilities. These conditional probabilities are the result of **training** our N-Gram model on a training corpus.\n",
    "\n",
    "To train an N-Gram model, we must compute a conditional probability for every $N$-token sequence in the corpus. For instance, suppose again that we are training a trigram model. Then, for every 3-token sequence $w_1, w_2, w_3$, we must compute $P(w_3 | w_1, w_2)$. To do so, we use:\n",
    "\n",
    "$$P(w_3 | w_1, w_2) = \\frac{C(w_1, w_2, w_3)}{C(w_1, w_2)}$$\n",
    "\n",
    "where $C(w_1, w_2, w_3)$ is the number of occurrences of the trigram sequence $w_1, w_2, w_3$ in the training corpus and $C(w_1, w_2)$ is the number of occurrences of the bigram sequence  $w_1, w_2$ in the training corpus. (Technical note: the probabilities that we compute using the ratios of counts are _estimates_ of the true conditional probabilities of N-Grams in the population of corpuses from which our corpus was drawn.)\n",
    "\n",
    "In general, for any $N$, conditional probabilities are computed by dividing the counts of N-Grams by the counts of the (N-1)-Grams they follow. \n",
    "\n",
    "**In the description of Question 5.2 we provide a detailed example of how we might compute such probabilities.**\n",
    "\n",
    "<br>\n",
    "\n",
    "### The `NGramLM` Class\n",
    "\n",
    "The `NGramLM` class contains a few extra methods and attributes beyond those of `UniformLM` and `UnigramLM`:\n",
    "\n",
    "1. Instantiating `NGramLM` requires both a list of tokens and a positive integer `N`, specifying the N in N-grams. This parameter is stored in an attribute `N`.\n",
    "1. The `NGramLM` class has a method `create_ngrams` that takes in a list of tokens and returns a list of N-Grams (recall from above, an N-Gram is a **tuple** of length N). This list of N-Grams is then passed to the `train` method to train the N-Gram model.\n",
    "1. While the `train` method still creates a language model (in this case, an N-Gram model) and stores it in the `mdl` attribute, this model is most naturally stored as a DataFrame. This DataFrame will have three columns:\n",
    "    - `'ngram'`, containing the N-Grams found in the text.\n",
    "    - `'n1gram'`, containing the (N-1)-Grams upon which the N-Grams in `ngram` are built.\n",
    "    - `'prob'`, containing the probabilities of each N-Gram in `ngram`.\n",
    "1. The `NGramLM` class has an attribute `prev_mdl` that stores an (N-1)-Gram language model over the same corpus (which in turn will store an (N-2)-Gram language model over the same corpus, and so on). This is necessary to compute the probability that a word occurs at the start of a text. This code is included for you in the constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1 ‚Äì Creating N-Grams\n",
    "\n",
    "<a name='question5a'></a>\n",
    "\n",
    "Complete the implementation of the `create_ngrams` method of the `NGramLM` class, which takes in a list of tokens and returns a list of N-Grams, as a tuple. Example behavior is shown below.\n",
    "\n",
    "```py\n",
    ">>> corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    ">>> tokens = tokenize(corpus)\n",
    ">>> tokens\n",
    "['\\x02', 'when', 'I', 'eat', 'pizza', ',', 'I', 'smile', ',', 'but', 'when', 'I', 'drink', 'Coke', ',', 'my', 'stomach', 'hurts', '\\x03']\n",
    ">>> pizza_model = NGramLM(3, tokens)\n",
    ">>> pizza_model.create_ngrams(tokens)\n",
    "[('\\x02', 'when', 'I'),\n",
    " ('when', 'I', 'eat'),\n",
    " ('I', 'eat', 'pizza'),\n",
    " ('eat', 'pizza', ','),\n",
    " ('pizza', ',', 'I'),\n",
    " (',', 'I', 'smile'),\n",
    " ('I', 'smile', ','),\n",
    " ('smile', ',', 'but'),\n",
    " (',', 'but', 'when'),\n",
    " ('but', 'when', 'I'),\n",
    " ('when', 'I', 'drink'),\n",
    " ('I', 'drink', 'Coke'),\n",
    " ('drink', 'Coke', ','),\n",
    " ('Coke', ',', 'my'),\n",
    " (',', 'my', 'stomach'),\n",
    " ('my', 'stomach', 'hurts'),\n",
    " ('stomach', 'hurts', '\\x03')]\n",
    "```\n",
    "\n",
    "Make sure you understand the above behavior before implementing `create_ngrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2 ‚Äì Training the N-Gram LM\n",
    "\n",
    "<a name='question5b'></a>\n",
    "\n",
    "Now, you will compute the probabilities that define N-Gram language model itself. Recall that the N-Gram LM consists of probabilities of the form\n",
    "\n",
    "$$P(w_n|w_{n-(N-1)},\\ldots,w_{n-1})$$\n",
    "\n",
    "which we estimate by  \n",
    "\n",
    "$$\\frac{C(w_{n-(N-1)}, w_{n-(N-2)}, \\ldots, w_{n-1}, w_n)}{C(w_{n-(N-1)}, w_{n-(N-2)}, \\ldots, w_{n-1})}$$\n",
    "\n",
    "for every N-Gram that occurs in the corpus. To illustrate, consider again the following example corpus:\n",
    "\n",
    "```py\n",
    ">>> corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    ">>> tokens = tokenize(corpus)\n",
    ">>> tokens\n",
    "['\\x02', 'when', 'I', 'eat', 'pizza', ',', 'I', 'smile', ',', 'but', 'when', 'I', 'drink', 'Coke', ',', 'my', 'stomach', 'hurts', '\\x03']\n",
    ">>> pizza_model = NGrams(3, tokens)\n",
    "```\n",
    "\n",
    "Here, `pizza_model.train` must compute $P(\\text{I | \\x02 when})$, $P(\\text{eat | when I})$, $P(\\text{pizza | I eat})$, and so on, until $P(\\text{\\x03 | stomach hurts})$.\n",
    "\n",
    "To compute $P(\\text{eat | when I})$, we must find the number of occurrences of `'when I eat'` in the training corpus, and divide it by the number of occurrences of `'when I'` in the training corpus. `'when I eat'` occurred exactly once in the training corpus, while `'when I'` occurred twice, so,\n",
    "\n",
    "$$P(\\text{eat | when I}) = \\frac{C(\\text{when I eat})}{C(\\text{when I})} = \\frac{1}{2}$$\n",
    "\n",
    "To store the conditional probabilities of all N-Grams, we will use a DataFrame with three columns, like so:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>ngram</th>\n",
    "      <th>n1gram</th>\n",
    "      <th>prob</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>(when, I, drink)</td>\n",
    "      <td>(when, I)</td>\n",
    "      <td>0.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>(when, I, eat)</td>\n",
    "      <td>(when, I)</td>\n",
    "      <td>0.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>(,, but, when)</td>\n",
    "      <td>(,, but)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>(,, I, smile)</td>\n",
    "      <td>(,, I)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>(I, smile, ,)</td>\n",
    "      <td>(I, smile)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>(,, my, stomach)</td>\n",
    "      <td>(,, my)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>(but, when, I)</td>\n",
    "      <td>(but, when)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>(\u0002, when, I)</td>\n",
    "      <td>(\u0002, when)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>(stomach, hurts, \u0003)</td>\n",
    "      <td>(stomach, hurts)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>(Coke, ,, my)</td>\n",
    "      <td>(Coke, ,)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>(eat, pizza, ,)</td>\n",
    "      <td>(eat, pizza)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>(I, drink, Coke)</td>\n",
    "      <td>(I, drink)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>12</th>\n",
    "      <td>(my, stomach, hurts)</td>\n",
    "      <td>(my, stomach)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>13</th>\n",
    "      <td>(pizza, ,, I)</td>\n",
    "      <td>(pizza, ,)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>14</th>\n",
    "      <td>(I, eat, pizza)</td>\n",
    "      <td>(I, eat)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>15</th>\n",
    "      <td>(drink, Coke, ,)</td>\n",
    "      <td>(drink, Coke)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>16</th>\n",
    "      <td>(smile, ,, but)</td>\n",
    "      <td>(smile, ,)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "The row at position **1** in the above table shows that the probability of the trigram `('when', 'I', 'eat')` conditioned on the bigram `('when', 'I')` is 0.5, as we computed above. Note that many of the above conditional probabilities are equal to 1 because many trigrams and their corresponding bigrams each appeared only once, and $\\frac{1}{1} = 1$. Note that `'\\x02'` and `'\\x03'` appear as spaces above, such as in row **7**.\n",
    "\n",
    "\n",
    "After you've understood the above example output, complete the implementation of the `train` method in `NGramLM`. Remember that your model may use any $N$, not just 3 (i.e not just trigrams)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.3 ‚Äì Computing Probabilities using the N-Gram Model\n",
    "\n",
    "<a name='question5c'></a>\n",
    "\n",
    "After we've trained our N-Gram model ‚Äì that is, after we've computed a DataFrame associating each N-Gram with a conditional probability ‚Äì we need to compute probabilities for new sentences.\n",
    "\n",
    "To illustrate how this may work, let's look at an example input tuple to `probability`. Assume our model is `pizza_model` from Question 5.2's description; we will not repeat the probability table here.\n",
    "\n",
    "Suppose our input tuple is `('when', 'I', 'eat', 'pizza', ',', 'I', 'smile')`, corresponding to the sentence `'when I eat pizza, I smile'` (remember again that the tuples provided to `probability` don't need to include `'\\x02'` or `'\\x03'`). Then,\n",
    "\n",
    "$$\n",
    "\\begin{align*} &P(\\text{when I eat pizza, I smile}) \\\\ &= P(\\text{when}) \\cdot P(\\text{I | when}) \\cdot P(\\text{eat | when I}) \\cdot P(\\text{pizza | I eat}) \\cdot P(\\text{, | eat pizza}) \\cdot P(\\text{I | pizza,})\\cdot P(\\text{smile | , I}) \\\\ &= \\frac{2}{19} \\cdot 1 \\cdot \\frac{1}{2} \\cdot 1 \\cdot 1 \\cdot 1 \\cdot 1 \\\\ &= \\frac{1}{19} \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "- To find the latter five probabilities ‚Äì $P(\\text{eat | when I}) , P(\\text{pizza | I eat}) , P(\\text{, | eat pizza}) , P(\\text{I | pizza,}),$ and $P(\\text{smile | , I})$, we can use the `mdl` DataFrame that the `train` method computes.\n",
    "- To find $P(\\text{I | when})$, we can't just look at the `mdl` DataFrame, because `('when', 'I')` is not a trigram, it is a bigram. Instead, we look at our model's `prev_mdl` attribute, which itself is another instance of `NGramLM`, corresponding to a bigram model over the same corpus. There, we can find the probability $P(\\text{I | when})$.\n",
    "- To find $P(\\text{when})$, we can't just look at the `mdl` DataFrame, because `'when'` is not a trigram. It is not a bigram either. Instead, we need to look at `prev_mdl`'s `prev_mdl`, which is a `UnigramLM`, to find $P(\\text{when})$.\n",
    "\n",
    "Note that if the input tuple contains an N-Gram that was never seen in the training corpus, the returned probability is 0. Convince yourself why `pizza_model.probability(('when', 'I', 'drink', 'Coke', ',', 'I', 'smile'))` is 0 before proceeding.\n",
    "\n",
    "After you've understood the above example output, complete the implementation of the `probability` method in `NGramLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.4 ‚Äì Sampling from the N-Gram Model\n",
    "\n",
    "<a name='question5d'></a>\n",
    "\n",
    "The last method you implemented in the `UniformLM` and `UnigramLM` classes was `sample`, which gave you a way of generating new sentences. \n",
    "\n",
    "Now, you will implement the `sample` method in the `NGramLM` class. It should take in a positive integer `M` and generate a string of M tokens using the trained language model. It should begin with a starting token `'\\x02'`, then generate subsequent tokens from the probabilities in `self.mdl` and continue picking words conditional on the previous choice. \n",
    "\n",
    "Let's illustrate how sampling works using a small concrete example. Suppose our corpus and **trigram** model are defined below:\n",
    "\n",
    "```py\n",
    ">>> short_corpus = 'zebras eat green peas \\n\\n cows eat green grass \\n\\n zebras eat green peppers'\n",
    ">>> short_tokens = tokenize(short_corpus)\n",
    ">>> short_tokens\n",
    "['\\x02', 'zebras', 'eat', 'green', 'peas', '\\x03', '\\x02', 'cows', 'eat', 'green', 'grass', '\\x03', '\\x02', 'zebras', 'eat', 'green', 'peppers', '\\x03']\n",
    ">>> grass_model = NGramLM(3, short_tokens)\n",
    "```\n",
    "\n",
    "Suppose we are told to execute `grass_model.sample(5)`. Here's how we'd proceed:\n",
    "\n",
    "0. The first character in the output is `'\\x02'`, as specified above. **We won't count `'\\x02'` in the length of our output string**, so we still need to find 5 more tokens.\n",
    "1. The next character needs to be either `'zebras'` or `'cows'`, since `('\\x02', 'zebras')` and `('\\x02', 'cows')` are the only **bigrams** in `short_tokens` that start with an `'\\x02'`. $P(\\text{zebras | \\x02})$ is $\\frac{2}{3}$ and $P(\\text{cows | \\x02})$ is $\\frac{1}{3}$, so we select either `'zebras'` or `'cows'` for our next token according to these probabilities. For the sake of example, suppose we select `'cows'`. 4 more tokens to go.\n",
    "2. Now, we must look for **trigrams** that start with the bigram `('\\x02', 'cows')`. There is just one, `('\\x02', 'cows', 'eat')`, so our next token must be `'eat'`. 3 more tokens to go.\n",
    "3. Now, we must look for **trigrams** that start with the bigram `('cows', 'eat')`. Again, there is just one, `('cows', 'eat', 'green')`, so our next token must be `'green'`. 2 more tokens to go.\n",
    "4. Now, we must look for **trigrams** that start with the bigram `('eat', 'green')`. There are three options ‚Äì `('eat', 'green', 'peas')`, `('eat', 'green', 'grass')`, and `('eat', 'green', 'peppers')`. Since $P(\\text{peas | eat green}) = P(\\text{grass | eat green}) = P(\\text{peppers | eat green}) = \\frac{1}{3}$, we pick either `'peas'`, `'grass'`, or `'peppers'` uniformly at random. For the sake of example, suppose we select `'peppers'`. 1 more token to go.\n",
    "5. We must end the output string now with `'\\x03'`, putting us at `'\\x02'` plus 5 tokens, which is the number of tokens we were told to sample. Note that `'\\x03'` **does** count towards the number of tokens we were asked to sample.\n",
    "\n",
    "Our result is `'\\x02 cows eat green peppers \\x03'`. **Note that in our training corpus we never encountered an instance of cows üêÑ eating green peppers ü´ë, but we were able to generate a coherent sentence in which they did ‚Äì pretty cool!**\n",
    "\n",
    "\n",
    "Some additional guidance:\n",
    "- If you run into a situation where there are no N-Grams that match the most recent (N-1)-Gram, you should add `'\\x03'` (STOP) token as the next token in your output sentence. There is a chance that your sampled sentence ends in many `'\\x03'`s, and that's fine.\n",
    "- Helper functions and recursion will be very helpful.\n",
    "\n",
    "After you've understood the above example, complete the implementation of the `sample` method in `NGramLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "# note ‚Äì these tests are different than the doctests; you should run them both \n",
    "tokens = \"\\x02 Humpty Dumpty sat on a wall , Humpty Dumpty had a great fall . \\x03 \\x02 All the king ' s horses and all the king ' s men couldn ' t put Humpty together again . \\x03\".split()\n",
    "tokens = tuple(tokens)\n",
    "ngram = NGramLM(2, tokens)\n",
    "out_5a1 = ngram.create_ngrams(tokens)\n",
    "out_5b1 = ngram.mdl\n",
    "out_5c1 = ngram\n",
    "out_5d1 = ngram.sample(500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've built an N-Gram language model, let's use it to actually generate sentences!\n",
    "\n",
    "Uncomment and run the cell below to define a bigram model using the `shakes` corpus and to generate a sentence of length 50 using the model. **The cell should run in under 30 seconds.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run\n",
    "# shakes_bigram = NGramLM(2, shakes)\n",
    "# shakes_bigram.sample(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden tests in Part 3/Question 5 will test your `NGramLM` implementation on corpuses that are much longer than the doctests/public tests. One of the corpuses we will test your implementation on is in `data/homertokens.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homer_tokens = tuple(open('data/homertokens.txt').read().split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, it's a good idea to make sure that you can instantiate an `NGramLM` object using `homer_tokens` and that all methods (`probability`, `sample`) run in under ~20 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGramLM(5, homer_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you're satisfied with your `NGramLM` implementation, do a little bit of reflecting. How might you improve the `NGramLM` model? One major deficit is that it assigns a probability of 0 to sentences that contain N-Grams that weren't seen in the corpus; how might you address this? _Hint: You encountered a similar issue when learning Na√Øve Bayes in DSC 40A! üòä_\n",
    "\n",
    "You don't need to actually make any improvements to `NGramLM`, these are just points for you to think about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations, you've finished Project 4! üéâ\n",
    "\n",
    "Submit your `project.py` file to Gradescope. Note that you only need to submit the `project.py` file; this notebook should not be uploaded because there are no manually-graded questions in this project.\n",
    "\n",
    "Before submitting, you should ensure that all of your work is in the `project.py` file. You can do this by running the doctests below, which will verify that your work passes the public tests **and** that your work is in the `project.py` file. Run the cell below; you should see no output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m doctest project.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, `grader.check_all()` will verify that your work passes the public tests. Ultimately, the Gradescope autograder is also going to run `grader.check_all()`, so you should ensure these pass as well (which they should if the doctests above passed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> beowulf[:20] == '\\n\\n\\n\\n\\nProduced by Dav'\nTrue",
         "failure_message": "make sure front matter not in string",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> license = 're-use it under the terms of the Project Gutenberg License'\n>>> license not in beowulf\nTrue",
         "failure_message": "make sure front matter not in string",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> story = 'king of the Danes, or Scyldings'\n>>> story in beowulf\nTrue",
         "failure_message": "text from book check",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> story = 'Said he was kindest of kings'\n>>> story in beowulf\nTrue",
         "failure_message": "text from book check",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> end = 'This file should be named 16328-8.txt'\n>>> end not in beowulf\nTrue",
         "failure_message": "end matter check",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 1000000 <= len(shakes) <= 1500000\nTrue",
         "failure_message": "approx correct number of tokens",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> elapsed <= 10\nTrue",
         "failure_message": "shakespeare fast enough",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> 1000000 <= len(shakes) <= 1500000\nTrue",
         "failure_message": "approx correct number of tokens",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> shakes[:3] == ['\\x02', 'The', 'Complete']\nTrue",
         "failure_message": "check beginning",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> shakes[-3:] == ['William', 'Shakespeare', '\\x03']\nTrue",
         "failure_message": "check ending",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> shakes[41] == 'IT'\nTrue",
         "failure_message": "Check specific token",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> 'youth' in shakes[490:510]\nTrue",
         "failure_message": "approx correct number of tokens",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> (unif.mdl == 0.25).all()\nTrue",
         "failure_message": "only one probability",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> unif.mdl.shape[0] == 4\nTrue",
         "failure_message": "number of tokens in mdl",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(unif.mdl, pd.Series)\nTrue",
         "failure_message": "mdl correct type",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> set(unif.mdl.index) == set('one two three four'.split())\nTrue",
         "failure_message": "correct indices for mdl",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> unif.probability(('five',)) == 0\nTrue",
         "failure_message": "five not a token",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> unif.probability(('one', 'two')) == 0.0625\nTrue",
         "failure_message": "probability of given sequence appearing",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> np.isclose(unif.probability(('one', 'two', 'three', 'two', 'one', 'four', 'one', 'two')), 0.25 ** 8, atol=1e-6)\nTrue",
         "failure_message": "probability of given sequence appearing",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> isinstance(unif.sample(1000), str)\nTrue",
         "failure_message": "sample is string",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> len(unif.sample(1000).split()) == 1000\nTrue",
         "failure_message": "length of sample",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> s = pd.Series(unif.sample(1000).split()).value_counts(normalize=True)\n>>> np.isclose(s, 0.25, atol=0.05).all()\nTrue",
         "failure_message": "prop of words in sample close to 0.25",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(unigram.mdl, pd.Series)\nTrue",
         "failure_message": "mdl correct type",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> set(unigram.mdl.index) == set('one two three four'.split())\nTrue",
         "failure_message": "correct indices for mdl",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> np.isclose(unigram.mdl.loc['one'], 3/7, atol=0.005)\nTrue",
         "failure_message": "mdl: one",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> unigram.probability(('five',)) == 0\nTrue",
         "failure_message": "five not a token",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> p = unigram.probability(('one', 'two')) \n>>> np.isclose(p, 0.12244897959, atol=0.0001)\nTrue",
         "failure_message": "probability of given sequence appearing",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> p = unigram.probability(('one', 'two', 'one', 'one', 'four'))\n>>> p_out = (unigram.mdl['one'] ** 3) * unigram.mdl['two'] * unigram.mdl['four']\n>>> np.isclose(p, p_out, atol=0.0001)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> len(unigram.sample(1000).split()) == 1000\nTrue",
         "failure_message": "sample length",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> isinstance(unigram.sample(1000), str)\nTrue",
         "failure_message": "sample is string",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> s = pd.Series(unigram.sample(1000).split()).value_counts(normalize=True).loc['one']\n>>> np.isclose(s, 0.41, atol=0.05).all()\nTrue",
         "failure_message": "prop of words in sample close to observed prop, 0.41",
         "hidden": false,
         "locked": false,
         "points": 0.25
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 36 <= len(out_5a1) <= 40\nTrue",
         "failure_message": "test length",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> all([len(x) == 2 for x in out_5a1])\nTrue",
         "failure_message": "ngram lengths",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out_5a1[0] == ('\\x02', 'Humpty')\nTrue",
         "failure_message": "padded with START",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> ('Humpty', 'Dumpty') in out_5a1\nTrue",
         "failure_message": "Humpty Dumpty in bigrams",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> ('great', 'fall') in out_5a1\nTrue",
         "failure_message": "specific bigram in bigrams",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
