{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"project.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 ‚Äì The Other Side of Gradescope üíØ\n",
    "\n",
    "## DSC 80, Spring 2022\n",
    "\n",
    "### Checkpoint Due Date: Thursday, April 7th (Questions 1-4)\n",
    "### Due Date: Thursday, April 14th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Welcome to Project 1! Run the cell below to watch a [quick video üé•](https://youtu.be/Os-BT0FTzVg) that introduces the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('Os-BT0FTzVg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on the Project\n",
    "\n",
    "This Jupyter Notebook contains the statements of the problems and provides code and Markdown cells to display your answers to the problems.\n",
    "\n",
    "\n",
    "* Like the lab, your coding work will be developed in the accompanying `project.py` file, that will be imported into the current notebook. This code will be autograded.\n",
    "* Note that there is no manually-graded component to Project 1, so the only thing you will ever submit is `project.py`.\n",
    "* **For the checkpoint, you only need to turn in a `project.py` containing solutions for Questions 1-4!**\n",
    "    - The \"Project 1 Checkpoint\" autograder on Gradescope does not thoroughly check your code -- it only runs the doctests/public tests on Questions 1-4 to make sure that you have completed them. When you submit the final version of the project, we will use hidden tests to check your answers more thoroughly.\n",
    "* Note that this means you will ultimately have to submit the project twice ‚Äì once to the \"Project 1 Checkpoint\" autograder (Questions 1-4 only), and once to the \"Project 1\" autograder (once you're fully done).\n",
    "\n",
    "\n",
    "**Do not change the function names in the `project.py` file!**\n",
    "- The functions in the `project.py` file are how your assignment is graded, and they are graded by their name.\n",
    "- If you changed something you weren't supposed to, just use git to revert! Ask us if you need help with this, or google around for `git revert`.\n",
    "\n",
    "**Tips for developing in the `project.py` file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are **encouraged to write your own additional functions** to solve the questions! \n",
    "- Always document your code!\n",
    "\n",
    "### Working with a Partner\n",
    "\n",
    "You are allowed to work with a partner on projects in DSC 80. If you do work with a partner, you must follow the [Pair Programming Guidelines](https://dsc10.com/pair-programming/). Specifically, you must be actively working on the project at the same time on one computer. Splitting up the project and working on it separately **is not** pair programming.\n",
    "\n",
    "You can use [this sheet](https://docs.google.com/spreadsheets/d/1PMtGpd4U6rYBn6Ut6eHQzSo4PdBwluU-ppx87ROy_N8/edit?usp=sharing) to find a partner. Your partner does not have to be in the same lecture section as you.\n",
    "\n",
    "Note that if you do work with a partner, you and your partner must submit the Checkpoint together and the whole project together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Assignment\n",
    "\n",
    "The file contains the gradebook from a fictional data science course with 535 students. \n",
    "\n",
    "***Note: this dataset is synthetically generated; it does not contain real student grades. The course syllabus below is similar, but not quite the same as the course syllabus for this class!***\n",
    "\n",
    "In this project, you will:\n",
    "1. Clean and process the data to compute total course grades according to a fictional syllabus (below).\n",
    "2. Qualitatively understand how students did in the course.\n",
    "3. Understand how student grades vary with small changes in performance on each assignment.\n",
    "\n",
    "---\n",
    "\n",
    "### Navigating the Project\n",
    "\n",
    "Click on the links below to navigate to different parts of the project. Note that Questions 1, 2, 3, and 4 constitute your Checkpoint submission.\n",
    "\n",
    "- [Part 1: Enumerating the Assignments  üî¢](#part1)\n",
    "    - [Question 1 (Checkpoint Question)](#Question-1-(Checkpoint-Question))\n",
    "- [Part 2: Computing Project Grades üßÆ](#part2)\n",
    "    - [Question 2 (Checkpoint Question)](#Question-2-(Checkpoint-Question))\n",
    "- [Part 3: Computing Lab Grades üß™](#part3)\n",
    "    - [Question 3 (Checkpoint Question)](#Question-3-(Checkpoint-Question))\n",
    "    - [Question 4 (Checkpoint Question)](#Question-4-(Checkpoint-Question))\n",
    "    - [Question 5](#Question-5)\n",
    "    - [Question 6](#Question-6)\n",
    "- [Part 4: Putting It All Together üß©](#part4)\n",
    "    - [Question 7](#Question-7)\n",
    "    - [Question 8](#Question-8)\n",
    "- [Part 5: Do Seniors Get Worse Grades? üëµ](#part5)\n",
    "    - [Question 9](#Question-9)\n",
    "- [Part 6: What is the True Distribution of Grades? üßê](#part6)\n",
    "    - [Question 10](#Question-10)\n",
    "    - [Question 11](#Question-11)\n",
    "\n",
    "---\n",
    "\n",
    "### The Syllabus\n",
    "\n",
    "The course syllabus for this fictional class is as follows:\n",
    "\n",
    "* **Lab assignments (20% total)**\n",
    "    - Each lab is worth the same amount, regardless of each lab's raw point total.\n",
    "    - The lowest lab is dropped.\n",
    "    - Each lab may be revised for up to (and including) one week after the deadline for a 10% penalty, for up to (and including) two weeks after the deadline for a 30% penalty, and beyond that for a 60% penalty. Such revisions are reflected in the `'Lateness'` columns in the gradebook.\n",
    "* **Projects (30% total)** \n",
    "    - Each project consists of an autograded portion, and **possibly** a free response portion.\n",
    "    - The total points for a single project consist of the sum of the raw score of the two portions.\n",
    "    - Each project is worth the same amount, regardless of each project's raw point total.\n",
    "* **Checkpoints (2.5% total)**\n",
    "    - Each project checkpoint is worth the same amount, regardless of each project checkpoint's raw point total.\n",
    "* **Discussions (2.5% total)**\n",
    "    - Each discussion is worth the same amount, regardless of each discussion's raw point total.\n",
    "* **Midterm Exam (15%)**\n",
    "* **Final Exam (30%)**\n",
    "\n",
    "You will need to refer to this syllabus repeatedly throughout the project, and several questions will link you back to it.\n",
    "\n",
    "---\n",
    "\n",
    "### Generalization\n",
    "\n",
    "You may assume that your code will only need to work on a gradebook for a class with the syllabus given above. That is, you may assume that the DataFrame `grades` looks **like** the given one in `data/grades.csv`.\n",
    "\n",
    "However, such a class:\n",
    "1. May have a different numbers of labs, projects, discussions, and project checkpoints.\n",
    "2. May have a different number of students.\n",
    "\n",
    "You may assume the course components and the naming conventions are as given in the data file, and you may assume that the course has no more than 99 of any type of assignment.\n",
    "\n",
    "---\n",
    "\n",
    "### Putting Everything Together\n",
    "\n",
    "Here are a few remarks and tips for approaching Project 1, and projects more generally:\n",
    "\n",
    "1. If you are having trouble figuring out what a question is asking you to do, look at the big picture and try to understand what the current step is doing to contribute to this big picture. This may clarify what's being asked!\n",
    "1. These questions intentionally build off of each other and the final result matters! In fact, you can \"get a question correct\", but only receive partial credit for it because a previous answer was wrong.\n",
    "    - Credit for a question will typically receive partial credit based on *how close* your answer is to correct (as well as some credit for a solution in the correct form). \n",
    "    - You should try to assess your answer to each question based on what you understand of the data. This might involve writing extensive code (that isn't turned in) just to check your work! Suggestions on checking your work are given in the assignment, but you should also think of your own ways of checking your work.\n",
    "    - As you do this project, think about the data from the perspective of the student (which should be easy to do, since you've used Gradescope before!)\n",
    "1. To test the correctness of your answers:\n",
    "     - Once you have implemented a particular function in `project.py`, you should test out your function in the notebook. In particular, you should inspect/analyze the output to assess its correctness.\n",
    "    - Run your functions on the main dataset (`grades`) and ask yourself if the output *looks correct.*\n",
    "    - Run your functions on very small datasets (e.g. 1-5 row DataFrames that you construct by hand), calculate the expected output by hand, and see if the function output matches (this *is* unit-testing your code with data).\n",
    "    * Run your functions on (large and small) samples of the dataset `grades`. Does your code break, or does it still run as expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to load in the aforementioned `grades` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_fp = os.path.join('data', 'grades.csv')\n",
    "grades = pd.read_csv(grades_fp)\n",
    "grades.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip:** The `grades` DataFrame has 100 columns, and you can't see them all right now. To get a feel for what all of the columns represent, you might consider opening `grades.csv` with a spreadsheet application, like Google Sheets or Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='part1'></a>\n",
    "\n",
    "## Part 1: Enumerating the Assignments üî¢\n",
    "\n",
    "To start, you'll list out the names of each assignment in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (Checkpoint Question)\n",
    "\n",
    "Complete the implementation of the function `get_assignment_names`, which takes in a DataFrame like `grades` and returns a dictionary with the following structure:\n",
    "- The keys are the general areas of [the syllabus](#The-Syllabus): `'lab'`, `'project'`, `'midterm'`, `'final'`, `'disc'`, and `'checkpoint'`.\n",
    "- The values are **lists** that contain all the assignment names of that type. For example, the lab assignments all have names of the form `'labXX'` where `XX` is a zero-padded two digit number. If the class has 5 labs, the returned dictionary's value for the `'lab'` key should be `['lab01', 'lab02', 'lab03', 'lab04', 'lab05']`.\n",
    "\n",
    "See the doctests for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='part2'></a>\n",
    "\n",
    "## Part 2: Computing Project Grades üßÆ\n",
    "\n",
    "Now you're ready to compute each student's overall grade on the first type of assignment ‚Äì projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (Checkpoint Question)\n",
    "\n",
    "Complete the implementation of the function `projects_total`, which takes in a DataFrame `grades` and returns a Series containing the total project grade for each student for the entire quarter, according to [the syllabus](#The-Syllabus). The output Series should contain values between 0 and 1.\n",
    "\n",
    "***Note***: Don't forget to properly handle students who didn't turn in assignments. Also, don't forget the fact that some projects have free response components that you need to account for.\n",
    "\n",
    "***Hints:*** To check your work, try:\n",
    "1. Calculating the total project scores for a few types of students by hand.\n",
    "2. Calculating summary statistics for the whole class' performance on a few projects in particular and ensuring the results seem reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='part3'></a>\n",
    "\n",
    "## Part 3: Computing Lab Grades üß™\n",
    "\n",
    "Now, you will clean and process the lab grades, which involves a bit more work than was necessary for projects. To do this, you will develop functions that:\n",
    "- identify late submissions (Questions 3 and 4), \n",
    "- compute normalized scores for each lab assignment, factoring in late penalties (Question 5), and \n",
    "- drop the lowest lab grade and compute a total lab score for each student (Question 6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (Checkpoint Question)\n",
    "\n",
    "Unfortunately, Gradescope sometimes experiences a delay in registering when an assignment is submitted during \"periods of heavy usage\" (i.e. near a submission deadline). For instance, let's say that 15 students submit their assignment at 11:59 PM, right before the deadline. In this case, Gradescope has trouble registering the 15 submissions at the same time. As a result, Gradescope registers these submissions one-by-one **after** the deadline over some period of time, and these submissions are **marked as despite being submitted on-time**. \n",
    "\n",
    "Your job is to identify when a student's lab assignment was actually submitted on time, even if Gradescope did not process it in time and marked it as late. To do this, it is helpful to know that in our fictional class:\n",
    "* Late submissions are turned off, so the students cannot submit their assignment on their own after the deadline.\n",
    "* The only way that a student can make a late submission is by attending office hours and having a tutor submit for them.\n",
    "* However, there are no office hours \"just after\" the deadline, since deadlines are at 11:59 PM and tutors are asleep by then.\n",
    "* As a result, **truly late submissions are not submitted within a few hours of a deadline, but are instead submitted later**.\n",
    "\n",
    "Complete the implementation of the function `last_minute_submissions`, which takes in a DataFrame `grades` and outputs a **Series indexed by lab assignment that contains the number of submissions that were turned in on time by students that were marked \"late\" by Gradescope**. For instance, if the value for the `'lab01'` index in your returned Series is 15, that should mean that 15 students submitted Lab 1 on time but were marked as late by Gradescope.\n",
    "\n",
    "***Notes:***\n",
    "\n",
    "- You have to figure out what a truly late submission is by looking at the data and understanding the data generating process described above. This question is about \"cleaning\" a messy \"data recording process\". There is some ambiguity in finding which submissions are truly late; you will make a best guess for **a threshold** by looking at this dataset.  All the submissions that occur before this **threshold time** are on-time submissions that are incorrectly marked as \"late\".\n",
    "\n",
    "- There is no one correct value for the threshold; a range of threshold values will return the correct answer.\n",
    "\n",
    "- This function only involves **labs**, do not look at any other assignments categories.\n",
    "\n",
    "- If you're curious, this is not how Gradescope actually works.\n",
    "\n",
    "***Hints:*** \n",
    "\n",
    "- At some point, you'll need to convert times of the form `'00:11:08'` into a more usable numeric form.\n",
    "- Plot the distribution of number of submissions over time and use that to determine the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (Checkpoint Question)\n",
    "\n",
    "Now you need to adjust lab grades for truly late submissions. However, you need to take into account your investigation in the previous question, since students shouldn't be penalized by a bug in Gradescope!\n",
    "\n",
    "Complete the implementation of the function `lateness_penalty`, which takes in a Series (like `grades['lab01 - Lateness (H:M:S)']`) containing data on how late a student turned in an assignment and returns a Series of penalties (represented by the values `1.0` ,`0.9`, `0.7`, and `0.4` according to [the syllabus](#The-Syllabus)). **Only truly late submissions should be counted as late**.\n",
    "\n",
    "***Note***: For the purpose of this project, we will only be calculating lateness for labs. There is no penalty for lateness for projects, discussions, nor checkpoints (unlike in real DSC 80 üò¢)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Complete the implementation of the function `process_labs`, which takes in a DataFrame like `grades` and returns a DataFrame of processed lab scores. The returned DataFrame should:\n",
    "* have the same index as `grades`,\n",
    "* have one column for each lab assignment (e.g. `'lab01'`, `'lab02'`,..., `'lab09'`),\n",
    "* have values representing the final score for each lab assignment, adjusted for lateness and **normalized** to a score between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Complete the implementation of the function `lab_total`, which takes in a DataFrame of processed assignments (like the output of Question 5) and returns a Series containing the total lab grade for each student according to [the syllabus](#The-Syllabus). Your answers should be proportions between 0 and 1. \n",
    "\n",
    "For example, if the class only has 3 labs, and a student received scores of 20%, 90%, and 100%, then your output should be `0.95`. This is because we drop the lowest score, and thus in effect we only compute the average of 90% and 100%, which is 95%, or 0.95 as a proportion.\n",
    "\n",
    "***Note:*** If a student does not turn in a lab, their score for that lab is a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='part4'></a>\n",
    "\n",
    "## Part 4: Putting It All Together üß©\n",
    "\n",
    "It's time to compute the letter grade of each student."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "First, you need to compute each student's course grade, which results from adding their total grades in each course component according to the weights given in [the syllabus](#The-Syllabus).\n",
    "\n",
    "Complete the implementation of the function `total_points`, which takes in a DataFrame `grades` and returns a Series containing each student's course grade. **Course grades should be proportions between 0 and 1.**\n",
    "\n",
    "***Notes***: \n",
    "\n",
    "- Don't repeat yourself when computing the checkpoint and discussion portions of the course.\n",
    "- Remember, only the lab portion of the course accounts for late assignments; you may assume all assignments in other portions are turned in without penalty.\n",
    "- Do the work by hand for a few students to check your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Two more functions to go in this part!\n",
    "\n",
    "#### `final_grades`\n",
    "\n",
    "Complete the implementation of the function `final_grades`, which takes in final course grades (as computed in Question 7) and returns a Series of letter grades as determined by the standard cutoffs (without pluses or minuses):\n",
    "\n",
    "| Letter Grade | Cutoff |\n",
    "|:--- | --- |\n",
    "| A | grade >= 0.9 |\n",
    "| B | 0.8 <= grade < 0.9 |\n",
    "| C | 0.7 <= grade < 0.8 |\n",
    "| D | 0.6 <= grade < 0.7 |\n",
    "| F | grade < 0.6 |\n",
    "\n",
    "***Note:*** Do not round anyone's course grade when determining their letter grade.\n",
    "\n",
    "#### `letter_proportions`\n",
    "\n",
    "Complete the implementation of the function `letter_proportions`, which takes in a DataFrame `grades` and returns a Series containing the proportion of the class that received each letter grade. For instance, this Series might tell us that the proportion of the class receiving Bs was 0.45, As was 0.33, Cs was 0.16, Ds was 0.05, and Fs was 0.01 (though these are made up numbers). The index of this Series should be letters, and the **values should be sorted in decreasing order**.\n",
    "\n",
    "***Notes***: \n",
    "\n",
    "- The values in your returned Series should add up to exactly 1.0. If you are getting something close such as 0.99999, that means there is an issue with your code in a function you implemented earlier. \n",
    "\n",
    "- To check your work, verify the course grade distribution and relevant statistics! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='part5'></a>\n",
    "\n",
    "## Part 5: Do Seniors Get Worse Grades? üëµ\n",
    "\n",
    "Now that you've computed the overall course grades and letter grades for all students in our fictional class, it's time to perform some analyses of these grades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "You notice that students who are seniors on average did worse in the class (if you can't verify this, you should go back and check your work!). Is this difference significant, or just due to noise?\n",
    "\n",
    "Perform a hypothesis test, assessing the likelihood of the above statement under the null hypothesis: \n",
    "> Seniors earn grades that are roughly equal on average to the rest of the class.\n",
    "\n",
    "To do this, complete the implementation of the function `simulate_pval`, which takes in a DataFrame `grades` and a number of simulations `N` and returns the **probability that the mean grade (between 0 and 1) earned by a random subset of students* is less than or equal to the average grade of seniors** (i.e. calculate and return the p-value).\n",
    "\n",
    "*_The size of each random subset must be same the number of seniors in the class._\n",
    "\n",
    "***Note:*** To check your work, plot the sampling distribution of your test statistic along with the observed test statistic. Do these values look reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='part6'></a>\n",
    "\n",
    "## Part 6: What is the True Distribution of Grades? üßê\n",
    "\n",
    "The gradebook for this class only reflects one particular instance of each student's performance, subject to the effects of all the little events and hiccups that occurred throughout the quarter. Would you have done better on the midterm if your roommate didn't kept you up all night with their coughing? Wasn't it lucky that the example you were studying just before the final happened to appear on the exam?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "This question will simulate these \"(un)lucky, random events\" by **adding or subtracting random amounts to each assignment right before calculating the final grades for that assignment**. These \"random amounts\" will be drawn from a Gaussian distribution with mean 0 and standard deviation 0.02:\n",
    "```\n",
    "np.random.normal(0, 0.02, size=(num_rows, num_cols))\n",
    "```\n",
    "Intuitively, such a model says that random events may bump up or down a particular grade, given as a proportion, in a way that:\n",
    "- on average has no effect on the class as a whole (mean 0), but\n",
    "- could perturb an individual grade by 0.02 or more (standard deviation 0.02).\n",
    "\n",
    "Create a function `total_points_with_noise` that takes in a DataFrame like `grades`, adds noise to the assignments as described above, and returns the final scores using **the same procedure** as Questions 1-8.\n",
    "\n",
    "***Notes:*** \n",
    "\n",
    "- **For any given assignment, the noise should be applied after calculating the normalized scores (i.e. proportions) for that assignment. For example, the noise should be added to `'lab1'` once the column contains scores that are less than or equal to 1.**\n",
    "\n",
    "- Once adding the noise to the assignment scores, use the `np.clip` function to be sure each assignment retains a score between 0 and 1.\n",
    "\n",
    "- You should be able to reuse (or minorly change) the code from previous problems. Try to practice DRY (don't repeat yourself)!\n",
    "\n",
    "- To check your work - what would you expect the difference between the actual scores and noisy scores to be, on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "To conclude, you will answer the following five questions about the class described in `grades`. The function you are required to implement in this question, `short_answer`, should return a **hard-coded list of length 5** containing your answers to the following questions, in order:\n",
    "\n",
    "0. What is the **mean difference** between students' scores (`total_points`) and their scores with noise (`total_points_with_noise`), amongst all students in `grades`? (***Hint:*** Plot the distribution of differences.)\n",
    "1. What **proportion** of the class only sees their grade change at most (but not including) $\\pm 0.01$? (Your answer should be a number between 0 and 1.)\n",
    "2. What is the 95% confidence interval for the statistic above, as a **tuple**? (***Hint:*** See the [DSC 10 course notes](https://notes.dsc10.com/05-hypothesis_testing/1_hypothesis_tests.html) and use `np.percentile`).\n",
    "3. What **proportion** of the class sees a change in their letter grade when moving from `total_points` to `total_points_with_noise`?\n",
    "4. Answer the following two true-or-false questions with a tuple, like `(True, True)`.\n",
    "    - True or False: The model in Question 10 assumes that the observed gradebook (i.e. `grades`) is a good representation of the grades of students in DSC 80.\n",
    "    - True or False: The Series that `total_points_with_noise(grades)` evaluates to **cannot** be interpreted as a Series of grades drawn from the population of students in DSC 80.\n",
    "\n",
    "For example, `[0.0, 0.0, (0.0, 1.0), 0.0, (True, True)]` is an answer in valid form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations, you've finished Project 1! üéâ\n",
    "\n",
    "Submit your `.py` file to Gradescope. Note that you only need to submit the `.py` file; this notebook should not be uploaded because there are no manually-graded questions in this project.\n",
    "\n",
    "Before submitting, you should ensure that all of your work is in the `.py` file. You can do this by running the doctests below, which will verify that your work passes the public tests **and** that your work is in the `.py` file. Run the cell below; you should see no output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m doctest lab.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, `grader.check_all()` will verify that your work passes the public tests. Ultimately, the Gradescope autograder is also going to run `grader.check_all()`, so you should ensure these pass as well (which they should if the doctests above passed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = get_assignment_names(grades)\n>>> out['final'] == ['Final']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = get_assignment_names(grades)\n>>> set(out.keys()) == {'lab', 'project', 'midterm', 'final', 'disc', 'checkpoint'}\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = get_assignment_names(grades)\n>>> 'project02' in out['project']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q10": {
     "name": "q10",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = total_points_with_noise(grades)\n>>> np.all((0 <= out) & (out <= 1))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = total_points_with_noise(grades)\n>>> 0.7 < out.mean() < 0.9\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q11": {
     "name": "q11",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = short_answer()\n>>> len(out) == 5\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = short_answer()\n>>> (len(out[2])) == 2 and (0.5 < out[2][0] < 1) and (0 < out[3] < 1)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = short_answer()\n>>> isinstance(out[4][0], bool) and isinstance(out[4][1], bool)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = projects_total(grades)\n>>> np.all((0 <= out) & (out <= 1))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = projects_total(grades)\n>>> 0.7 < out.mean() < 0.9\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = last_minute_submissions(grades)\n>>> isinstance(out, pd.Series)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = last_minute_submissions(grades)\n>>> np.all(out.index == ['lab0%d' % d for d in range(1, 10)])\nTrue",
         "failure_message": "Checking that all labs, and only labs, are included in the results of last_minute_submissions.",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = last_minute_submissions(grades)\n>>> (out > 0).sum() == 8\nTrue",
         "failure_message": "Checking that there are exactly 8 labs that are incorrectly marked as late.",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = lateness_penalty(grades['lab01 - Lateness (H:M:S)'])\n>>> isinstance(out, pd.Series)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = lateness_penalty(grades['lab01 - Lateness (H:M:S)'])\n>>> set(out.unique()) <= {1.0, 0.9, 0.7, 0.4}\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = process_labs(grades)\n>>> out.columns.tolist() == ['lab%02d' % x for x in range(1, 10)]\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = process_labs(grades)\n>>> np.all((0.65 <= out.mean()) & (out.mean() <= 0.90))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> cols = 'lab01 lab02 lab03'.split()\n>>> processed = pd.DataFrame([[0.2, 0.90, 1.0]], index=[0], columns=cols)\n>>> np.isclose(lab_total(processed), 0.95).all()\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = total_points(grades)\n>>> np.all((0 <= out) & (out <= 1))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = total_points(grades)\n>>> 0.7 < out.mean() < 0.9\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8": {
     "name": "q8",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = final_grades(pd.Series([0.92, 0.81, 0.41]))\n>>> np.all(out == ['A', 'B', 'F'])\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = letter_proportions(grades)\n>>> np.all(out.index == ['B', 'C', 'A', 'D', 'F'])\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = letter_proportions(grades)\n>>> out.sum() == 1.0\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9": {
     "name": "q9",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = simulate_pval(grades, 1000)\n>>> 0 <= out <= 0.1\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
